{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/pythainlp/thai-it-books/data/train-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json(id,text:str,source:str,metadata:dict=None):\n",
    "    _temp = {\n",
    "        \"id\":id,\n",
    "        \"text\":str(text).strip(),\n",
    "        \"source\":source\n",
    "    }\n",
    "    if metadata != None:\n",
    "        _temp[\"metadata\"]=metadata\n",
    "    return _temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>src</th>\n",
       "      <th>license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine Learning</td>\n",
       "      <td># Machine Learning\\n\\nโดย [ชิตพงษ์ กิตตินราดร]...</td>\n",
       "      <td>https://guopai.github.io</td>\n",
       "      <td>CC-BY 4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Python ภาษาไทย</td>\n",
       "      <td># Python ภาษาไทย\\n\\nPython ภาษาไทย เขียนโดย วร...</td>\n",
       "      <td>https://python3.wannaphong.com</td>\n",
       "      <td>CC-BY 4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>มหาวิหารกับตลาดสด (The Cathedral and the Bazaar)</td>\n",
       "      <td>มหาวิหารกับตลาดสด\\nEric\\nSteven\\nRaymond\\nThyr...</td>\n",
       "      <td>https://linux.thai.net/~thep/catb/cathedral-ba...</td>\n",
       "      <td>Open Publication License v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ลงหลักปัญญาภูมิ (Homesteading the Noosphere)</td>\n",
       "      <td>ลงหลักปัญญาภูมิ\\nEric\\nSteven\\nRaymond\\nThyrsu...</td>\n",
       "      <td>https://linux.thai.net/~thep/catb/homesteading/</td>\n",
       "      <td>Open Publication License v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>คำตาม</td>\n",
       "      <td>คำตาม : พ้นไปกว่าซอฟต์แวร์?\\nเอริก\\nสตีเวน\\nเร...</td>\n",
       "      <td>https://linux.thai.net/~thep/catb/afterword/</td>\n",
       "      <td>Open Publication License b2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>คู่มือภาษาไทย Ubuntu Desktop Guide</td>\n",
       "      <td>Ubuntu desktop guide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>https://web.archive.org/web/20080828013052/htt...</td>\n",
       "      <td>CC-BY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ubuntu Desktop Guide</td>\n",
       "      <td>Ubuntu Desktop Guide นี้ ตัดมาจากหนังสือ[http:...</td>\n",
       "      <td>https://web.archive.org/web/20080716093752/htt...</td>\n",
       "      <td>CC-BY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title  \\\n",
       "0                                  Machine Learning   \n",
       "1                                    Python ภาษาไทย   \n",
       "2  มหาวิหารกับตลาดสด (The Cathedral and the Bazaar)   \n",
       "3      ลงหลักปัญญาภูมิ (Homesteading the Noosphere)   \n",
       "4                                             คำตาม   \n",
       "5                คู่มือภาษาไทย Ubuntu Desktop Guide   \n",
       "6                              Ubuntu Desktop Guide   \n",
       "\n",
       "                                                text  \\\n",
       "0  # Machine Learning\\n\\nโดย [ชิตพงษ์ กิตตินราดร]...   \n",
       "1  # Python ภาษาไทย\\n\\nPython ภาษาไทย เขียนโดย วร...   \n",
       "2  มหาวิหารกับตลาดสด\\nEric\\nSteven\\nRaymond\\nThyr...   \n",
       "3  ลงหลักปัญญาภูมิ\\nEric\\nSteven\\nRaymond\\nThyrsu...   \n",
       "4  คำตาม : พ้นไปกว่าซอฟต์แวร์?\\nเอริก\\nสตีเวน\\nเร...   \n",
       "5  Ubuntu desktop guide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "6  Ubuntu Desktop Guide นี้ ตัดมาจากหนังสือ[http:...   \n",
       "\n",
       "                                                 src  \\\n",
       "0                           https://guopai.github.io   \n",
       "1                     https://python3.wannaphong.com   \n",
       "2  https://linux.thai.net/~thep/catb/cathedral-ba...   \n",
       "3    https://linux.thai.net/~thep/catb/homesteading/   \n",
       "4       https://linux.thai.net/~thep/catb/afterword/   \n",
       "5  https://web.archive.org/web/20080828013052/htt...   \n",
       "6  https://web.archive.org/web/20080716093752/htt...   \n",
       "\n",
       "                       license  \n",
       "0                    CC-BY 4.0  \n",
       "1                    CC-BY 4.0  \n",
       "2  Open Publication License v2  \n",
       "3  Open Publication License v2  \n",
       "4  Open Publication License b2  \n",
       "5                        CC-BY  \n",
       "6                        CC-BY  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'dict' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,d \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m----> 3\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmake_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitbook_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpythainlp/thai-it-books\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msrc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msrc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdomain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbook\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'dict' and 'str'"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "for i,d in df.iterrows():\n",
    "    data.append(make_json(\"itbook_\"+str(i),d[\"text\"].strip(),\"pythainlp/thai-it-books\",{\"src\":d[\"src\"],\"domain\":\"book\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'itbook_0',\n",
       " 'text': '# Machine Learning\\n\\nโดย [ชิตพงษ์ กิตตินราดร](https://guopai.github.io/about.html)\\n\\nแนะนำ Machine learning และ Deep learning อธิบายหลักการทำงานของ Algorithm วิธีการสร้างและเทรนโมเดลด้วย scikit-learn และ TensorFlow\\n\\n* บทที่ 1 บทนำ\\n\\n**Machine Learning**\\n\\n* บทที่ 2 Linear Regression Algorithm\\n* บทที่ 3 Linear Regression Programming\\n* บทที่ 4 Logistic Regression\\n* บทที่ 5 Categorical Encoding\\n* บทที่ 6 Feature Scaling\\n* บทที่ 7 Bias and Variance\\n* บทที่ 8 Support Vector Machines\\n* บทที่ 9 Decision Tree\\n* บทที่ 10 Random Forest\\n* บทที่ 11 Boosting\\n* บทที่ 12 Clustering\\n* บทที่ 13 Anomaly Detection\\n\\n**Deep Learning**\\n\\n* บทที่ 14 Neural Network Algorithm\\n* บทที่ 15 Neural Network Programming\\n* บทที่ 16 Neural Network Vanishing Gradients\\n* บทที่ 17 Neural Network Optimisers\\n* บทที่ 18 Neural Network Regularisation\\n* บทที่ 19 Convolutional Neural Network\\n\\n**ภาคผนวก**\\n\\n* ภาคผนวก 1: ทำไมต้อง Machine Learning\\n* ภาคผนวก 2: พื้นฐาน Python อย่างสั้นที่สุด\\n\\n## บทที่ 1 บทนำ\\n\\nMachine learning (ML) เป็นการประยุกต์คณิตศาสตร์และวิทยาศาสตร์คอมพิวเตอร์เพื่อทำให้คอมพิวเตอร์สามารถเรียนรู้และตัดสินใจได้โดยไม่ต้องกำหนดกฏและเงื่อนไขตายตัว\\n\\nML เรียนรู้ด้วยการอ่านข้อมูลที่มนุษย์เตรียมให้ และประมวลผลข้อมูลนั้นเพื่อให้สามารถตัดสินใจตามที่ต้องการ โดยการตัดสินใจที่ ML ทำได้ มีหลายรูปแบบ เช่น\\n\\n* พยากรณ์ว่าข้อมูลชิ้นใหม่ที่ ML ไม่เคยเห็น มีค่าเท่าใด หรืออยู่ในหมวดหมู่ใด\\n* จัดข้อมูลเป็นกลุ่มๆ โดยให้ข้อมูลที่มีคุณลักษณะคล้ายกันอยู่ในกลุ่มเดียวกัน\\n* ตรวจหาข้อมูลชิ้นที่ผิดปกติแปลกไปจากพวก\\n* จำแนกข้อมูลบางอย่างออกจากข้อมูลทั้งหมด เช่น จำแนกเสียงพูดออกจากเสียงสภาพแวดล้อม\\n* แนะนำข้อมูลชิ้นใหม่ที่คาดว่ามนุษย์น่าจะสนใจโดยเรียนรู้จากความสนใจเดิม\\n\\nจะเห็นว่าประโยชน์ของ ML มีหลากหลายมาก สำหรับใครหรือองค์กรที่อยากใช้ ML ในงานของตนเอง สิ่งที่จำเป็นต้องมี (หรือต้องสร้างให้มี) คือ:\\n\\n1.  ข้อมูล: ML ทำงานได้ด้วยการเรียนรู้จากข้อมูล ไม่ใช่ด้วยการป้อนกฏเกณฑ์เข้าไป ดังนั้นผู้ใช้ ML จึงต้องเตรียมข้อมูลที่มีคุณภาพและมีปริมาณเพียงพอ เพื่อให้ ML เรียนรู้ได้ผลดี เรื่องนี้เป็นเรื่องสำคัญและมีรายละเอียด จะอธิบายโดยละเอียดในส่วนต่อๆ ไป\\n2.  ทักษะ Programming: ปัจจุบันนิยมใช้ภาษา Python เป็นภาษาหลักในการทำ ML ดังนั้นควรมีความรู้พื้นฐาน Python เช่น มีความเข้าใจหลักการเขียนโปรแกรม การสร้างและคำนวนตัวแปร ประเภทและโครงสร้างข้อมูลแต่ละประเภท การเขียนเงื่อนไข เช่น if, for, while loop และการเรียกใช้ Library ต่างๆ เช่น numpy (สำหรับการคำนวน Vector และ Matrix), pandas (สำหรับการจัดการและวิเคราะห์ข้อมูลแบบตาราง), matplotlib (สำหรับการพล็อตกราฟเพื่อแสดงผลเป็นภาพ)\\n3.  ทักษะในการใช้ ML Framework: ในอดีตเราต้องเขียนโค้ดเพื่อแปลงสมการคณิตศาสตร์ให้กลายเป็น Algorithm เพื่อคำนวนด้วยตนเอง ซึ่งต้องใช้ความรู้และมีความซับซ้อนค่อนข้างมาก แต่ปัจจุบันมีผู้สร้างซอฟท์แวร์อำนวยความสะดวกในการทำ ML ให้เราใช้ได้ฟรีๆ แบบ Open-source จึงแนะนำให้เรียนรู้วิธีใช้ Framework เหล่านี้ โดย Framework ที่นิยม เช่น scikit-learn สำหรับ Machine learning และ TensorFlow สำหรับ Deep learning\\n4.  ความรู้เกี่ยวกับ ML: ศึกษาหลักการทำงานของ ML Algorithm ประเภทต่างๆ การเตรียมข้อมูล วิธีการทดสอบความแม่นยำของโมเดล การปรับแต่งโมเดลให้ทำงานได้ดีขึ้น การแก้ปัญหาทางเทคนิคต่างๆ ความรู้เหล่านี้สำคัญมาก ถ้าขาดความรู้ความเข้าใจเรื่องเหล่านี้ก็จะไม่สามารถพัฒนา ML ที่ดีได้ ถึงแม้จะใช้เครื่องมือต่างๆ เป็นก็ตาม\\n\\n### ประเภทของปัญหา Machine learning\\n\\nปัญหา Machine learning สามารถจำแนกได้ 2 ประเภทใหญ่ๆ คือ\\n\\n1.  Supervised learning: คือปัญหาที่ในชุดข้อมูลนั้นมีคำตอบอยู่แล้ว ตัวอย่างเช่น:\\n    * Regression: หาคำตอบที่เป็นตัวเลข เช่น ให้ข้อมูลโภชนาการของเด็กคนหนึ่ง พยากรณ์ว่าเด็กคนนี้จะมีความสูงกี่ ซ.ม.\\n    * Classification: หาคำตอบที่เป็นหมวดหมู่ เช่น ให้ภาพ X-ray ทรวงอก พยากรณ์ว่าคนไข้เป็นมะเร็งปอดหรือไม่ (คำตอบมีแต่ ใช่/ไม่ใช่ เรียกว่า Binary classification) หรือให้ข้อมูลเกี่ยวกับครอบครัวของเด็กคนหนึ่ง ทำนายว่าเด็กคนนี้จัดอยู่ในกลุ่มพัฒนาการดี / พัฒนาการปกติ / พัฒนาการช้า (คำตอบมีหลายกลุ่ม เรียกว่า Multiclass classification)\\n2.  Unsupervised learning: คือปัญหาที่ยังไม่มีใครรู้ว่าคำตอบที่ถูกต้องคืออะไร\\n    * Clustering: จัดกลุ่มข้อมูล เช่น มีข้อมูลผู้บริจาค ต้องการจัดกลุ่มผู้บริจาคเป็น 3 กลุ่ม เพื่อออกแบบกลยุทธ์การสื่อสารกับลูกค้าแต่ละกลุ่มที่ต่างกันออกไป\\n    * Non-clustering: ปัญหาอื่นๆ ที่ไม่ใช่การจัดกลุ่ม เช่น ตรวจหาข้อมูลชิ้นที่มีความผิดปกติจากพวก (Anomaly detection) แนะนำข้อมูลที่ผู้ใช้น่าจะสนใจ (Recommendation system) เป็นต้น\\n\\nนอกจากนี้ยังมี ML ประเภทอื่นๆ สำหรับงานเฉพาะด้าน เช่น Reinforcement learning คือการเรียนรู้ที่มีกลไกการเสริมแรงเพื่อให้คอมพิวเตอร์มีพฤติกรรมที่เราต้องการ เป็นต้น\\n\\nตอนนี้เรารู้จักแล้วว่า Machine learning คืออะไร มีกี่ประเภท เอาไปทำอะไรได้บ้าง และถ้าจะทำ ML ต้องมีความรู้และทักษะอะไรบ้าง ในตอนต่อไปจะเริ่มอธิบายกลไกการทำงานของ ML อย่างง่ายที่สุด คือ Linear regression\\n\\n## บทที่ 2 Linear Regression Algorithm\\nวิธีที่ดีที่สุดในการทำความเข้าใจ Machine learning คือการอธิบายพร้อมกับตัวอย่าง โดยเราจะเริ่มที่ Algorithm ที่ง่ายที่สุด นั่นคือ Linear regression\\nเราจะใช้ข้อมูลตัวอย่างจาก [Kaggle](https://www.kaggle.com/venjktry/simple-linear-regression/data) ซึ่งแสดงความสัมพันธ์ระหว่างตัวแปร x บนแกนนอนกับตัวแปร y บนแกนตั้ง ถ้ามองด้วยตาเราจะเห็นว่า x กับ y มีความสัมพันธ์ที่เรียกว่า Positive correlation อย่างชัดเจน นั่นคือยิ่ง x มีค่ามาก y ก็มีค่ามากตามไปด้วย เราจะเห็นความสัมพันธ์ลักษณะในโลกจริงได้มากมาย ตัวอย่างเช่น ยิ่งจำนวนรถมาก เวลาที่ใช้บนถนนต่อการเดินทางหนึ่งเที่ยวก็มีแนวโน้มจะมากขึ้นด้วย\\n\\nปัจจุบัน ชุดข้อมูลนี้มีค่า x และ y จำนวน 999 คู่ ตัวอย่างเช่น คู่แรก x = 24, y = 21.54 ส่วนคู่ที่สอง x = 50, y = 47.46 เราเรียกข้อมูลแต่ละคู่เช่นนี้ว่าข้อมูลหนึ่งรายการ\\n\\nจริงอยู่ที่เรามีข้อมูลถึง 999 รายการ แต่ถ้าเราเก็บข้อมูลมาใหม่เป็นรายการที่ 1000 โดยข้อมูลรายการนี้รู้แค่ค่า x เราจะรู้ได้อย่างไรว่าค่า y เป็นเท่าไร\\n\\nตัวอย่างเช่น ถ้าข้อมูลใหม่มี x = 22 เราอาจจะลองลากเส้นแนวดิ่งจากแกน x ที่ 22 แล้วไปบรรจบตั้งฉากกับแกน y ซึ่งดูเหมือนจะได้ประมาณ 20 แต่ช้าก่อน มันอาจจะเป็น 18, 25 หรือแม้กระทั่ง 30 ก็ได้ เพราะข้อมูลตัวอย่างของเรามีรายการที่ x มีค่าประมาณ 22 อยู่หลายรายการ ซึ่งแต่ละรายการก็มีค่า y ไม่เท่ากันเลย\\n\\nMachine learning จึงมาช่วยเราได้ โดยการใช้ Algorithm ซึ่งก็คือสูตรและกลไกทางคณิตศาสตร์ มาเรียนรู้ข้อมูลที่เรามี เพื่อสร้างโมเดลของข้อมูลขึ้นมา เราจะใช้โมเดลที่สร้างขึ้นนี้ ในการพยากรณ์คำตอบ ซึ่งก็คือค่า y ที่เราต้องการ\\n\\nซึ่งในกรณีนี้ โมเดลที่ได้คือ:\\n\\n```tex\\n\\\\hat{y} = 1.00065638x - 0.10726546\\\\tag{1}\\n```\\n\\nดังนั้นถ้าเราลองใส่ค่า x = 22 ลงไป จะได้ y = 21.90\\n\\nคำถามคือ โมเดลนี้ได้มาอย่างไร คำตอบอยู่ในส่วนต่อไป\\n\\n### Hypothesis function\\n\\nหัวใจของโมเดลพยากรณ์ คือฟังก์ชันคณิตศาสตร์ที่เป็นตัวแทนความสัมพันธ์ระหว่างตัวแปร x กับคำตอบ y เราเรียกฟังก์ชันนี้ว่า Hypothesis function ซึ่งหมายถึงฟังก์ชันที่เป็น \"สมมุติฐาน\" ว่าข้อมูลมีความสัมพันธ์กันอย่างไรนั่นเอง โดยในปัญหา Linear regression นั้น Hypothesis function จะอยู่ในรูปแบบ:\\n\\n```tex\\n\\\\hat{y} = wx + b\\\\tag{2}\\n```\\n\\n*  `\\\\hat{y}` คือค่าคำตอบที่พยากรณ์ได้\\n*  `w` คือค่าน้ำหนักของตัวแปร x (มาจากคำว่า Weight) ซึ่งกำหนดความชันของฟังก์ชัน\\n*  `x` คือตัวแปร x ที่เรารู้ค่า\\n*  `b` คือจุดตัดแกน y (มาจากคำว่า Bias)\\n\\nฟังก์ชันนี้อยู่ในรูปแบบสมการเส้นตรง นั่นคือค่า x ที่ใส่ลงไป จะมีความสัมพันธ์กับค่า y แบบที่ลากเป็นเส้นตรงได้\\n\\nหน้าที่ของ Machine learning คือการหาตัวแปร w และ b ซึ่งจะประกอบกันเป็น Hypothesis function ที่มีความคลาดเคลื่อนน้อยที่สุดเมื่อเทียบกับข้อมูลที่เรามี ซึ่งในกรณีตัวอย่างนี้ เราได้ w = 1.00065638 และ b = -0.10726546\\n\\nเรารู้ได้อย่างไรว่า w และ b ต้องมีค่าเท่าใดจึงจะทำให้โมเดลมีความคลาดเคลื่อนน้อยที่สุด คำตอบมีอยู่ 2 ส่วน ส่วนแรกคือเราต้องมีวิธีการวัดความคลาดเคลื่อน ซึ่งเรียกว่า Cost function ส่วนที่สองคือเราต้องมีวิธีการหาค่า w และ b ที่จะทำให้ค่าความคลาดเคลื่อนใน Cost function นั้นต่ำที่สุด เรียกกระบวนการนี้ว่า Gradient descent\\n\\n### Cost function\\n\\nสำหรับ Hypothesis function แต่ละฟังก์ชัน จะมี Cost function คู่อยู่เสมอ ซึ่งทำหน้าที่วัดว่าค่า y ที่พยากรณ์ได้จาก Hypothesis function มีความคลาดเคลื่อนกับค่า y ที่เรามีข้อมูลอยู่จริงเท่าใด โดยสำหรับ Linear regression เราใช้ฟังก์ชันต่อไปนี้เป็น Cost function:\\n\\n`J(w, b) = \\\\frac{1}{2m} \\\\sum\\\\limits_{i = 1}^m(\\\\hat{y}_i - y_i)^2\\\\tag{3}`\\n\\n*  `J(w, b)` คือผลความต่างระหว่างค่าพยากรณ์กับค่าจริง ที่สอดคล้องกับตัวแปร w และ b\\n*  `m` คือจำนวนรายการข้อมูลทั้งหมดที่มี (เช่นในกรณีตัวอย่างนี้ m = 699 มาจากข้อมูล 999 รายการ แต่กัน 300 รายการเอาไว้ทดสอบความแม่นยำ เรื่องนี้จะอธิบายภายหลัง)\\n*  `\\\\sum\\\\limits_{i = 1}^m` คือผลรวมของรายการถัดไป โดยรวมจากรายการที่ 1 ถึงรายการที่ m\\n*  `(\\\\hat{y}_i - y_i)^2` คือผลต่างระหว่างค่า y ที่พยากรณ์ได้ กับค่า y จริง ของแต่ละรายการ แล้วนำไปยกกำลัง 2 ตัวอย่างเช่น ถ้ารายการแรก พยากรณ์ได้ 20 แต่ในความเป็นจริงต้องเป็น 21.54 จะได้ผลต่างยกกำลัง 2 เท่ากับ `(20-21.54)^2 = 2.37`\\n\\nอนึ่ง ฟังก์ชันนี้ มีชื่อเรียกเฉพาะตัว ว่า Mean Squared Error หรือ MSE\\n\\nเป้าหมายของ Algorithm คือการทำให้ `J(w, b)`  มีค่าต่ำที่สุดเท่าที่จะเป็นไปได้ เราเรียกจุดนี้ว่า Global minimum ซึ่งหาได้โดยใช้กระบวนการที่เรียกว่า Gradient descent\\n\\n### Gradient descent\\n\\nทบทวนอีกครั้งว่าเรามี Cost function ที่กำหนดความสัมพันธ์ระหว่างตัวแปร w, b กับความคลาดเคลื่อนของโมเดล ในโมเดลที่ข้อมูลมีมิติเดียว (คือมี x เพียงหนึ่งตัวสำหรับข้อมูลแต่ละรายการ แต่ละ x ก็มี w เพียงตัวเดียวเป็นค่าน้ำหนัก) รูปร่างของ Cost function จะมีลักษณะเป็นเหมือนชาม คือมีจุดต่ำที่สุด ซึ่งจุดนั้นเป็นจุดที่เราต้องการเนื่องจากจะทำให้ได้ค่าความคลาดเคลื่อนที่ต่ำที่สุด หน้าที่ของ Gradient descent คือการหาตัวแปร w และ b ที่จะทำให้ได้ J ที่ต่ำที่สุด\\n\\nในที่นี้เราจะพิจารณาเฉพาะ Cost function ของ `J(w)`  ซึ่งผันแปรตามตัวแปร w เพียงตัวเดียวก่อน\\n\\nการทำงานของ Gradient descent ใช้ประโยชน์จากข้อเท็จจริงที่ว่า ในแต่ละจุดของเส้นฟังก์ชัน `J(w)` เราจะสามารถลากเส้นไปแตะเพื่อชี้วัด \"ความชัน\" ของจุดนั้นๆ ได้ จากภาพเราจะเห็นว่าจุดที่ 1 มีความชันมากกว่าจุดที่ 2 และจุดที่ 2 ก็ชันมากกว่าจุดที่ 3 จากคุณสมบัตินี้ หากเรามีกลไกที่จะหาจุดที่มีความชันน้อยลงเรื่อยๆ ไปจนถึงจุดที่แทบไม่มีความชันเลย (จุดที่ 3) เราก็จะได้ Global minimum ที่เราต้องการ\\n\\nการหาความชันทำได้ด้วยวิชา Calculus โดยการหาอนุพันธ์ (Derivative) ของฟังก์ชัน  เมื่อเปรียบเทียบกับ w ซึ่งแทนเป็นสัญลักษณ์ได้ด้วย:\\n\\n```tex\\n\\\\frac{\\\\partial J(w)}{\\\\partial w}\\\\tag{4}\\n```\\n\\nจากวิชา Calculus เราหาอนุพันธ์ของฟังก์ชันดังกล่าว จะได้ Algorithm ทั่วไปคือ:\\n\\nทำซ้ำจนกระทั่งผลลัพธ์ล่าสุดไม่เปลี่ยนแปลงจากผลลัพธ์ครั้งก่อน:\\n\\n```tex\\n\\\\theta_j := \\\\theta_j - \\\\alpha \\\\frac{\\\\partial}{\\\\partial\\\\theta_j} J(\\\\theta_0, \\\\theta_1)\\\\tag{5}\\n```\\n\\n*  `\\\\theta` อ่านว่า Theta คือตัวแปรต่างๆ ของโมเดล เช่น b, w\\n*  `j` คือลำดับที่ของตัวแปร โดยจุดตัดแกน y หรือ b อยู่ลำดับที่ 0 ส่วน w อยู่ลำดับที่ 1\\n*  `\\\\alpha` อ่านว่า Alpha คือค่าคงที่ที่กำหนดอัตราความเร็วของการเคลื่อนที่ไปสู่จุดที่ความชันน้อยลง\\n\\nจาก Algorithm ทั่วไปนี้ นำมาประยุกต์กับ Linear regression จะได้ว่า:\\n\\nทำซ้ำจนกระทั่งผลลัพธ์ล่าสุดไม่เปลี่ยนแปลงจากผลลัพธ์ครั้งก่อน:\\n\\n```tex\\nb := b - \\\\alpha \\\\frac{1}{m} \\\\sum\\\\limits_{i = 1}^m(\\\\hat{y_i} - y_i)\\\\tag{6} \\n\\nw := w - \\\\alpha \\\\frac{1}{m} \\\\sum\\\\limits_{i = 1}^m(\\\\hat{y_i} - y_i)x_i)\\\\tag{7}\\n```\\n\\nมาลองทำความเข้าใจ Algorithm นี้ด้วยตัวอย่าง สมมุติว่าเราเริ่มคำนวนด้วยค่าตั้งต้นดังนี้: ;  ถ้าเราหาอนุพันธ์ครั้งแรกได้ 1 ครั้งที่สองได้ 0.7 และครั้งที่สามได้ 0.4 เราจะได้:\\n\\n```tex\\n2 := 2 - (0.5)1 = 1.5\\\\tag{8}\\n\\n1.5 := 1.5 - (0.5)0.7 = 1.15\\\\tag{9}  \\n\\n1.15 := 1.15 - (0.5)0.4 = 0.95\\\\tag{10}\\n```\\n\\nจะเห็นว่า ผลลัพธ์ค่า b ที่ได้มาค่าน้อยลงเรื่อยๆ คือจาก 1.5 เป็น 1.15 เป็น 0.95 โดยผลลัพธ์ที่ได้แต่ละรอบจะส่งผลให้ความชันของ Cost function ลดลงเรื่องๆ ทำให้อนุพันธ์น้อยลงเรื่อยๆ จาก 1 เป็น 0.7 เป็น 0.4 เป็นต้น\\n\\nเป้าหมายของกระบวนการ Gradient descent นี้คือการหาค่าตัวแปรของโมเดลที่ทำให้โมเดลมีผลลัพธ์ y ที่คลาดเคลื่อนกับค่า y ที่แท้จริงให้น้อยที่สุด นั่นคือเราจะได้โมเดลที่เป็นตัวแทนของชุดข้อมูลนี้ได้ดีที่สุดนั่นเอง\\n\\nถ้าสังเกตดีๆ จะเห็นว่า เราสามารถควบคุมกระบวนการนี้ได้ด้วยการกำหนดตัวแปรสองสิ่ง เราเรียกตัวแปรที่เราเป็นผู้กำหนดเองนี้ว่า Hyperparameter ซึ่งในกรณีนี้ ประกอบด้วย:\\n\\n*  `N_{iteration}` จำนวนครั้งที่เราคำนวนและอับเดตค่าใน Algorithm เช่น 100 ครั้ง หมายความว่าจะมีการคำนวนเพื่อลดความชันลงเรื่อยๆ 100 รอบ โดยถ้าจำนวนครั้งนี้น้อยเกินไป Algorithm ก็จะยังคำนวนไปไม่ถึงจุดที่จะได้ Cost ที่ต่ำที่สุด แต่ถ้าจำนวนครั้งนี้มากเกินไป ก็จะเสียเวลาและพลังงานในการคำนวนมากโดยไม่จำเป็น เพราะรอบหลังๆ ที่ได้ทำให้ Cost function เปลี่ยนแปลง\\n*  `\\\\alpha` เรียกว่า Learning rate หรืออัตราความเร็วในการอับเดตค่าความชัน (อนุพันธ์ของ J เมื่อเทียบกับตัวแปร) ถ้า Learning rate มีค่ามาก จะทำให้ค่าความชันส่งผลไปยังค่าของตัวแปรมากขึ้น ทำให้โมเดลหาจุด Global minimum ได้เร็วขึ้น ในทางกลับกัน ถ้า Learning rate น้อย ตัวแปรก็จะเปลี่ยนแปลงช้าลง\\n\\nทีนี้ดูเหมือนกับว่า ถ้าเรากำหนด Learning rate สูงๆ ก็จะทำให้ได้ค่า Global minimum ของ Cost function เร็วขึ้น อย่างไรก็ตามถ้าค่า Learning rate สูงเกินไป อาจจะทำให้โมเดลหา Global minimum ไม่เจอ เพราะในขั้นท้ายๆ ของ Gradient descent ตัวแปรอาจจะกระโดดเลยจุดต่ำสุดของ J ไปอีกด้านหนึ่ง\\n\\n### Normal equation\\n\\nนอกจากวิธี Gradient descent แล้ว เรายังมีวิธีการหา Parameter  ที่ให้ Cost ที่ต่ำที่สุดอีกวิธี เราเรียกวิธีนี้ว่า Normal equation โดยเพียงใช้สมการ:\\n\\n```tex\\n\\\\theta = (X^T X)^{-1} X^T y \\\\tag{11}\\n```\\n\\nNormal equation ทำงานได้ดีและเร็วเมื่อจำนวน Feature ไม่มาก ผู้สนใจสามารถหาอ่านวิธีการ Derive ให้ได้มาซึ่ง Normal equation นี้ด้วยตนเอง\\n\\nตอนนี้เรามีความรู้และเข้าใจขั้นตอนการสร้างโมเดล Linear regression แล้ว ตอนถัดไปเราจะใช้ตัวอย่างนี้มาสร้างโมเดลจริงกัน\\n\\n## บทที่ 3 Linear Regression Programming\\n\\nเมื่อเราเข้าใจแล้วว่า Linear regression algorithm ทำงานอย่างไร ทีนี้ก็มาลองสร้างโมเดลพยากรณ์กัน\\n\\nโดยทั่วไป การสร้างโมเดลมีขั้นตอนต่อไปนี้:\\n\\n1.  เตรียมข้อมูล\\n2.  Import Library ที่จะใช้งาน และโหลดข้อมูล\\n3.  สำรวจข้อมูล\\n4.  ฝึกโมเดล\\n5.  ทดสอบความแม่นยำของโมเดล\\n6.  นำไปใช้พยากรณ์\\n\\nโดยจะแนะนำทีละขั้นตอน\\n\\n### เตรียมข้อมูล\\n\\nข้อมูลที่จะนำมาสร้างโมเดล Machine learning มีที่มาหลากหลาย ซึ่งเราสามารถใช้ Python ดึงข้อมูลจากหลากหลายรูปแบบมาใช้ได้ ในขั้นนี้เราจะเน้นการเตรียมข้อมูลให้อยู่ในรูปแบบข้อมูลที่มีโครงสร้าง (Structured data) ที่นำเสนอในรูปแบบตาราง\\n\\n|     | x   | y   |\\n| --- | --- | --- |\\n| 0   | 24  | 21.549452 |\\n| 1   | 50  | 47.464463 |\\n| 2   | 15  | 17.218656 |\\n\\nหลักการเตรียมข้อมูล คือให้สร้างข้อมูลใน Spreadsheet โดยแนวนอนคือตัวอย่างแต่ละรายการ ส่วนแนวตั้งคือข้อมูลของตัวอย่างแต่ละรายการ เราเรียกข้อมูลแนวตั้ง ว่า Feature ซึ่งในกรณี จะเห็นว่ามี 1 Feature คือ x ส่วนคอลัมน์สุดท้าย เราเก็บไว้สำหรับคำตอบ ซึ่งเราต้องการหาเมื่อนำโมเดลมาพยากรณ์จากข้อมูลใหม่ เราเรียกคอลัมน์คำตอบนี้ว่า Label\\n\\nตัวอย่างเช่น ข้อมูลชิ้นที่ 1 มี Feature x คือ 50 ซึ่งเชื่อมโยงกับ Label y คือ 47.464463 เราจะฝึกโมเดลให้เรียนรู้ความสัมพันธ์ระหว่าง Feature กับ Label เพื่อให้ได้สมมุติฐานที่ดีที่สุด ว่า Feature กับ Label สัมพันธ์กันอย่างไร เมื่อได้สมมุติฐานนี้ก็จะสามารถพยากรณ์ Label จากข้อมูลชิ้นใหม่ที่รู้แค่ Feature ได้\\n\\nอนึ่ง เราไม่จำเป็นต้องใส่ลำดับที่ของข้อมูลในคอลัมน์แรกอย่างที่เห็นในตัวอย่าง เพราะเดี๋ยว Library pandas จะสร้างดัชนีเหล่านี้ให้เราเอง\\n\\nเมื่อเตรียมข้อมูลเสร็จ ให้ Save เป็น .csv ซึ่งเป็นรูปแบบมาตรฐานสำหรับข้อมูลแบบตาราง\\n\\n### Import Library ที่จะใช้งาน และโหลดข้อมูล\\n\\nขั้นต่อไปเราต้องตัดสินใจว่า เราจะสำรวจข้อมูลและสร้างโมเดลด้วยเครื่องมือใด เพื่อที่จะได้โหลด Library ของ Python ที่ต้องใช้เข้ามา ซึ่งสำหรับตัวอย่างนี้เราจะใช้เครื่องมือดังนี้\\n\\n* numpy เป็น Library มาตรฐานสำหรับการคำนวนและจัดการกับ Vector และ Matrix หลากมิติ ต้องใช้ในการแปลงข้อมูลให้ ML Framework ของเราอ่านได้\\n* pandas เป็น Library สำหรับโหลดจัดการ และวิเคราะห์ข้อมูล Structured data เช่นตาราง csv ต้องใช้เพื่อโหลดข้อมูลของเราและทำการสำรวจข้อมูลโดยใช้เครื่องมือทางสถิติ\\n* matplotlib เป็น Library ที่เอาไว้แปลงข้อมูลเป็นกราฟ เพื่อแสดงผลให้เห็นเข้าใจง่าย และเพื่อช่วยในการวิเคราะห์และตัดสินใจ\\n* scikit-learn เป็น Machine learning framework library มีเครื่องมือและ Algorithm ที่เกี่ยวข้องกับการฝึกโมเดลและตรวจสอบความแม่นยำ\\n\\nเราเลือกใช้ scikit-learn เพราะเป็น Library ที่ออกแบบมาดี ใช้งานง่าย สามารถเลือก Algorithm ได้หลากหลายโดยมีวิธีการฝึกคล้ายๆ กัน การใช้ Framework อย่าง scikit-learn จะช่วยย่นเวลาและลดโอกาสที่จะเกิดข้อผิดพลาดได้มาก เมื่อเทียบกับการเขียน Algorithm ขึ้นมาเอง\\n\\nแนะนำให้ไปอ่านคู่มือและคำแนะนำการใช้แต่ละ Library เพื่อทำความคุ้นเคย จะได้ใช้งานได้คล่องและประยุกต์กับสถานการณ์ได้หลากหลาย\\n\\nเมื่อตัดสินใจได้แล้วเราก็เริ่ม Import Library เหล่านี้ได้:\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom sklearn.linear_model import LinearRegression\\n```\\n    \\n\\nสำหรับใครที่มาถึงจุดนี้แล้วไม่เข้าใจว่าต้องพิมพ์โค้ดนี้ที่ไหน อย่างไร และแต่ละบรรทัดหมายความว่าอะไร แนะนำให้หยุดก่อน แล้วไปศึกษา Python ขั้นพื้นฐาน ซึ่งปัจจุบันหาอ่านได้ทั่วไปบนเว็บหรือจะเรียนตามคอร์สออนไลน์ต่างๆ ก็ได้\\n\\nต่อมาเราจะเริ่มโหลดข้อมูล โดยใช้ pandas:\\n\\n```python\\n# Load the data\\n# Data from https://www.kaggle.com/venjktry/simple-linear-regression/data\\ndata_train = pd.read_csv(\"data/lr_train.csv\")\\ndata_test = pd.read_csv(\"data/lr_test.csv\")\\n```\\n\\nข้อสังเกตที่สำคัญในขั้นนี้ คือจากตัวอย่างเราจะเห็นว่าได้แบ่งข้อมูลเป็นสองไฟล์ คือ `lr_train.csv` และ `lr_test.csv` การแบ่งนี้เกิดขึ้นเพราะในกระบวนการสร้างโมเดล เราจะต้องกันข้อมูลส่วนหนึ่งเอาไว้ไม่ให้โมเดลได้เห็นก่อน เพื่อจะได้เอาโมเดลที่ได้ไปทดลองพยากรณ์กับข้อมูลที่กันเอาไว้ เพื่อทดสอบความแม่นยำ โดยปกติเราจะใช้สัดส่วน Train set: Test set ประมาณ 80:20 แต่ถ้าหากมีข้อมูลหลายรายการมาก เช่นเป็นหลักล้านขึ้นไป ก็สามารถเพิ่มสัดส่วนเป็น 90:10 หรือไปจนถึง 99:1 ก็ได้ เพราะโดยทั่วไปเราต้องการข้อมูลที่เอาไว้ฝึกโมเดลที่มีจำนวนมากที่สุดเท่าที่จะทำได้ ส่วนข้อมูลที่เอาไว้ทดสอบ ถ้าปริมาณข้อมูลรวมมีเยอะมาก ข้อมูลทดสอบก็มีจำนวนมากเพียงพอถึงแม้สัดส่วนจะน้อยก็ตาม\\n\\nในกรณีของเรา Train set มี 699 รายการ ส่วน Test set มี 300 รายการ คิดเป็นสัดส่วนประมาณ 70:30\\n\\nสิ่งสำคัญอีกประการคือการแบ่งข้อมูลออกเป็น Train set และ Test set จะต้องแบ่งแบบสุ่ม ไม่ใช่การตัดเอาส่วนต้นมาเป็น Train set และที่เหลือเป็น Test set เพราะเราต้องการให้ข้อมูลทั้งสองชุดเป็นตัวแทนของรายการข้อมูลที่มี Feature และ Label ที่หลากหลาย กระจายตัวออกตามธรรมชาติ อนึ่ง ในความเป็นจริงเราไม่จำเป็นต้องแบ่งข้อมูลออกเป็นสองไฟล์ ให้ใช้ไฟล์เดียว แล้วใช้ฟังก์ชัน `train_test_split` ของ scikit-learn สุ่มแบ่งให้โดยอัตโนมัติได้\\n\\nข้อมูลที่โหลดมา ให้เก็บไว้ใน Object โดยในที่นี้ Train set เก็บไว้ใน `data_train` ส่วน Test set เก็บไว้ใน `data_test`\\n\\n### สำรวจข้อมูล\\n\\nเมื่อเราโหลดข้อมูลลงใน Object แล้ว เราก็จะสำรวจข้อมูลที่มี โดยใช้ Method ต่างๆ ของ pandas object ดังนี้:\\n\\n```python\\n# Explore the data\\nprint(\"See the data sample:\\\\n\")\\nprint(data_train.head())\\nprint(\"See the summary of features:\\\\n\")\\nprint(data_train.info())\\nprint(\"Calculate basic stat of each feature:\\\\n\")\\nprint(data_train.describe())\\n```\\n\\n`data_train.head()` จะแสดงข้อมูล 5 รายการแรกให้เราดู:\\n\\n|     | x   | y   |\\n| --- | --- | --- |\\n| 0   | 24  | 21.549452 |\\n| 1   | 50  | 47.464463 |\\n| 2   | 15  | 17.218656 |\\n| 3   | 38  | 36.586398 |\\n| 2   | 87  | 87.288984 |\\n\\n`data_train.info()` จะแสดงจำนวนและประเภทของข้อมูลแต่ละคอลัมน์ และข้อมูลอื่นๆ เช่นจำนวน Memory ที่ใช้:\\n```\\n    RangeIndex: 699 entries, 0 to 698\\n    Data columns (total 2 columns):\\n    x    699 non-null int64\\n    y    699 non-null float64\\n    dtypes: float64(1), int64(1)\\n    memory usage: 11.0 KB\\n```\\n\\nเราจะเห็นว่ามีข้อมูล 699 รายการ โดยจำนวนรายการของ x และ y จะต้องเท่ากัน เพื่อให้ Algorithm สามารถจับคู่คำนวนได้ถูกต้อง นอกจากนี้ ให้สังเกตว่า x เป็นข้อมูลในรูปแบบ int64 คือจำนวนเต็ม ส่วน y เป็นประเภท float64 คือจำนวนที่มีทศนิยม\\n\\nต่อมา `data_train.describe()` จะแสดงสถิติพื้นฐานของข้อมูลแต่ละคอลัมน์:\\n\\n|     | x   | y   |\\n| --- | --- | --- |\\n| count | 699.000000 | 699.000000 |\\n| mean | 50.014306 | 49.939869 |\\n| std | 28.954560 | 29.109217 |\\n| min | 0.000000 | -3.839981 |\\n| 25% | 25.000000 | 24.929968 |\\n| 50% | 49.000000 | 48.973020 |\\n| 75% | 75.000000 | 74.929911 |\\n| max | 100.000000 | 108.871618 |\\n\\nใครที่อ่านสถิติขั้นพื้นฐานเป็น จะพบว่าทั้ง x และ y มีลักษณะคล้ายกันมาก เช่น มีค่า Mean และส่วนเบี่ยงเบนมาตรฐานใกล้เคียงกัน มีการกระจายตัวแบบ Normal distribution มีค่าต่ำสุดและสูงสุดใกล้กัน ซึ่งถ้าเราเห็นข้อมูลแบบนี้ก็อาจจะเริ่มนึกออกว่า x และ y น่าจะมีความสัมพันธ์อย่างใกล้ชิดกันมาก (มี Correlation สูง) ซึ่งจะยืนยันได้ด้วยการพล็อตกราฟแบบ Scatter plot โดยใช้ matplotlib:\\n\\n```python\\n# Visualise the data\\nplt.figure(figsize=(12,9))\\nplt.scatter(data_train[\"x\"], data_train[\"y\"])\\nplt.xlabel(\"x\")\\nplt.ylabel(\"y\")\\n```\\n    \\nซึ่งยืนยันการวิเคราะห์ของเราจากการอ่านสถิติในขั้นตอนที่แล้ว ว่า x กับ y มีความสัมพันธ์เป็นเส้นตรงซึ่งกันและกัน ซึ่งทำให้เราตัดสินใจเลือกใช้ Linear regression algorithm ในการสร้างโมเดล เพราะเป็นโมเดลที่มีสมมติฐานว่าความสัมพันธ์ระหว่าง Feature กับ Label เป็นเส้นตรง\\n\\nอนึ่งจะอธิบายไวยากรณ์ของโค้ดด้านบนเพื่อความเข้าใจที่กระจ่าง:\\n\\n* `plt.figure(figsize=(12,9))` สร้างกราฟว่างๆ ขนาด 12 x 9 เพื่อเป็นพื้นที่ให้เราใส่ข้อมูล\\n* `plt.scatter(data_train[\"x\"], data_train[\"y\"])` สั่งให้พล็อตกราฟแบบ Scatter plot โดยกำหนด Argument คือ `(ข้อมูลสำหรับแกน x, ข้อมูลสำหรับแกน y)` ซึ่งเราจะดึงข้อมูลจาก pandas object ที่โหลดมาไว้แล้ว โดยข้อมูลแกน x กำหนดให้เป็น `data_train[\"x\"]` ซึ่งแปลว่า \"ให้เลือกข้อมูลทั้งหมดจากคอลัมน์ที่ชื่อ \"x\"\" ส่วนข้อมูลสำหรับแกน y ก็ทำลักษณะเดียวกัน\\n* `plt.xlabel(\"x\")` และ `plt.ylabel(\"y\")` กำหนดชื่อป้ายบนแกน x และ y ว่า \"x\" และ \"y\" ตามลำดับ\\n\\nเมื่อเราเห็นข้อมูลแล้วตัดสินใจเลือก Algorithm แล้ว ก็ถึงเวลาสร้างโมเดลจริงๆ เสียที\\n\\n### ฝึกโมเดล\\n\\nเราจะสร้างโมเดลด้วยการเรียกใช้คลาส `LinearRegression` จากโมดูล `linear_model` ของ scikit-learn library\\n\\nแต่ก่อนอื่นเราต้องเข้าใจก่อนว่า scikit-learn กำหนดว่าข้อมูลที่จะป้อนเข้าสู่ Algorithm จะต้องเป็น Array ที่มีมิติชัดเจนทั้งด้านแถวและด้านคอลัมน์ เช่น ถ้า Feature มี 1 Feature คือ x และแต่ละ x มี 699 รายการ มิติของข้อมูลชุดนี้ต้องเป็น (699,1)\\n\\nซึ่งปัญหาจะเกิดขึ้นตรงนี้ ว่า Object ข้อมูลที่เราโหลดเข้ามา อาจไม่ได้มีมิติครบทั้งสองด้าน เช่น pandas object `data_train` ของเรา มีมิติ (699,) คือกำหนดมิติชัดเจนแค่ด้านแถว ส่วนด้านคอลัมน์ไม่ได้กำหนด เราจึงต้องแปลงข้อมูลให้มีมิติครบทั้งสองด้านเสียก่อน\\n\\nวิธีการแปลงคือ เราจะแปลง pandas object เป็น numpy array โดยในกรณีของเราทำดังนี้:\\n\\n```python\\n# Format X_train and y_train into 2D vector\\nX_train = np.array(data_train[\"x\"]).reshape(-1,1)\\ny_train = np.array(data_train[\"y\"]).reshape(-1,1)\\nX_test = np.array(data_test[\"x\"]).reshape(-1,1)\\ny_test = np.array(data_test[\"y\"]).reshape(-1,1)\\n\\nprint(\"X_train shape = \" + str(X_train.shape))\\nprint(\"y_train shape = \" + str(y_train.shape))\\nprint(\"X_test shape = \" + str(X_test.shape))\\nprint(\"y_test shape = \" + str(y_test.shape))\\n```\\nMethod `.reshape(-1,1)` เป็นการเติมมิติให้ด้านคอลัมน์ ผลที่ได้คือ:\\n```python\\nX_train shape = (699, 1)\\ny_train shape = (699, 1)\\nX_test shape = (300, 1)\\ny_test shape = (300, 1)\\n```\\nซึ่งเป็นรูปแบบข้อมูลที่พร้อมจะนำเข้า Algorithm ให้ฝึกโมเดล\\n\\nต่อมาคือขั้นตอนที่สำคัญที่สุด แต่สั้นที่สุด นั่นคือการฝึกโมเดลด้วย Linear regression algorithm:\\n\\n```python\\n# Train the data\\nlr = LinearRegression().fit(X_train, y_train)\\nprint(\"LR coefficient is\", lr.coef_)\\nprint(\"LR intercept is\", lr.intercept_)\\n```\\nผลที่ได้คือ:\\n* `lr = LinearRegression().fit(X_train, y_train)` เราเรียกฟังก์ชัน `LinearRegression()` แล้วใช้ Method `.fit` ป้อน Vector ของ feature และ Vector ของ label ลงไปในฟังก์ชัน โดยฟังก์ชันจะใช้ข้อมูลเหล่านี้มาคำนวนหา Parameter ที่จะลด Cost function ให้มีค่าต่ำที่สุด โดย Parameter สำหรับ Linear regression ก็คือ Coefficient (= w) และ Intercept (= b) อยู่ใน Hypothesis function ของ Linear regression:\\n\\n`\\\\hat{y} = wx + b\\\\tag{1}`\\n\\n* จากนั้นเราพิมพ์ Parameter ทั้งสองตัวมาดู ซึ่งเมื่อเอามาใส่ใน Hypothesis function ก็จะได้สมการเส้นตรงที่เป็นโมเดลของชุดข้อมูลนี้ นั่นก็คือ:\\n\\n`\\\\hat{y} = 1.00065638x - 0.10726546\\\\tag{2}`\\n\\nซึ่งทำให้เราสามารถพยากรณ์ค่า y ได้ด้วยการใส่ค่า x ลงไปตามที่ต้องการ\\n\\nแต่ก่อนที่เราจะนำโมเดลไปใช้ เราต้องมั่นใจเสียก่อนว่าโมเดลของเราเป็นโมเดลที่ดี นั่นคือมีความแม่นยำสูงพอที่เราจะเชื่อได้ว่าเป็นตัวแทนข้อมูลชุดนี้ได้\\n\\n### ทดสอบความแม่นยำของโมเดล\\nหนึ่งในข้อสงสัยที่แทบทุกคนจะถามเมื่อกำลังพิจารณาจะนำ Machine learning มาแก้ปัญหาของตน ก็คือจะรู้ได้อย่างไรว่าโมเดลจะมีความแม่นยำ คำตอบคือมีวิธีการทดสอบและชี้วัดความแม่นยำของโมเดลหลากหลายวิธี ซึ่งแต่ละวิธีก็จะเหมาะสมกับ Algorithm และสถานการณ์ที่แตกต่างกันออกไป\\n\\nสำหรับ Linear regression เราจะใช้ Method `.score` ของ scikit-learn ในการตรวจสอบความแม่นยำของโมเดล โดยมีวิธีการเรียกใช้ คือ:\\n```python\\n# Evaluate model\\'s accuracy on train and test set\\nprint(\"Train set accuracy = \" + str(lr.score(X_train, y_train)))\\nprint(\"Test set accuracy = \" + str(lr.score(X_test, y_test)))\\n```\\nMethod `.score` นี้จะแตกต่างกันสำหรับแต่ละโมเดล โดย Linear regression จะเรียกใช้วิธีที่เรียกว่า `R^2 Score`  ซึ่งมีสูตรคำนวนดังนี้:\\n\\n```tex\\nR^2 = 1 - \\\\frac{SS_{res}}{SS_{tot}}\\\\tag{3}\\n\\nSS_{res} = \\\\sum\\\\limits_{i = 1}^m(y_i - \\\\hat{y}_i)^2\\\\tag{4}\\n\\nSS_{tot} = \\\\sum\\\\limits_{i = 1}^m(y_i - \\\\bar{y}_i)^2\\\\tag{5}\\n```\\n*  `SS_{res}` คือ Residual sum of squares ชี้วัดความต่างระหว่างค่าจริงกับค่าที่พยากรณ์ได้\\n*  `SS_{tot}` คือ Total sum of squares ชี้วัดความต่างระหว่างค่าจริงกับค่าเฉลี่ยของค่าจริงทั้งหมด\\n* ยิ่ง `R^2`  มีค่าใกล้ 1 แปลว่าโมเดลมีความแม่นยำมาก\\n\\nสำหรับตัวอย่างของเรา ได้ผลดังนี้:\\n```\\nTrain set accuracy = 0.9907015319025518\\nTest set accuracy = 0.9888014444327563\\n```\\nพูดง่ายๆ ก็คือเมื่อเทียบกับ Train set 699 ตัวอย่าง มีความแม่นยำ 99.07% ส่วนเมื่อเทียบกับ Test set 300 ตัวอย่าง มีความแม่นยำ 98.88%\\n\\nสุดท้าย เรามาลองพล็อตกราฟดูว่าโมเดลของเรามีหน้าตาเป็นอย่างไร เมื่อเทียบกับข้อมูลจริง:\\n\\n```python\\n# Make a prediction\\ny_pred = lr.predict(X_test)\\n\\n# Visualise the model VS actual data\\nplt.figure(figsize=(12,9))\\nplt.scatter(X_test, y_test)\\nplt.plot(X_test, y_pred, color=\"red\", linewidth=3)\\nplt.xlabel(\"x\")\\nplt.ylabel(\"y\")\\n```\\n\\nการสร้างกราฟของโมเดล ทำได้โดยการเรียก Method .predict โดยใส่ข้อมูล X_test ลงไป เพื่อให้โมเดลพยากรณ์ค่า y ออกมาเป็นเส้นตรง กำหนดผลที่ได้ลงไปใน Object y_pred จากนั้นเอา y_pred ไปพล็อตในแกน y เป็นเส้นตรงเมื่อเทียบกับ X_test ก็จะได้ฟังก์ชันเส้นตรงของโมเดล\\n\\nจะเห็นว่าเส้นฟังก์ชันโมเดลสีแดงของเรา ผ่ากลางกลุ่มข้อมูลตัวอย่างแบบพอดีๆ เป็นการยืนยันให้เห็นด้วยตา ว่าโมเดลของเราน่าจะมีความแม่นยำสูงที่สุดเท่าที่จะเป็นไปได้\\n\\nเป็นอันว่าเราได้สร้างโมเดลพยากรณ์แบบ Linear regression ตัวแปรเดียว ได้สำเร็จเป็นที่เรียบร้อย ตอนต่อไปจะมาเรียนรู้การสร้างโมเดลแบบ Classification เพื่อจำแนกข้อมูลออกเป็นหมวดต่างๆ กัน\\n\\n## บทที่ 4 Logistic Regression\\n\\nคราวที่แล้วเราได้เรียนรู้ Linear Regression เพื่อสร้างโมเดลพยากรณ์ผลลัพธ์ที่เป็นตัวเลขต่อเนื่อง (Continuous number) คราวนี้เราจะมาสร้างโมเดลสำหรับพยากรณ์หมวดหมู่ หรือที่เรียกว่า Classification กันบ้าง\\n\\nโจทย์ของเราคือการจำแนกสายพันธุ์ของพืชตระกูล Iris ออกเป็น 3 กลุ่ม คือ Sentosa, Versicolor, และ Virginica โดยมีข้อมูลอยู่ 4 Feature คือ ความยาวกลีบเลี้ยง (Sepal length), ความกว้างกลีบเลี้ยง (Sepal width), ความยาวกลีบดอก (Petal length), และความกว้างกลีบดอก (Petal width) โดยทั้งหมดมีหน่วยวัดเป็นเซนติเมตร\\n\\n### Load\\n\\nเราจะใช้ชุดข้อมูล [Iris plants dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) ซึ่งเป็นชุดข้อมูลยอดนิยมในการทดสอบโมเดลการจำแนกแบบหลายหมวดหมู่ โดยชุดข้อมูลนี้ถูกฝังอยู่ใน scikit-learn เรียบร้อยแล้ว สามารถเรียกใช้ได้เลยโดยการเรียก `load_iris()` ฟังก์ชัน ในคลาส `datasets`:\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom sklearn import datasets\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\n\\n# Load the iris data\\niris = datasets.load_iris()\\n```\\n### Explore\\n\\nเมื่อโหลดแล้วลองดูโครงสร้างและคำอธิบายข้อมูล:\\n\\n```python\\nprint(iris.keys())\\nprint(iris[\"DESCR\"])\\nprint(\"Feature names are: \", iris[\"feature_names\"])\\nprint(\"Target names are: \", iris[\"target_names\"])\\n```\\nMethod `.keys` ของ iris object จะเรียกดู Dictionary key ของชุดข้อมูล ซึ่งประกอบด้วย:\\n`dict_keys([\\'data\\', \\'target\\', \\'target_names\\', \\'DESCR\\', \\'feature_names\\', \\'filename\\'])`\\nหมายความว่าถ้าเราเรียกดู Keys เหล่านี้ ก็จะเจอข้อมูลใน Key นั้นๆ เช่น ถ้าเราอยากรู้ว่าหมวดหมู่ที่จะพยากรณ์ มีอะไรบ้าง ก็เรียก `iris[\"target_names\"]` ก็จะได้:\\n \\n `[\\'setosa\\' \\'versicolor\\' \\'virginica\\']`\\n\\nแนะนำให้เรียก `iris[\"DESCR\"]` เพื่อดูคำอธิบายชุดข้อมูลด้วย จะได้เข้าใจที่มาที่ไป ลักษณะ และจำนวนตัวอย่างข้อมูล:\\n\\n```\\nIris plants dataset\\n    --------------------\\n    \\n    **Data Set Characteristics:**\\n    \\n        :Number of Instances: 150 (50 in each of three classes)\\n        :Number of Attributes: 4 numeric, predictive attributes and the class\\n        :Attribute Information:\\n            - sepal length in cm\\n            - sepal width in cm\\n            - petal length in cm\\n            - petal width in cm\\n            - class:\\n                    - Iris-Setosa\\n                    - Iris-Versicolour\\n                    - Iris-Virginica\\n    \\n        :Summary Statistics:\\n    \\n        ============== ==== ==== ======= ===== ====================\\n                        Min  Max   Mean    SD   Class Correlation\\n        ============== ==== ==== ======= ===== ====================\\n        sepal length:   4.3  7.9   5.84   0.83    0.7826\\n        sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n        petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n        petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n        ============== ==== ==== ======= ===== ====================\\n    \\n        :Missing Attribute Values: None\\n        :Class Distribution: 33.3% for each of 3 classes.\\n        :Creator: R.A. Fisher\\n        :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n        :Date: July, 1988\\n    \\n    The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\n    from Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\n    Machine Learning Repository, which has two wrong data points.\\n    \\n    This is perhaps the best known database to be found in the\\n    pattern recognition literature.  Fisher\\'s paper is a classic in the field and\\n    is referenced frequently to this day.  (See Duda & Hart, for example.)  The\\n    data set contains 3 classes of 50 instances each, where each class refers to a\\n    type of iris plant.  One class is linearly separable from the other 2; the\\n    latter are NOT linearly separable from each other.\\n```\\n    \\n\\n### Prepare\\n\\nพอเข้าใจข้อมูลแล้ว ก็ต้องเตรียมข้อมูลให้อยู่ในรูปแบบที่จะนำไปให้โมเดลฝึกได้ ซึ่งโดยหลักการคือการกำหนด Matrix ข้อมูลให้อยู่ในตัวแปร X ส่วน Vector เป้าหมายให้อยู่ในตัวแปร y โดยสำหรับชุดข้อมูลที่ฝังอยู่ใน scikit_learn เราสามารถเรียก Method `.data` และ `.target` ได้เลย\\n```python\\nX = iris.data\\ny = iris.target\\nprint(\"X_shape shape is:\", X.shape)\\nprint(\"y_shape shape is:\", y.shape)\\n```\\nสองบรรทัดสุดท้าย เป็นการตรวจสอบมิติของข้อมูลทั้ง X และ y ซึ่งจะได้:\\n\\n    X_shape shape is: (150, 4)\\n    y_shape shape is: (150,)\\n    \\n\\nแปลว่า X เป็น Matrix ขนาด (150, 4) คือมี 150 แถวเท่ากับจำนวนตัวอย่าง และ 4 คอลัมน์เท่ากับจำนวน Feature ส่วน y เป็น Column vector ขนาด 150 ซึ่งจะต้องเท่ากับจำนวนแถวของ X matrix\\n\\nอนึ่ง เราใช้ตัวพิมพ์ใหญ่ เช่น X เวลาแทน Matrix ส่วน Vector ใช้ตัวพิมพ์เล็กเช่น y เพื่อทำให้ชัดเจนว่าข้อมูลอยู่ในประเภทอะไรทางคณิตศาสตร์\\n\\nจากนั้นเราจะแบ่งข้อมูลออกเป็น Train set กับ Test set โดยการสุ่มด้วยฟังก์ชัน `train_test_split` ในโมดูล `model_selection` โดยฟังก์ชันนี้จะ Return ตัวแปร 4 ตัว ได้แก่ Matrix X เพื่อเทรน, Matrix X เพื่อทดสอบ, Vector Y เพื่อเทรน, และ Vector Y เพื่อทดสอบ ตามลำดับ ดังนั้นให้เรากำหนดตัวแปรทั้ง 4 เมื่อเรียกฟังก์ชันนี้\\n\\n```python\\n# Split the data into train and test set\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\nprint(\"X_train shape is:\", X_train.shape)\\nprint(\"y_train shape is:\", y_train.shape)\\nprint(\"X_test shape is:\", X_test.shape)\\nprint(\"y_test shape is:\", y_test.shape)\\n```\\n    \\n\\nสังเกตว่า `train_test_split` มี Argument `X`, `y` ซึ่งก็คือชุดข้อมูลที่เราเพิ่งเตรียม และมี `random_state` ซึ่งทำให้เราสามารถกำหนดได้การสุ่มแต่ละครั้งได้ผลออกมาเหมือนกัน ซึ่งเป็นประโยชน์ในการทดสอบโมเดล เพราะถ้าเราเรียกฟังก์ชันแต่ละครั้งแล้วผลออกมาไม่เหมือนกัน คือแต่ละครั้งก็สุ่มใหม่ เราจะไม่สามารถควบคุมตัวแปรในการทดสอบโมเดลได้\\n\\nวิธีการกำหนด `random_state` คือการใส่ตัวเลขจำนวนเต็มอะไรก็ได้ลงไป ถ้าดูหนังสือหรือโค้ดคนอื่นจะเห็นว่าบางทีจะใส่เลข `42` อันนี้เป็น [Meme](https://www.independent.co.uk/life-style/history/42-the-answer-to-life-the-universe-and-everything-2205734.html) ซึ่งมาจากนิยายวิทยาศาสตร์เรื่อง A Hitchhiker\\'s Guide to the Galaxy ของ Douglas Adams\\n\\n### Visualise\\n\\nก่อนจะสร้างโมเดลก็น่าจะลอง Visualise ข้อมูลให้เห็นภาพสักหน่อย ในที่นี่เราจะสร้าง Scatterplot matrix ซึ่งแสดงความสัมพันธ์ระหว่างข้อมูลแต่ละ Feature และจำแนกจุดที่เป็นตัวแทนของข้อมูลที่อยู่ในหมวดหมู่แต่ละหมวดจาก 3 หมวด โดยการใช้สีที่ไม่เหมือนกัน\\n\\n```python\\n# Plot the data\\niris_df = pd.DataFrame(X_train, columns=iris.feature_names)\\npd.plotting.scatter_matrix(iris_df, c=y_train, figsize=(12,12), marker=\"o\")\\n```\\n\\nวิธีอ่าน Scatterplot matrix มีดังนี้:\\n\\n* Scatterplot matrix แสดงความสัมพันธ์ระหว่างตัวแปรสองตัว ซึ่งอาจจะเป็น Feature ทั้งสองตัว หรืออาจจะเป็น Feature กับ Label ก็ได้ โดยแสดงทุกคู่ความสัมพันธ์ที่เป็นไปได้อยู่ในภาพเดียวกัน เลยมีหน้าตาเป็น Matrix\\n* จะเห็นว่า Matrix ช่องบนซ้ายแทยงลงมาช่องล่างขวา ไม่ได้แสดงเป็น Scatter plot แต่เป็น Distribution plot เพราะมันคือความสัมพันธ์ของตัวมันเอง โดย Distribution plot ก็มีประโยชน์ ทำเราจะได้เห็นว่ารายการข้อมูลทั้งหมดมีการกระจายตัวใน Feature นั้นอย่างไร เช่น ช่อง Sepal width พบว่ามีการกระจายตัวแบบ Normal distribution คือข้อมูลส่วนมากมีค่าอยู่กลางๆ แถวๆ Mean แล้วกระจายตัวออกทั้งด้านลบและด้านบวก\\n* ส่วนช่องอื่นๆ เราสามารถกำหนดให้แยกสีตาม Label ได้ ทำให้เห็นว่าในแต่ละคู่ความสัมพันธ์ ข้อมูล Label ไหนอยู่ตรงไหน ตัวอย่างเช่น คู่ Petal length VS. Sepal length (แถว 3 คอลัมน์ 1) จะเห็นว่า Label แรกจะมี Petal และ Sepal length น้อย, Label ที่สองอยู่ตรงกลางๆ, และ Label ที่สามมีค่ามาก แต่เมื่อดู Sepal width VS. Sepal length (แถว 2 คอลัมน์ 1) พบว่า Label 2 และ 3 มีค่าผสมผสานกัน แยกกจากกันไม่เด็ดขาด เป็นต้น\\n\\nสำหรับเรา ประโยชน์หนึ่งจากการอ่าน Scatterplot matrix คือการสร้างความเข้าใจในภาพรวมว่าชุดข้อมูลนี้น่าจะ \"ยาก\" หรือ \"ง่าย\" ในการสร้างโมเดล โดยถ้าข้อมูลแต่ละ Label แยกจากกันค่อนข้างชัด การสร้างโมเดลก็จะค่อนข้างง่ายและแม่นยำ\\n\\n### Logistic regression algorithm\\n\\nและแล้วก็เกือบถึงเวลาที่จะฝึกโมเดลให้เข้ากับชุดข้อมูล แต่ก่อนหน้านั้นเรามาทำความเข้าใจว่า Algorithm ที่เราจะใช้นั้นทำงานอย่างไร โดย Algorithm ที่เราเลือกใช้คือ Logistic Regression ชื่ออาจจะฟังดูเหมือน Regression ที่มีเป้าหมายพยากรณ์ค่าต่อเนื่อง แต่ในความเป็นจริงไม่ได้เป็นอย่างนั้น เราลองมาดูกันว่าโมเดลนี้ทำงานอย่างไร\\n\\n#### Hypothesis function\\n\\nใน Classification model เราต้องการให้ y มีคำตอบ คือ 0 หรือ 1 เท่านั้น ซึ่งหมายความว่า \"ไม่ใช่\" หรือ \"ใช่\" (ตอนนี้กำหนดให้มีสองคำตอบไปก่อน การใช้หลักการเดียวกันมาใช้กับการจัดหมวดหมู่ที่มีหลายคำตอบ ทำได้โดยการใช้ Softmax function ซึ่งจะกล่าวถึงภายหลัง)\\n\\nแต่ในความเป็นจริง เราไม่สามารถมั่นใจอะไรได้ร้อยเปอร์เซ็น ว่าคำตอบคือ \"ไม่ใช่\" หรือ \"ใช่\" ดังนั้น สิ่งที่เราต้องการ คือเราจะสร้าง Hypothesis function ที่ให้ค่าความเป็นไปได้ ที่คำตอบจะคือ \"ไม่ใช่\" หรือ \"ใช่\" โดยกำหนดขอบเขตการตัดสินใจ (Decision boundary) ไว้ที่ 0.5 ซึ่งจะทำให้เราได้ขอบเขตการตัดสินใจดังนี้:\\n\\n```tex\\ny = 1 \\\\text{ if } h_\\\\theta(x) \\\\geq 0.5\\\\tag{1.1}\\n\\ny = 0 \\\\text{ if } h_\\\\theta(x) < 0.5\\\\tag{1.2}\\n```\\n\\nโดย `h_\\\\\\\\theta(x)` คือ Hypothesis function ที่มี x เป็น Input ซึ่งอยู่ในรูปของ:\\n\\n`h_\\\\theta(x) = \\\\sigma(z)\\\\tag{2}`\\n\\n`\\\\sigma(z)` อ่านว่า Sigma และ  อ่านว่า Sigmoid z ซึ่ง `\\\\sigma(z)`  นี้เป็นฟังก์ชันที่เรียกว่า Sigmoid function หรือ Logistic function ซึ่งเป็นที่มาของชื่อ Logistic regression นั่นเอง\\n\\n`\\\\sigma(z) = \\\\frac{1}{1+e^{-z}}\\\\tag{3}`\\n\\n* แกนนอนคือ Input ซึ่งในที่นี้คือ `z` ส่วนแกนตั้งคือ Output ซึ่งในที่นี้คือ `h_\\\\theta(x) = \\\\sigma(z)`\\n* สังเกตว่าเมื่อ `z` มีค่ามาก  จะมีค่าเข้าใกล้ 1 ส่วนเมื่อ `z`  มีค่าน้อย `\\\\sigma(z)` จะมีค่าเข้าใกล้ 0\\n* สังเกตว่าเมื่อ `z = 0`, `\\\\sigma(z)=0.5`  ซึ่งอยู่กึ่งกลางระหว่าง Limit ด้านบน คือ 1 และ Limit ด้านล่าง คือ 0 ดังนั้นเราจึงเลือก 0.5 เป็นขอบเขตการตัดสินใจนั่นเอง\\n* `e`คือค่าคงที่ของ Euler มีค่าเท่ากับ 2.7182 (และทศนิยมลำดับต่อไปเรื่อยๆ)\\n* `z` คือ Linear function `z = wx`  ซึ่งเราเคยใช้ใน Linear regression โดยไม่มีตัวแปร Intercept `b`\\n\\nนั่นหมายความว่า:\\n\\n`\\\\sigma(z) = \\\\frac{1}{1+e^{-(wx)}}\\\\tag{4}`\\n\\nอนึ่ง สำหรับใครที่สนใจจะเขียน Algorithm เอง จะควรจะแปลง `wx`  ให้เป็น Vectorised form เพื่อเร่งความเร็วในการคำนวน ซึ่งมีวิธีคือหาการ Dot product ของ W transpose และ X (ทั้งคู่เป็น Matrix) ดังนั้นเราจะได้ Sigmoid function ดังนี้:\\n\\n`\\\\sigma(Z) = \\\\frac{1}{1+e^{-(W^T X)}}\\\\tag{5}`\\n\\nเมื่อได้ Hypotheses function แล้ว หน้าที่ของเรา คือการหาค่า w ที่จะทำให้ค่าความคลาดเคลื่อนระหว่าง `h_\\\\theta(x)`  กับ `y`  นั้นน้อยที่สุด การที่จะทำให้ค่าความคลาดเคลื่อนน้อยที่สุดดังกล่าว ก็ต้องอาศัย Cost function และการหา Gradient descent ของ Cost function เหมือนที่เราเคยทำ\\n\\n### Cost function\\n\\nสำหรับ Logistic regression เราจะใช้ Cost function ดังนี้:\\n\\n```tex\\n\\\\text{cost}(h_\\\\theta(x),y) =\\n\\\\begin{cases}\\n    -\\\\log h_\\\\theta(x) \\\\text{ if } y = 1 \\\\\\\\\\n    -\\\\log(1 - h_\\\\theta(x)) \\\\text{ if } y = 0\\n\\\\end{cases}\\\\tag{6}\\n```\\n\\nลองทำความเข้าใจเงื่อนไขของ Cost function ด้านบน พิจารณาว่าหน้าที่ของ Cost function คือการหาค่าตัวแปรที่จะส่งผลให้ Cost นั้นต่ำที่สุด ซึ่งแบ่งได้เป็น 2 กรณี คือเมื่อ y เท่ากับ 1 และ y เท่ากับ 0\\n\\nโดยในกรณีของ y เท่ากับ 1 หากเราได้ Hypothesis function `h_\\\\theta(x) = 1` จะทำให้ Cost function ซึ่งก็คือ `-\\\\log h_\\\\theta(x) = 0` ซึ่งมีค่าน้อยที่สุดที่จะเป็นไปได้ ดังนั้นเราจึงใช้ `-\\\\log h_\\\\theta(x)` เป็นตัวแทนของ Cost function ในกรณี y=1\\n\\nส่วนในกรณีของ y เท่ากับ 0 หากเราได้ Hypothesis function `h_\\\\theta(x) = 0`  จะทำให้ Cost function ซึ่งก็คือ `-\\\\log(1 - h_\\\\theta(x)) = 0`  ซึ่งมีค่าน้อยที่สุดที่จะเป็นไปได้ ดังนั้นเราจึงใช้ `-\\\\log(1 - h_\\\\theta(x))`  เป็นตัวแทนของ Cost function ในกรณี `y=0`\\n\\nเงื่อนไขทั้งสองแบบ สามารถนำมารวมกันเป็นสมการเดียวได้ว่า:\\n\\n```tex\\n\\\\text{cost}(h_\\\\theta(x),y) = -y\\\\log(h_\\\\theta(x)) - (1-y)\\\\log(1-h_\\\\theta(x))\\\\tag{7}\\n```\\n\\nนำ Cost function นี้มาใส่ในรูปแบบ Cost function ของ Linear regression:\\n\\n```tex\\nJ(\\\\theta) = \\\\frac{1}{m} \\\\sum\\\\limits_{i = 1}^m \\\\text{cost}(h_\\\\theta(x^{(i)}), y^{(i)})\\\\tag{8}\\n```\\n\\nจะได้ Cost function เต็มรูปแบบ คือ:\\n\\n```tex\\nJ(\\\\theta) = - \\\\frac{1}{m} \\\\sum\\\\limits_{i = 1}^m \\\\left( y^{(i)}\\\\log h_\\\\theta(x^{(i)}) + (1-y^{(i)})\\\\log(1-h_\\\\theta(x^{(i)})) \\\\right) \\\\tag{9}\\n```\\n\\nหรือใน Vectorised form ดังนี้:\\n\\n```tex\\nJ(\\\\theta) = \\\\frac{1}{m} \\\\cdot \\\\left( -y^T \\\\log (h) - (1-y)^T \\\\log (1-h) \\\\right) \\\\tag{10}\\n```\\n\\n### Gradient descent\\n\\nเชื่อหรือไม่ว่าอนุพันธ์ของ J(θ) ของ Logistic regression cost function นั้นเหมือนกับอนุพันธ์ J(θ) ของ Linear regression cost function:\\n\\n```tex\\n\\\\theta_j := \\\\theta_j - \\\\alpha \\\\frac{1}{m} \\\\sum\\\\limits_{i = 1}^m(h_\\\\theta (x^{(i)}) - y^{(i)})x_j^{(i)}\\\\tag{11}\\n```\\n\\nส่วน Vectorized version ก็คือ:\\n\\n```tex\\n\\\\theta := \\\\theta - \\\\frac{\\\\alpha}{m} X^T(\\\\sigma(X\\\\theta) - y)\\\\tag{12}\\n```\\n\\nดังนั้น วิธีการอับเดตตัวแปรจึงทำเหมือน Linear regression ทุกประการ\\n\\n#### Softmax function\\n\\nAlgorithm ของเราสามารถให้คำตอบสำหรับปัญหาที่มีคำตอบแค่ 2 ค่า คือ 1 กับ 0 เรียกว่า Binary classification แล้วถ้าคำตอบมีหลายค่า เช่นในโจทย์ของเรา ที่ต้องการจำแนกดอก Iris ออกเป็น 3 สายพันธุ์ล่ะ เราเรียกปัญหาแบบนี้ว่า Multiclass classification\\n\\nLogistic Regression สามารถให้คำตอบปัญหา Multiclass classification โดยการแก้ไขรายละเอียดของกลไกเล็กน้อย ซึ่งจบลงที่การใช้ Softmax function ตอน Output โดยมีหลักการและขั้นตอนดังนี้:\\n\\n1) คำนวนหาผลลัพธ์ Linear function `z` ของแต่ละ Class `k`:\\n\\n```tex\\nz_k = \\\\theta_k^T x\\\\tag{13}\\n```\\n\\nเช่นกรณีของเรา มี `k = 3` เราจะได้ , , และ  ของข้อมูลแต่ละรายการ\\n2) นำ `z_k` ไปประกอบกันใน Softmax function แทน Logistic function เดิม ดังนี้:\\n```tex\\n\\\\hat{p}_k = \\\\sigma(z)_k = \\\\frac{e^{z_k}}{\\\\sum\\\\limits_{j = 1}^K e^{z_j}}\\\\tag{14}\\n```\\n*  `\\\\hat{p}_k` คือความเป็นไปได้ที่รายการนี้จะอยู่ใน Class `k`\\n*  `\\\\sigma(z)_k` เป็นค่าความเป็นไปได้ที่รายการนี้จะอยู่ใน Class `k` โดยเทียบกับคะแนน Linear function ของแต่ละ Class ของรายการนั้น\\n\\nโดย Softmax จะเลือกพยากรณ์ Class ที่ได้คะแนนความเป็นไปได้สูงที่สุด\\n\\nถ้าอ่านแล้วงง ลองโค้ด Softmax function ใน Python ดู:\\n\\n```python\\nz = [1, 2, 3]\\np = np.exp(z)/np.sum(np.exp(z))\\n```\\n\\nเมื่อเรียก p จะได้ผลว่า array([0.09003057, 0.24472847, 0.66524096]) แปลว่าถ้า Linear function z ของ Class แรก ให้ผลเท่ากับ 1 ความเป็นไปได้ของ Class นี้จะเท่ากับ 9% เมื่อเทียบกับ Class ที่สอง (2 --> 24.47%) และ Class ที่สาม (3 --> 66.52%) ดังนั้น Softmax function จะเลือก Class 3 เป็นคำตอบ\\n\\n3) Algorithm จะนำ `p\\\\hat{p}_k` ไปคำนวนใน Cost function ที่ดัดแปลงจากเดิมเล็กน้อย เรียกว่า Cross entropy cost function เพื่อหา Global minimum ที่จะทำให้ความต่างของค่าที่พยากรณ์กับค่าจริงมีน้อยที่สุด:\\n```tex\\nJ(\\\\theta) = - \\\\frac{1}{m} \\\\sum\\\\limits_{i = 1}^m \\\\sum\\\\limits_{k = 1}^K y^{(i)}_k \\\\log(\\\\hat{p}^{(i)}_k) \\\\tag{15}\\n```\\n*  `y^{(i)}_k` คือค่าจริงที่รายการที่ `i` จะอยู่ใน Class `k` ซึ่งจะมีค่า 1 หรือ 0\\n\\n4) หาอนุพันธ์ในกระบวนการ Gradient descent โดยใช้สูตรเดิม:\\n\\n```tex\\n\\\\theta_j := \\\\theta_j - \\\\alpha \\\\frac{1}{m} \\\\sum\\\\limits_{i = 1}^m(\\\\hat{p}^{(i)}_k - y^{(i)}_k)x^{(i)}\\\\tag{16}\\n```\\n\\nอนึ่ง ฟังก์ชัน `LogisticRegression` ใน scikit-learn จะเลือกใช้ Softmax function โดยอัตโนมัติอยู่แล้ว โดยดูจากข้อมูล Label `y` ของเรา ซึ่งสะดวกมาก ไม่ต้องทำอะไรเพิ่มแล้ว แค่เรียกฟังก์ชัน\\n\\n### Modelling\\n\\nตอนนี้เราก็พร้อมแล้วในการสร้างโมเดล โดยเรียก Class `LogisticRegression` และพ่วง Method `.fit` เพื่อสร้างเทรนโมเดลไปเลย\\n\\n```python\\n# Train the model\\nlogreg = LogisticRegression(max_iter=200, random_state=42).fit(X_train, y_train)\\n```\\n\\nจะเห็นว่า Class `LogisticRegression` มี Argument `max-iter` อยู่ด้วย Argument นี้ทำหน้าที่ควบคุม Hyperparameter ของโมเดล โดยเราสามารถกำหนดว่าจะให้ Algorithm (เรียกใน scikit-learn ว่า \"Solver\") ทำงานกี่รอบ ในลักษณะเดียวกับที่ Gradient descent อับเดต Parameter ซ้ำไปเรื่อยๆ เพื่อให้ Cost function ลดลงทุกๆ รอบ\\n\\nอันที่จริง แต่ละโมเดลจะมี Argument แบบนี้มากมาย เช่น `LogisticRegression` มี Argument ดังนี้ (ดูจากเอกสารอ้างอิงของ scikit-learn):\\n```\\nclass sklearn.linear_model.LogisticRegression(penalty=\\'l2\\', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=\\'lbfgs\\', max_iter=100, multi_class=\\'auto\\', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\\n```\\n\\nเราจะพูดถึง Hyperparameter เหล่านี้ในหัวข้อถัดไป ในขั้นนี้เพียงแค่ให้รู้ก่อนว่าเราสามารถกำหนดค่าต่างๆ เพื่อปรับแต่งโมเดลของเราได้ในที่นี้\\n\\n### Evaluate\\n\\nเมื่อเทรนโมเดลแล้ว เราก็มีประเมินความแม่นยำกัน:\\n\\n```python\\n# Evaluate the model\\'s accuracy\\nprint(\"Train set accuracy = \" + str(logreg.score(X_train, y_train)))\\nprint(\"Test set accuracy = \" + str(logreg.score(X_test, y_test)))\\n```\\n\\nได้ผลว่า:\\n\\n```\\nTrain set accuracy = 0.9642857142857143\\nTest set accuracy = 1.0\\n```\\n\\nนั่นคือเมื่อเทรนกับ Train set ได้ความแม่นยำ 96.42% ส่วนเมื่อนำมาทดสอบกับ Test set ได้ความแม่นยำถึง 100% ทีเดียว\\n\\nสำหรับวิธีการคำนวนความแม่นยำของ Logistic regression สูตรนั้นง่ายมาก คือคำนวนว่าสัดส่วนระหว่าง `h_\\\\theta(x)` ที่ให้ค่าตรงกับ y ที่แท้จริงนั้น เป็นเท่าไหร่\\n\\n### Predict\\n\\nมาลองดูกันว่าเวลาเราต้องการพยากรณ์จริงๆ นั้นทำอย่างไร สมมุติว่าเราไปเจอดอก Iris ที่อยากรู้ว่าเป็นสายพันธุ์ไหน เราจึงไปวัด Feature ทั้ง 4 มา ได้ว่า:\\n\\n* sepal length 6 cm\\n* sepal width 2.5 cm\\n* petal length 4 cm\\n* petal width 1.5 cm\\n\\nเราจะต้องใส่ข้อมูลนี้ลงไปในให้โมเดลพยากรณ์ โดยทำข้อมูลให้อยู่ในรูปแบบและมิติเดียวกันกับ X ที่เอาไว้เทรน ซึ่งถ้าจำได้ เราเคยหามิติของ X ไว้ ได้ดังนี้:\\n```\\n    X_shape shape is: (150, 4)\\n```\\n\\nคือเป็น Array ขนาด 150 แถว 4 คอลัมน์ โดยแต่ละแถวคือ 1 รายการ ดังนั้น X ใหม่ของเรา ตั้งชื่อว่า `X_new` จีงจะต้องเป็น Array มิติ (1, 4) ซึ่งเขียนใน Python ได้ว่า `np.array([[6, 2.5, 4, 1.5]])`:\\n\\n```python\\n# Make a prediction\\nX_new = np.array([[6, 2.5, 4, 1.5]])\\ny_pred = logreg.predict(X_new)\\ny_pred_prob = logreg.predict_proba(X_new)\\nprint(\"Prediction:\", y_pred, \"with the probability array:\", y_pred_prob)\\nprint(\"Predicted target name:\", iris[\"target_names\"][y_pred])\\n```\\n    \\n\\nได้คำตอบคือ:\\n```\\n    Prediction: [1] with the probability array: [[0.01372466 0.91809317 0.06818217]]\\n    Predicted target name: [\\'versicolor\\']\\n```\\n\\nนั่นคือโมเดลได้พยากรณ์ว่าดอก Iris ดอกนี้ เป็นสายพันธุ์ Versicolor โดยมีความมั่นใจ 91.8% อย่างไรก็ตามก็มีความเป็นไปได้ที่จะเป็นสายพันธุ์ Setosa 1.37% และ Virginica 6.81%\\n\\nถ้าดูโค้ดแล้วงง ลองพิจารณารายละเอียดดังนี้:\\n\\n* Method `.predict_proba` หมายว่าว่า ให้พยากรณ์โดย Output ออกเป็น Array ของความเป็นไปได้\\n* ทำไม `iris[\"target_names\"][y_pred]` จึงให้ผลออกมาเป็นชื่อสายพันธุ์ได้ ถ้าจำได้ตอนต้น เรารู้ว่าชุดข้อมูลนี้ได้เตรียม Dictionary key ที่ชื่อ `target_names` ไว้ให้ โดยมีค่าว่า `[\\'setosa\\' \\'versicolor\\' \\'virginica\\']` คำสั่งนี้คือการเรียกค่าของ Key `target_names` ลำดับที่ `[y_pred]` ซึ่งในที่นี้คือ `[1]` นั่นเอง\\n\\n### สรุป\\n\\nเป็นอันว่าเราสร้างโมเดล Logistic regression เพื่อพยากรณ์ปัญหา Multiclass classification สำเร็จ\\n\\nในบทต่อไปจะเจาะลึกรายละเอียดเกี่ยวกับการเตรียมข้อมูล โดยเฉพาะข้อมูลที่เป็นข้อความหรือหมวดหมู่ ซึ่งเราต้องแปลงให้เป็นตัวเลขเสียก่อน\\n\\n## บทที่ 5 Categorical Encoding\\n\\nในบทนี้เราจะเริ่มมาเจาะลึกในรายละเอียด โดยจะอธิบายเรื่องการเตรียมข้อมูลให้มีความพร้อมสำหรับการสร้างโมเดล\\n\\nเราได้ลองสร้าง Machine learning model มาสองเรื่องแล้ว ถ้าสังเกตว่าพบว่า ข้อมูล Input `x` นั้นจะต้องเป็นตัวเลข แต่ในความเป็นจริง เรามักได้ข้อมูลมาเป็นข้อความ เช่น \\\\[ใช่, ไม่ใช่\\\\] หรือ \\\\[น้อย, กลาง, มาก\\\\] ถ้าเรามีข้อมูลแบบนี้ เราจะต้องแปลงข้อมูลที่เป็นข้อความหรือหมวดหมู่ ให้เป็นตัวเลขเสียก่อน\\n\\nสมมุติเรามีข้อมูลรายได้ ระดับความสุข ภูมิภาคที่อาศัย และอายุขัย ทั้งหมด 20 ตัวอย่าง เราต้องการพยากรณ์อายุขัยโดยใช้ข้อมูล 3 อย่างแรก:\\n\\n| Income | Happiness | Area | LifeExpectancy |\\n| --- | --- | --- | --- |\\n| 132546 | High | BKK | 80  |\\n| 190998 | Medium | NE  | 75  |\\n| 49308 | Medium | S   | 72  |\\n| 32062 | Low | NE  | 59  |\\n| 71707 | High | BKK | 65  |\\n\\n(แสดง 5 ตัวอย่างแรก)\\n\\nจะเห็นว่า:\\n\\n* Income หรือรายได้ เป็นตัวเลขอยู่แล้ว\\n* Happiness หรือระดับความสุข เป็นหมวดหมู่ มี 3 ค่า คือ \\\\[High, Medium, Low\\\\]\\n* Area หรือภูมิภาคที่อาศัย เป็นหมวดหมู่ มี 4 ค่า คือ \\\\[BKK, NE, S, N\\\\]\\n* LifeExpectacy เป็นตัวเลขอยู่แล้ว\\n\\nหน้าที่ของเราคือจะต้องแปลง Happiness และ Area เป็นตัวเลข โดยหลักการมี 2 ขั้น ได้แก่\\n\\n### Factorisation\\n\\nคือการแทนค่าในแต่ละหมวดด้วยตัวเลขต่างๆ กัน เช่น High = 0, Medium = 1, Low = 2 เป็นต้น โดยเราสามารถแปลงเองใน Excel หรือจะให้ pandas แปลงให้ก็ได้ ลองเขียนโค้ดตั้งแต่เริ่มต้นดังนี้:\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn import metrics\\nimport matplotlib.pyplot as plt\\n\\n# Load the dataset\\ndf = pd.read_csv(\"data/life_expectancy.csv\")\\n\\n# Explore\\nprint(df.head())\\nprint(df.info())\\nprint(df.describe())\\n```\\n\\nเราจะเห็นว่า Income และ LifeExpectancy เป็นจำนวนเต็ม (`int64`) ส่วน Happiness และ Area เป็น `object` ซึ่งสองตัวนี้เป็นสิ่งที่เราต้องจัดการ\\n```\\n    RangeIndex: 20 entries, 0 to 19\\n    Data columns (total 4 columns):\\n    Income            20 non-null int64\\n    Happiness         20 non-null object\\n    Area              20 non-null object\\n    LifeExpectancy    20 non-null int64\\n    dtypes: int64(2), object(2)\\n```\\nจากนั้นให้เลือกเฉพาะคอลัมน์ที่เป็น `object` แล้วลองดูข้อมูลที่ถูกเลือก\\n\\n```python\\n# Limit to categorical data using df.select_dtypes()\\ndf_categories = df.select_dtypes(include=[object])\\nprint(df_categories.head())\\nprint(df_categories.columns)\\nprint(df_categories.shape)\\n```\\n\\nจะได้ Dataframe ใหม่ที่มีคอลัมน์ Happiness และ Area เท่านั้นตามต้องการ และมีมิติ (20, 2)\\n```\\n    Index([\\'Happiness\\', \\'Area\\'], dtype=\\'object\\')\\n    (20, 2)\\n```\\n\\nจากนั้นเรียก Class `LabelEncoder()` ใส่เป็น Argument ของ Method `.apply` ของ Dataframe ใหม่ของเรา แล้วพ่วง Method `.fit_transform` เพื่อแปลง scipy sparse matrix ให้เป็น numpy dense matrix ซึ่งอยู่ในรูปแบบที่เราจะใช้\\n\\n```python\\n# Create a LabelEncoder object and fit it to each feature in X\\nX_label = df_categories.apply(LabelEncoder().fit_transform)\\nprint(X_label.head())\\n```\\n\\nถ้างง บรรทัดนี้คือการสั่งให้ Dataframe ของเราดำเนินการเข้ารหัส Label จากข้อความให้กลายเป็นตัวเลข แล้วแปลงผลลัพธ์ให้อยู่ในรูปแบบที่เอาไปใช้ต่อในการเทรนโมเดลได้\\n\\nได้ผลคือ:\\n\\n```\\nHappiness  Area\\n0          0     0\\n1          2     2\\n2          2     3\\n3          1     2\\n4          0     0\\n```\\nถ้าลองเทียบกลับกับข้อมูลจริง เราจะได้คู่รหัสดังนี้:\\n\\n* {0: \"Happiness-Hi\", 1: \"Happiness-Lo\", 2: \"Happiness-Mid\"}\\n* {0: \"Area-BKK\", 1: \"Area-N\", 2: \"Area-S\", 3: \"Area-NE\"}\\n\\nจากนั้นไปสู่ขั้นตอนต่อไป คือการเข้ารหัส One-hot encoding\\n\\n### One-hot encoding\\n\\nขั้นตอนที่แล้ว เราได้รหัสตัวเลขเดี่ยวๆ แทน Label ของแต่ละคอลัมน์ จริงๆ ดูเหมือนเราน่าจะนำรหัสนี้ไปใช้เป็น Input ของโมเดลได้เลย ซึ่งจริงในบางกรณี อธิบายคือการเข้ารหัสแบบ Factorisation ตัวเลขที่อยู่ใกล้กันจะถูกตีความว่ามี \"คุณค่า\" ใกล้กัน เช่น 1 กับ 2 ใกล้กันมากกว่า 1 กับ 3 ดังนั้น ถ้าค่าจริงของ 1, 2, 3 มีความสัมพันธ์เชิงคุณค่าที่ต่อเนื่องกัน เราก็สามารถใช้ Factorisation ได้เลย เช่นถ้า 1 แปลว่า \"น้อย\", 2 แปลว่า \"ปานกลาง\", และ 3 แปลว่า \"มาก\" เป็นต้น\\n\\nแต่ในความเป็นจริง มีหลายกรณีที่ค่าจริงของแต่ละเลขนั้นไม่ได้มีความสัมพันธ์ต่อเนื่องเป็นลำดับขั้นกัน เช่นในกรณีภูมิภาคที่อยู่อาศัย ค่าของแต่ละเลขล้วนมีความหมายของมันเอง และไม่เกี่ยวข้องทางลำดับชั้นกับค่าอื่น\\n\\nวิธีการแก้ปัญหานี้ คือการใช้ One-hot encoding ซึ่งคือการเข้ารหัสแบบ Binary แทนแต่ละ Label ตัวอย่างเช่น [1, 0, 0, 0] แทน BKK (กทม.), [0, 1, 0, 0] แทน N (ภาคเหนือ) เป็นต้น วิธีการนี้จะทำให้ Label แต่ละอันนั้นเป็นอิสระต่อกัน ทำให้การสร้างโมเดลนั้นแม่นยำมากขึ้น\\n\\nวิธีการทำ One-hot encoding คือ:\\n\\n```python\\n# Create a OneHotEncoder object, and fit it to all of X\\nX_1hot = OneHotEncoder().fit_transform(X_label).toarray()\\nprint(X_1hot[0:5, :])\\nprint(X_1hot.shape)\\n```\\n\\n### Re-creating input X\\n\\nทบทวนว่าตอนนี้เรามี Dataframe อยู่ 2 ชุด\\n\\n* `df` คือ Dataframe ต้นฉบับ มีหัวข้อคือ Income, Happiness, Area, และ LifeExpectancy\\n* `df_1hot` คือ Dataframe เฉพาะ Happiness และ Area ที่ถูกเข้ารหัส (และขยายขนาด) ด้วย One-hot encoding แล้ว\\n\\nทีนี้เรารู้ว่าการจะเทรนโมเดล เราจะต้องเตรียม Dataframe 2 ชุด คือ X ซึ่งเป็น Input ทั้งหมด และ y ซึ่งเป็น Label คำตอบทั้งหมด จะทำอย่างไรดี? อธิบายไปตอนนี้อาจจะงง ลองมาดูโค้ดแล้วอธิบายด้วยโค้ดดีกว่า:\\n\\n```python\\n# Recombine numerical and one-hot dataframes to make X\\ndf_1hot = pd.DataFrame(X_1hot)\\nX = pd.concat([df[\"Income\"], df_1hot], axis=1, sort=False,)\\nX.columns = [\"Income\", \\n             \"Happiness-Hi\", \"Happiness-Lo\", \"Happiness-Mid\", \\n             \"Area-BKK\", \"Area-N\", \"Area-S\", \"Area-NE\"]\\ny = df[\"LifeExpectancy\"]\\nprint(X.head())\\nprint(X.shape)\\nprint(y.head())\\nprint(y.shape)\\n```\\n\\nอธิบายดังนี้:\\n\\n* เราจะรวม Dataframe เข้าด้วยกัน ดังนั้น ขั้นแรกเราต้องแปลง `X_1hot` ที่เป็น Array ให้เป็น pandas dataframe ก่อน ตั้งชื่อว่า `df_1hot`\\n* จากนั้นเราประกอบ Dataframe ใหม่ที่จะเป็น X ของเรา ด้วยการ \"ต่อ\" (Concatenate) คอลัมน์ Income ของ `df` ต้นฉบับ เข้ากับ `df_1hot` ทั้งหมด โดยกำหนด Argument `axis=1` เพื่อให้ต่อออกไปทางแนวนอน\\n* เพื่อความไม่สับสนและเอาไว้อ้างอิง เราจะตั้งชื่อคอลัมน์ใหม่ให้ Dataframe X ของเรา เช่น รหัสแรกของ Happiness ถ้ามีเลข 1 ตรงนั้น (\\\\[1, 0, 0\\\\]) แปลว่า Happiness-Hi เป็นต้น\\n* สุดท้ายก็สร้าง Dataframe y โดยเลือกเฉพาะคอลัมน์ที่เป็นคำตอบ จาก Dataframe `df` ต้นฉบับ\\n\\nอย่าลืมตรวจดูว่าผลที่ได้นั้นเป็นไปตามที่เราคิด ด้วยการ `print` head และ shape ของ X กับ y ออกมาดู\\n\\nเป็นอันว่าเราได้ Dataframe X และ y ที่พร้อมสำหรับการสร้างโมเดลแล้ว\\n\\n### Model training\\n\\nเรามาฝึกโมเดล Linear regression กันเลย โดนอย่าลืมแยกข้อมูลออกเป็น Train set กับ Test set ก่อน:\\n\\n```python\\n# Split the data into train and test set\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\\nprint(\"X_train shape is:\", X_train.shape)\\nprint(\"y_train shape is:\", y_train.shape)\\nprint(\"X_test shape is:\", X_test.shape)\\nprint(\"y_test shape is:\", y_test.shape)\\n\\n# Train the model\\nlr = LinearRegression().fit(X_train, y_train)\\ncoeff_df = pd.DataFrame(lr.coef_, X.columns, columns=[\\'Coefficient\\'])\\nprint(coeff_df)\\n```\\n\\nพอฝึกแล้ว มาลองดูว่าโมเดลให้คำตอบเป็นค่า Coefficient ของน้ำหนักสำหรับแต่ละ Feature อย่างไร สำหรับ Linear regression หลายตัวแปร หรือที่เรียกว่า Multivariate linear regression นี้ สูตร Linear function คือ:\\n\\n```tex\\n\\\\hat{y} = w_1x_1 + w_2x_2 + w_3x_3 + \\\\dots + w_nx_n\\\\ + b \\\\tag{1}\\n```\\n\\nโดย `n` คือจำนวน Feature ซึ่งในกรณีของเราคือ 8\\n\\nเราสามารถสร้างตารางแสดงค่า Coefficient ของแต่ละ Feature ได้ด้วย pandas dataframe โดยกำหนดใน Argument ให้ `lr.coef_` เป็น Input, ให้ชื่อคอลัมน์ `X.columns` เป็น Label แกนนอน และตั้งชื่อคอลัมน์โดยการกำหนด `columns=[\\'Coefficient\\']` ได้ผลดังนี้:\\n\\n```\\n               Coefficient\\nIncome            0.000055\\nHappiness-Hi      8.306516\\nHappiness-Lo     -8.859939\\nHappiness-Mid     0.553423\\nArea-BKK         -9.442937\\nArea-N            3.505052\\nArea-S            4.850387\\nArea-NE           1.087498\\n```\\nจากนั้น เราลองมาดูเลยดีกว่าว่าค่าที่พยากรณ์ได้เทียบกับค่าจริงใน Test set เป็นอย่างไร:\\n```python\\n# Make a prediction on test set\\ny_pred = lr.predict(X_test)\\ndf_pred = pd.DataFrame({\\'Actual\\': y_test, \\'Predicted\\': y_pred})\\nprint(df_pred)\\n```\\nเราพยากรณ์ `y_pred` โดยใช้ `X_test` เป็น Input เพื่อให้ `y_pred` ที่ได้ สามารถนำไปเปรียบเทียบกับ `y_test` ที่เรารู้ค่าอยู่แล้ว เราป้อนทั้งสองตัวเป็น Input ของ pandas dataframe จะได้ผลว่า:\\n```\\n    Actual  Predicted\\n3       59  63.213900\\n16      89  74.945451\\n6       49  60.294754\\n10      76  82.234579\\n2       72  69.815254\\n```\\nผลที่ได้ไม่ไกล แต่ก็ไม่ถึงกับใกล้ เราอยากรู้เพิ่ม จึงลองคำนวนค่าเฉลี่ยของ `y_test` และ `y_pred` ดู:\\n\\n```python\\nprint(\"Mean of y_test is\", np.mean(np.array(y_test)))\\nprint(\"Mean of y_pred is\", np.mean(y_pred))\\n```\\n\\nได้ผลว่า:\\n```\\nMean of y_test is 69.0\\nMean of y_pred is 70.10078755113827\\n```\\n\\nซึ่งใกล้กันมาก แสดงว่าการพยากรณ์แต่ละรายการ มีความแปรผันมาก เลยลองพล็อตกราฟเทียบให้เห็นๆ:\\n```python\\n# Plot the conparison between actual and predicted y\\ndf_pred.plot(kind=\"bar\", figsize=(9,7))\\nplt.show()\\n```\\n\\n### Evaluation metrics\\n\\nการดูด้วยตาย่อมไม่เพียงพอ เราเลยต้องลองใช้มาตรวัดต่างๆ สำหรับ Linear regression มาวัดดู:\\n```python\\n# Evaluate the model\\nprint(\"Train set R2-score = \" + str(lr.score(X_train, y_train)))\\nprint(\"Test set R2-score = \" + str(lr.score(X_test, y_test)))\\n\\nprint(\\'Test set Mean Absolute Error:\\', metrics.mean_absolute_error(y_test, y_pred))\\nprint(\\'Test set Mean Squared Error:\\', metrics.mean_squared_error(y_test, y_pred))\\nprint(\\'Test set Root Mean Squared Error:\\', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\\n```\\nผลที่ได้มีดังนี้:\\n```\\nTrain set R2-score = 0.5401999191488698\\nTest set R2-score = 0.5965533912121594\\nTest set Mean Absolute Error: 7.596505622473046\\nTest set Mean Squared Error: 77.30037024375025\\nTest set Root Mean Squared Error: 8.792062911726136\\n```\\nอธิบายดังนี้\\n\\n1)  `R^2` Score วัดค่าสะสมของความต่างระหว่างค่าจริงกับค่าที่พยากรณ์ได้ เมื่อเทียบกับค่าสะสมระหว่างค่าจริงกับค่าเฉลี่ยของค่าจริงทั้งหมด:\\n```tex\\nR^2 = 1 - \\\\frac{SS_{res}}{SS_{tot}}\\\\tag{2}\\n\\nSS_{res} = \\\\sum\\\\limits_{i = 1}^m(y_i - \\\\hat{y}_i)^2\\\\tag{3}\\n\\nSS_{tot} = \\\\sum\\\\limits_{i = 1}^m(y_i - \\\\bar{y}_i)^2\\\\tag{4}\\n```\\n2) Mean Absolute Error (MAE) วัดค่าเฉลี่ยของผลรวมความต่างระหว่างค่าจริงกับค่าที่พยากรณ์ได้:\\n```tex\\nMAE = \\\\frac{1}{m} \\\\sum\\\\limits_{i = 1}^m|y_i - \\\\hat{y}_i|\\\\tag{5}\\n```\\nวัดค่าเฉลี่ยของความคลาดเคลื่อนของทั้งโมเดล\\n\\n3) Mean Squared Error (MSE) วัดค่าเฉลี่ยของผลรวมยกกำลังสองของความต่างระหว่างค่าจริงกับค่าที่พยากรณ์ได้:\\n```tex\\nMSE = \\\\frac{1}{m} \\\\sum\\\\limits_{i = 1}^m(y_i - \\\\hat{y}_i)^2\\\\tag{6}\\n```\\nสำหรับ MSE ตัวอย่างบางตัวที่มีความคลาดเคลื่อนระหว่างค่าจริงกับค่าพยากรณ์สูง จะถูกให้ค่าน้ำหนักมากเป็นทวีคูณ ดังนั้น MSE จึงชี้วัดได้ดีว่าโมเดลของเรามีตัวอย่างที่มีค่าความคลาดเคลื่อนสูงผิดปกติหรือไม่ (Sensitive to outliers)\\n4) Root Mean Squared Error (RMSE) คือ Square root ของ MSE:\\n```tex\\nRMSE = \\\\sqrt{\\\\frac{1}{m} \\\\sum\\\\limits_{i = 1}^m(y_i - \\\\hat{y}_i)^2}\\\\tag{7}\\n```\\nRMSE ใช้ Square root แปลง MSE กลับมาให้มีหน่วยใกล้เคียงกับ MAE เพื่อให้ตีความเปรียบเทียบได้ง่ายขึ้น สามารถชี้วัดการกระจายตัวของค่าความคลาดเคลื่อนได้ดี ในกรณีของเราให้ค่าใกล้เคียงกับ MAE คือประมาณ ~ 7-8\\n\\nจะเห็นว่าโมเดลของเรายังพยากรณ์ได้ไม่ดีนัก ซึ่งมีสาเหตุและทางแก้ดังนี้:\\n\\n* โมเดลเทรนบนข้อมูลขนาดเล็กมาก คือ Train set 15 รายการเท่านั้น Machine learning ต้องการข้อมูลจำนวนมากกว่านั้นมาก อย่างต่ำๆ ควรจะเป็นหลักหลายๆ ร้อย หรือหลักพัน โดยสำหรับ Linear regression ที่เส้นพยากรณ์เป็นเส้นตรง การที่มีข้อมูลจำนวนมากจะทำให้ค่าความคลาดเคลื่อนโดยเฉลี่ยนั้นลดลง ปัญหานี้เรียกว่าปัญหา Bias ซึ่งตรงข้ามกับปัญหา Variance ที่เป็นโมเดลที่ฟิตกับ Train set มากแต่พยากรณ์ Test set ได้ไม่ดี เราจะพูดถึงปัญหาเหล่านี้ในบทถัดๆ ไป\\n* ข้อมูลอาจจะมีความแปรผันมากจนสมการเส้นตรง `\\\\hat{y} = w_1x_1 + w_2x_2 + \\\\dots + w_nx_n\\\\ + b`  ไม่สามารถฟิตกับข้อมูลได้ดี ถ้าเรามีข้อมูลมากขึ้น เราอาจะลองพล็อต Scatterplot เพื่อดูรูปร่างความสัมพันธ์ข้อมูล แล้วเปลี่ยนสูตร Algorithm ให้ใช้ Hypothesis function ที่เหมาะสมกับรูปร่างข้อมูลขึ้น เช่น หากพบว่าความสัมพันธ์ใน Scatterplot เป็นเส้นโค้ง อาจพิจารณาใช้ Quadratic function ในรูปแบบ `\\\\hat{y} = w_1x^2_1 + w_2x_2 + \\\\dots + w_nx_n\\\\ + b` เรียกว่า Polynomial regression ซึ่งสามารถหา Class ใน scikit-learn ใช้ได้\\n\\nเป็นอันว่าเราสามารถแปลงข้อมูลที่เป็นข้อความหรือหมวดหมู่ให้เป็นตัวเลขได้แล้ว บทต่อไปเราจะพูดถึงการทำ Feature scaling ซึ่งเป็นอีกขั้นตอนของการเตรียมข้อมูล\\n\\n## บทที่ 6 Feature Scaling\\n\\nMachine learning algorithm หลายตัว เช่น SVM และ Neural Networks จะทำงานได้ดีเมื่อข้อมูล Input อยู่ใน Scale มาตรฐาน นั่นคือมีค่าเฉลี่ย Mean เท่ากับ 0 และ Variance เท่ากับ 1 ดังนั้นหากเราใช้ Algorithm เหล่านี้ หรือลองเทรนโมเดลแล้วได้ค่าความแม่นยำต่ำ หรือใช้เวลานานมากในการเทรน ให้ลองเปลี่ยน Scale ของ Feature ดู\\n\\nFeature scaling มีหลายสูตร แต่สูตรที่ใช้งานได้ดีและเป็นที่นิยม คือสูตร `StandardScaler` ใน `preprocessing` module ของ scikit-learn โดย `StandardScaler` จะมีสูตรดังนี้:\\n\\n```tex\\nx_{i (scaled)} = \\\\frac{x_i-\\\\mu}{\\\\sigma}\\\\tag{1}\\n```\\n\\n*  `x_i` คือ Input `x`\\n*  `\\\\mu` คือค่า Mean ของ Input `x` ทั้งหมด\\n*  `\\\\sigma` คือส่วนเบี่ยงเบนมาตรฐาน หรือ Standard deviation ของ Input `x` ทั้งหมด ซึ่งคิดมาจาก Square root ของ Variance\\n\\nโดย Variance (`\\\\sigma^2`) มีสูตรคือ:\\n\\n```tex\\n\\\\sigma^2 = \\\\frac{1}{m} \\\\sum\\\\limits_{i = 1}^m (x_i-\\\\mu)^2\\\\tag{2}\\n```\\n\\nค่า `x` ที่ Scale แล้ว จะกระจายตัวออกรอบๆ 0 และรวมกันจะมีส่วนเบี่ยงเบนมาตรฐานเท่ากับ 1 ซึ่งเดี๋ยวจะพิสูจน์ให้ดู\\n\\nเรามาลองทำ `StandardScaler` กัน โดยโหลดข้อมูลมาก่อน:\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.preprocessing import scale\\nfrom sklearn.preprocessing import StandardScaler\\nimport matplotlib.pyplot as plt\\n\\n# Load the dataset\\ndf = pd.read_csv(\"data/life_expectancy.csv\")\\n\\nprint(df.head())\\n```\\n\\nซึ่งเป็นชุดข้อมูลเดิมจากบทที่ 5 มีหน้าตาดังนี้:\\n\\n```\\n   Income Happiness Area  LifeExpectancy\\n0  132546      High  BKK              80\\n1  190998    Medium   NE              75\\n2   49308    Medium    S              72\\n3   32062       Low   NE              59\\n4   71707      High  BKK              65\\n```\\n\\nเราจะลองทำ Scaling เฉพาะ Feature \"Income\" และ Label \"LifeExpectancy\" โดยในความจริงเราไม่จำเป็นต้อง Scale label แต่จะทำให้ดูเพื่อพล็อต Scatterplot แสดงว่าความสัมพันธ์ของข้อมูลยังคงเหมือนเดิมหลังจาก Scale แล้ว\\n\\nสมมุติว่าจะสร้างโมเดลเฉพาะความสัมพันธ์ระหว่าง `Income` กับ `LifeExpectancy` ลองพล็อต Scatterplot ดูดังนี้:\\n\\n```python\\n# Plot Income against LifeExpectancy before scaling\\nplt.scatter(df[\"Income\"], df[\"LifeExpectancy\"])\\n```\\n\\nก่อนเราจะทำการ Scale มาเตรียมข้อมูลให้เป็น numpy array กันก่อนเพื่อให้ไม่ต้องเรียกคอลัมน์จาก pandas ทุกๆ ครั้ง:\\n\\n```python\\n# Prep X and y\\nx = np.array(df[\"Income\"]).reshape(-1,1)\\ny = np.array(df[\"LifeExpectancy\"]).reshape(-1,1)\\nprint(x.shape)\\nprint(y.shape)\\n```\\nได้มิติ `x` และ `y` เป็น (20, 1) ทั้งคู่\\n\\nต่อมาเราจะ Scale ข้อมูลสองหมวดนี้กันเลย โดยมี 2 วิธีดังนี้:\\n\\n1) Function `scale`\\n\\nเป็นวิธีที่ง่ายและสั้น ทำได้โดย:\\n\\n```python\\n# Method 1: Apply scaling using scale function\\nx_scaled = scale(x)\\ny_scaled = scale(y)\\nprint(x_scaled[:5, :])\\nprint(y_scaled[:5, :])\\n```\\nได้ผลว่า:\\n```\\n[[ 0.47792414]\\n [ 1.38439361]\\n [-0.81292487]\\n [-1.08037462]\\n [-0.46556277]]\\n[[ 0.77057895]\\n [ 0.35850465]\\n [ 0.11126006]\\n [-0.96013313]\\n [-0.46564397]]\\n```\\n2) Class `StandardScaler`\\n\\nวิธีนี้จะใช้ Transformer API คำนวน Mean และ Standard deviation ก่อน เพื่อนำค่าทั้งสองไปใช้ประโยชน์ภายหลังได้ เช่นการเอาไป Scale test set ให้มีลักษณะการกระจายตัวเหมือน Train set ซึ่งจะอธิบายความสำคัญของความสามารถในตอนจบบทนี้\\n\\nวิธีการคือ:\\n```python\\n# Method 2.1: Apply scaling using StandardScaler class (fit then transform)\\nx_scaler = StandardScaler().fit(x)\\ny_scaler = StandardScaler().fit(y)\\nprint(\"Mean of x is:\", x_scaler.mean_)\\nprint(\"Variance of x is:\", x_scaler.var_)\\nprint(\"Standard deviation of x is:\", x_scaler.scale_)\\nx_scaled = x_scaler.transform(x)\\ny_scaled = y_scaler.transform(y)\\nprint(x_scaled[:5, :])\\nprint(y_scaled[:5, :])\\n```\\nเราฟิต `StandardScaler` กับข้อมูลก่อน ซึ่งจะเป็นการเก็บ Parameter ที่ได้มาไว้ใน Object ก่อน เราสามารถเรียก Mean, Variance, และ Standard deviation มาดูได้โดยใช้ Method `.mean_`, `.var_`, และ `.scale_` โดยได้คำตอบคือ:\\n```\\nMean of x is: [1.11022302e-17]\\nVariance of x is: [1.]\\nStandard deviation of x is: [1.]\\n```\\nสังเกตว่า Mean เท่ากับ `1.11022302 \\\\times 10^{(-17)}` ซึ่งมีค่าน้อยมากๆ เข้าใกล้ 0 นั่นเอง\\n\\nจากนั้นเราค่อยเอา Object ที่ฟิตแล้ว มา Transform เพื่อให้ได้ Array ที่ Scale แล้ว ได้ผลคือ:\\n```\\n[[ 0.47792414]\\n [ 1.38439361]\\n [-0.81292487]\\n [-1.08037462]\\n [-0.46556277]]\\n[[ 0.77057895]\\n [ 0.35850465]\\n [ 0.11126006]\\n [-0.96013313]\\n [-0.46564397]]\\n```\\nสังเกตว่าเท่ากับวิธีที่ 1\\n\\nอนึ่ง เราสามารถควบ `.fit` และ `.transform` เอาไว้ในคำสั่งเดียว คือ `.fit_transform` ซึ่งกระชับกว่า ทำได้โดย:\\n```python\\n# Method 2.2: Apply scaling using StandardScaler class (fit_transform)\\nscaler = StandardScaler()\\nx_scaled = scaler.fit_transform(x)\\ny_scaled = scaler.fit_transform(y)\\nprint(x_scaled[:5, :])\\nprint(y_scaled[:5, :])\\n```\\nจะได้ผลเหมือนเดิม\\n\\nพบว่าหน้าตาความสัมพันธ์และระยะห่างระหว่างข้อมูลแต่ละรายการนั้นเหมือนเดิมทุกประการ สิ่งเดียวที่ต่างออกไปคือ Scale บนแกน x และ y ที่ตอนนี้ข้อมูลจะกระจายตัวรอบๆ ค่าเฉลี่ย 0 โดยจะลองเช็คอีกทีก็ได้:\\n```python\\n# Check that all means is 0 and std is 1.\\nprint(\"All x has mean of:\", x_scaled.mean(axis=0))\\nprint(\"All x has standard deviation of:\", x_scaled.std(axis=0))\\n```\\nแน่นอนว่าคำตอบคือ:\\n```\\nAll x has mean of: [1.11022302e-17]\\nAll x has standard deviation of: [1.]\\n```\\n\\nข้อสำคัญของการทำ Feature scaling คือต้อง Scale ทั้งข้อมูล Train set และ Test set โดยใช้ Mean และ Variance เดียวกัน เพื่อรักษารูปแบบการกระจายตัวของข้อมูลทั้งสองชุดให้เหมือนกัน อย่างไรก็ตาม ไม่ควร Scale ข้อมูลทั้งหมดทีเดียวและค่อยแยก Train set กับ Test set เพราะการทำอย่างนั้นจะทำให้ข้อมูลการกระจายตัวของ Test set \"รั่ว\" ไปส่งผลต่อการ Scale train set\\n\\nดังนั้น วิธีที่ถูกต้องคือ:\\n\\n1.  ใช้ `StandardScaler` เรียก `.fit` เข้ากับ `X_train` แล้วผลที่ได้ไว้เป็น Instance object\\n2.  เรียก `.transform` จาก Instance ในข้อ 1 โดยใส่ Argument เป็น `X_train` เพื่อเปลี่ยนชุด `X_train` ให้ได้ Scale\\n3.  จากนั้นเรียก `.transform` จาก Instance ในข้อ 1 โดยใส่ Argument เป็น `X_test` ซึ่งเป็นการเรียก Parameter (Mean, variance) ที่เคยฟิตจาก `X_train` มาใช้กับ `X_test`\\n\\nการทำ Feature scaling นั้นยังมีวิธีอื่นๆ อีก เช่น:\\n\\n* `MinMaxScaler` เป็นการ Scale ให้ข้อมูลมีค่าระหว่างค่าต่ำสุดและสูงสุด เหมาะกับข้อมูลที่มี Standard deviation น้อยมากๆ\\n* `RobustScaler` เป็นการ Scale โดยใช้ Median และ Quantile แทน Mean กับ Variance เพื่อให้ค่าที่ได้ไม่ได้รับอิทธิพลจากข้อมูลที่มีค่าโดดจากพวก (Outliers)\\n* `Normalizer` เป็นการ Scale ให้ข้อมูลแต่ละรายการมี Norm เท่ากับ 1 โดยไม่เกี่ยวกับข้อมูลรายการอื่น เหมาะกับงาน Text classification และ Clustering\\n\\nแนะนำให้ศึกษาเพิ่มจากคู่มือของ scikit-learn และแหล่งข้อมูลอื่นๆ ได้เอง\\n\\nบทต่อไปจะเข้าสู่รายละเอียดของการเทรนโมเดล โดยจะพูดถึงปัญหา Bias และ Variance และวิธีแก้\\n\\n## บทที่ 7 Bias and Variance\\n\\nสมมุติว่าเราเทรนโมเดล Binary classification (มีสองคำตอบ คือ \"ใช่\" กับ \"ไม่ใช่\") แล้วได้คะแนนความแม่นยำดังนี้:\\n\\n* Human-expert\\'s score = 90%\\n* Train set score = 75%\\n\\nจะเห็นว่าคะแนนค่อนข้างต่ำ คือถ้าให้มนุษย์จำแนกอาจจะถูกมากกว่า 90% แต่โมเดลจำแนกถูกเพียง 75% เราเรียกว่าโมเดลนี้มีปัญหา Bias\\n\\nปัญหานี้เกิดจากการที่โมเดลเราไม่สามารถฟิตกับข้อมูลได้ดีเท่าที่ควร เรียกว่าโมเดลมี Bias หรือโมเดลนั้น Underfit ข้อมูล\\n\\nสาเหตุของปัญหา Bias มีหลักๆ ดังนี้:\\n\\n* ข้อมูล Train set มีขนาดเล็กเกินไป โมเดลจึงไม่มี Information เพียงพอที่จะเรียนรู้ว่าควรจะตั้ง Parameter เท่าใดจึงจะเข้าได้กับข้อมูลส่วนมาก\\n* การตั้ง Hyperparameter ไม่เหมาะสมกับข้อมูล เช่น ตั้ง Learning rate สูงเกินไป หรือการตั้งจำนวนครั้งในการ Iterate น้อยเกินไป (ส่งผลเหมือนกับการบังคับให้โมเดลหยุดการทำงานก่อนกำหนด หรือ Early stopping) ทำให้ Algorithm ไม่สามารถหา Global minimum พบได้\\n* เราเลือกใช้ Algorithm ที่ไม่เหมาะกับข้อมูล หรือกำหนดโครงสร้าง Algorithm ไม่เหมาะกับข้อมูล โดยมีความซับซ้อนน้อยเกินไป เช่น ข้อมูลที่มีความสัมพันธ์แบบ Polynomial ไม่ใช่เส้นตรง แต่เราไปใช้ Linear regression ซึ่ง \"ง่าย\" เกินไปเมื่อเทียบกับลักษณะข้อมูล หรือ Neural networks มีจำนวน Layer และจำนวน Neuron ในแต่ละ Layer ไม่เพียงพอกับความซับซ้อนของข้อมูล เป็นต้น\\n\\nสมมุติว่าเราพยายามแก้ปัจจัยเหล่านี้ เช่น การเพิ่มจำนวน Training set, การลด Learning rate, การเลือก Algorithm ที่เหมาะสมขึ้น แล้วลองเทรนโมเดลใหม่ อาจพบว่าได้คะแนนความแม่นยำดังนี้:\\n\\n* Human-expert\\'s score = 90%\\n* Train set score = 87%\\n* Test set score = 65%\\n\\nจะเห็นว่า คราวนี้เราได้ความแม่นยำของ Train set สูงขึ้นจนเกือบเท่ามนุษย์แล้ว แต่เมื่อนำไปทดสอบกับ Test set ซึ่งเป็นข้อมูลชุดที่โมเดลไม่เคยเห็น พบว่าได้ความแม่นยำเพียง 65% เท่านั้น\\n\\nปัญหานี้เรียกว่า โมเดลมี Variance สูง หรือโมเดลได้ Overfit ข้อมูล ซึ่งมีลักษณะกลับกันกับปัญหา Bias/underfit กล่าวคือ โมเดลพยายาม \"รู้ดี\" จนเกินไป ด้วยการฟิตตัวเองเข้ากับข้อมูลใน Train set ทุกๆ รายการอย่างถูกต้อง แต่การทำอย่างนี้ไม่ได้แปลว่าขอบเขตการตัดสินใจที่เกิดขึ้นจะฟิตกับข้อมูลอื่นๆ ที่โมเดลไม่เคยเห็น ซึ่งทดสอบได้จากการพยากรณ์ด้วย Test set\\n\\nสาเหตุของปัญหา Variance มีหลักๆ ดังนี้:\\n\\n* ข้อมูล Test set มีขนาดเล็กเกินไป ทำให้ไม่เป็นตัวแทนที่ดีในการทดสอบ\\n* ข้อมูล Train set และ Test set มี่ลักษณะการกระจายตัวที่แตกต่างกันมาก ซึ่งอาจจะเกิดจากการไม่ได้สุ่มคัดแยก Train set / Test set อย่างถูกต้องตั้งแต่แรก หรือการเก็บข้อมูล Train set คนละครั้ง หรือจากคนละแหล่งที่มากับ Test set\\n* จำนวน Feature มากเกินไป ทำให้โมเดลมีความซับซ้อนจนเกินจำเป็น (อาจแก้ด้วยการตัด Feature ที่คิดว่าไม่ได้ส่งผลจากผลลัพธ์ออก แต่ก็เสี่ยงเพราะเรารู้ได้ยากว่า Feature ไหนสำคัญมากน้อย)\\n* Algorithm ขาดหรือไม่ได้ใช้กลไกการชดเชยน้ำหนัก ที่เรียกว่า Regularisation ซึ่งจะกล่าวถึงโดยละเอียดในส่วนถัดไป\\n\\nดังนั้น ทางแก้คือให้พิจารณาว่า Test set มีขนาดเพียงพอหรือไม่ และมีการกระจายตัวเหมือน Train set หรือไม่ ถ้าสองปัจจัยนี้ผ่าน ก็มาถึงการใช้เทคนิค Regularisation ซึ่งเมื่อทำแล้ว ก็น่าจะได้โมเดลที่มีคะแนนประมาณนี้:\\n\\n* Human-expert\\'s score = 90%\\n* Train set score = 85%\\n* Test set score = 83%\\n\\nสังเกตว่าคะแนน Train set อาจจะลดลงเล็กน้อย เพราะโมเดลถูกชดเชยน้ำหนัก ทำให้ความแม่นยำบน Train set ลดลง แต่ลดความซับซ้อนลงจนทำให้ฟิตกับข้อมูล Test set ที่โมเดลไม่เคยเห็นได้ดียิ่งขึ้น\\n\\nเป้าหมายของเรา คือการสร้างโมเดลที่ \"กำลังดี\" แบบนี้ ซึ่งวิธีที่ได้ผลที่สุดและควรลองเป็นอันดับแรก (เมื่อมั่นใจแล้วว่า Test set ไม่ได้เป็นปัญหา) คือ Regularisation\\n\\n### Regularisation\\nRegularisation คือเทคนิคการ \"ชดเชยน้ำหนัก\" ของ Parameter ต่างๆ ในโมเดล ซึ่งจะทำให้น้ำหนักของ Parameter ต่างๆ ลดลง ส่งผลให้โมเดลลดความซับซ้อนลง จึงเพิ่มโอกาสที่โมเดลจะสามารถฟิตกับข้อมูลที่ไม่เคยมองเห็น เช่น Test set ได้มากขึ้น เราเรียกสถานการณ์แบบนี้ว่าโมเดลสามารถ Generalise ได้ดี\\n\\nการชดเชยน้ำหนักของ Algorithm แต่ละตัวมีหลักการคล้ายกัน ตัวอย่างเช่น Linear regression ที่ปกติมี Cost function ดังนี้:\\n\\n```tex\\nJ(\\\\theta_0, \\\\theta_1) = \\\\frac{1}{2m} \\\\sum\\\\limits_{i = 1}^m(h_\\\\theta(x^{(i)}) - y^{(i)})^2 \\\\tag{1}\\n```\\n\\nการทำ Regularisation จะเปลี่ยน Cost function ให้เป็นดังนี้:\\n\\n```tex\\nJ(\\\\theta_0, \\\\theta_1) = \\\\frac{1}{2m} \\\\left(\\\\sum\\\\limits_{i = 1}^m(h_\\\\theta(x^{(i)}) - y^{(i)})^2 + \\\\lambda \\\\sum\\\\limits_{j=1}^n \\\\theta_j^2\\\\right) \\\\tag{2}\\n```\\n\\nและปรับ Gradient descent เล็กน้อย ยกเว้น  ที่ไม่ต้อง Regularise:\\n\\n_ทำซ้ำจนกระทั่งผลลัพธ์ล่าสุดไม่เปลี่ยนแปลงจากผลลัพธ์ครั้งก่อน:_\\n\\n```tex\\n\\\\theta_0 := \\\\theta_0 - \\\\alpha \\\\frac{1}{m} \\\\sum\\\\limits_{i = 1}^m(h_\\\\theta (x^{(i)}) - y^{(i)})x_0^{(i)} \\\\tag{3}\\n\\n\\\\theta_j := \\\\theta_j - \\\\alpha\\\\Biggl(\\\\biggl( \\\\frac{1}{m} \\\\sum\\\\limits_{i = 1}^m(h_\\\\theta (x^{(i)}) - y^{(i)})x_j^{(i)}\\\\biggl) + \\\\frac{\\\\lambda}{m} \\\\theta_j\\\\Biggl) \\\\tag{4}\\n```\\nสังเกตว่าเราเพิ่ม `\\\\lambda \\\\sum\\\\limits_{j=1}^n \\\\theta_j^2`  เข้าไปใน Cost function ซึ่งทั้ง Term นี้มีค่าเป็นบวกเสมอ จึงเป็นการบีบให้ `\\\\sum\\\\limits_{i = 1}^m(h_\\\\theta(x^{(i)}) - y^{(i)})^2` ต้องมีขนาดเล็กลง โดยการที่จะเป็นเช่นนั้นได้ หมายความว่า `h_\\\\theta(x^{(i)})`  จะต้องมีขนาดเล็กลง ซึ่งก็หมายความว่าทางเดียวที่จะทำอย่างนั้นได้คือน้ำหนัก `\\\\theta` (หรือ `w` ในบทที่ 2) จะต้องน้อยลงนั่นเอง โดยการลดขนาด `\\\\theta`  ลง จะเกิดขึ้นในอัตราคงที่ทุกๆ Iteration ดังนั้นเราอาจเรียก Term นี้ว่า Weight decay term ก็ได้\\nเรื่อง Regularisation มีรายละเอียดที่ควรรู้ดังนี้:\\n\\n*  `\\\\lambda` อ่านว่า Lambda คือ Hyperparameter ที่ควบคุมระดับของการ Regularise (Regularisation strength) ยิ่งมาก ยิ่งชดเชยมาก แต่ใน scikit-learn บาง Algorithm เช่น Logistic regression จะใช้ Argument `C` แทน ซึ่งเป็น Inverse ของ `\\\\lambda`  ดังนั่น เวลากำหนด `C` ยิ่งน้อย ยิ่งชดเชยมาก\\n*  `\\\\sum\\\\limits_{j=1}^n \\\\theta_j^2` มาจาก `l^2`  norm ของ Vector ซึ่งเป็นวิธีการวัด \"ระยะทาง\" ของ Vector เพื่อให้รู้ว่า Vector นี้มีขนาดเท่าไหร่ โดย `l^2` norm มีสูตรคือ:\\n\\n```tex\\n|| x ||_2 = \\\\sqrt{\\\\sum_i x_i^2} \\\\tag{5}\\n```\\n\\n* ใน scikit-learn ถ้าเป็น Linear regression ให้ใช้ Ridge regression algorithm ในการทำ Linear regression พร้อมกับ `l^2` regularisation เรียกใช้ได้จาก Class `Ridge` ใน `sklearn.linear_model`\\n* นอกจาก `l^2`  regularisation แล้ว ยังมี `l^1` regularisation ซึ่งใช้ `l^1` norm ที่มีสูตรคือ:\\n\\n```tex\\n|| x ||_1 = \\\\sum_i |x_i| \\\\tag{6}\\n```\\n\\n* ใน scikit-learn ให้ใช้ Lasso regression algorithm ในการทำ Linear regression พร้อมกับ `l^1` regularisation เรียกใช้ได้จาก Class `Lasso` ใน `sklearn.linear_model`\\n* ข้อแตกต่างสำคัญระหว่าง Ridge regression ที่ใช้ `l^2` regularisation กับ Lasso regression ที่ใช้ `l^1` regularisation ก็คือ Lasso มีแนวโน้มจะตัด Feature ที่มีความสำคัญน้อยมากๆ ออกไปเลย ซึ่งก็คือการลดจำนวน Feature อัตโนมัติแทนเรา ตรงกันข้ามกับ Ridge ที่จะรักษา Feature ทั้งหมดไว้\\n* ยังมีอีก Algorithm ชื่อว่า Elastic Net ที่รวม Ridge regression กับ Lasso regression เข้าไว้ด้วยกัน โดยเราสามารถเลือกสัดส่วนผสม `r` ว่าจะใช้ Ridge กี่ส่วน Lasso กี่ส่วน โดย `r = 0` จะได้ Ridge ทั้งหมด ส่วน `r = 1` จะได้ Lasso ทั้งหมด สามารถเรียกใช้ได้ด้วย Class `ElasticNet` ใน `sklearn.linear_model`\\n\\nเป็นอันว่าเราได้รู้จักปัญหา Bias / Variance และรู้วิธีแก้ปัญหาด้วย Regularisation แล้ว ตอนนี้เราก็มีความรู้และเครื่องมือสำหรับการสร้าง Machine learning model ด้วย Algorithm อื่นๆ ซึ่งจะนำเสนอในบทต่อไป\\n\\n## บทที่ 8 Support Vector Machines\\nคราวนี้ก็ถึงเวลาที่จะแนะนำ Algorithm ใหม่ ที่ชื่อ Support Vector Machines หรือ SVM ซึ่งทั้งยึดหยุ่นและทำงานได้ดี โดยเฉพาะอย่างยิ่งเมื่อข้อมูลมีความซับซ้อน (หลาย Feature) แต่จำนวนตัวอย่างไม่มาก (ต่ำกว่าแสนรายการ)\\n\\nปัญหา Binary classification เราต้องการจำแนกข้อมูลออกเป็นสองพวก คือสีน้ำเงินและสีแดง สิ่งที่ SVM ทำ คือการหาเส้นแบ่งการตัดสินใจที่เป็นเส้นทึบ ซึ่งเส้นนี้จะเกิดขึ้นระหว่างกลางของเส้นประด้านซ้ายและขวา โดยมีเงื่อนไขว่าจะต้องหาคู่ของเส้นประที่กว้างที่สุดเท่าที่จะเป็นไปได้\\n\\nโดยคู่ของเส้นประที่กว้างที่สุดเท่าที่จะเป็นไปได้นี้ จะมีสองแบบ คือ 1) Hard margin classification คือคู่เส้นประที่ห้ามไม่ให้มีจุดข้อมูลอยู่ในพื้นที่ระหว่างเส้นประ และ 2) Soft margin classification คืออนุญาตให้มีข้อมูลอยู่ในพื้นที่ระหว่างเส้นประได้บ้าง\\n\\nโดยใน scikit-learn เราสามารถกำหนด Hyperparameter `C` เพื่อเลือกระดับของการอนุญาตให้มีการละเมิดขอบเขตเส้นประ โดยถ้า `C` มาค่าน้อย หมายความว่ายอมให้มีขอบเขตที่กว้างขึ้น นั่นแปลว่ามี Regularisation มากขึ้นนั่นเอง\\n\\nSVM นั้นถึงแม้จะถูกออกแบบมาสำหรับ Binary classification แต่สามารถนำไปประยุกต์ใช้กับ Multiclass classification และ Linear regression ได้โดยง่าย โดยใช้หลักการเดิมแต่เปลี่ยนรายละเอียดเล็กน้อย ซึ่งจะไม่กล่าวถึงในที่นี้ ขอให้รู้ว่าในระดับการใช้งาน สามารถใส่ข้อมูลแบบ Multiclass ลงไปได้เลย ส่วน SVM สำหรับ Linear regression ให้เรียกใช้ Class `LinearSVR` จากโมดูล `sklearn.svm`\\n\\nก่อนที่จะลองทำจริง เรามาทำความเข้าใจกันก่อนว่า SVM ทำงานจริงๆ อย่างไร\\n\\n### SVM algorithm\\n\\nSVM ใช้ Hypothesis function แบบเส้นตรง เหมือนกับ Linear regression นั่นคือ:\\n\\n```tex\\n\\\\begin{align*}\\n    h_\\\\theta(x) & = w_1x_1 + w_2x_2 + \\\\dots + w_nx_n + b \\\\\\\\\\n    & = w^Tx + b\\n\\\\end{align*} \\\\tag{1}\\n```\\n\\nโดยถ้าผลลัพธ์เป็นบวก จะทำนาย Class ŷ  ว่าเป็น 1 ส่วนถ้าเป็นลบ ทำนายว่าเป็น 0 เราสามารถเขียนวิธีการตัดสินใจตามเงื่อนไขดังกล่าวได้ดังนี้:\\n\\n```tex\\n\\\\hat{y} =\\n\\\\begin{cases}\\n    0 \\\\text{ if } w^Tx + b < 0,\\\\\\\\\\n    1 \\\\text{ if } w^Tx + b \\\\geq 0\\n\\\\end{cases} \\\\tag{2}\\n```\\n\\nเมื่อเรานิยามเส้นแบ่งการตัดสินใจแล้ว (เส้นทึบ) เรากำหนดเส้นประทั้งสองด้านของเส้นทึบ โดยเส้นประแต่ละด้านคือตำแหน่งที่  `h_\\\\theta(x)` เท่ากับ -1 และ 1\\n\\nต่อมา ลองพิจารณาว่า ความชันของฟังก์ชันการตัดสินใจ เท่ากับ Norm ของ Vector ค่าน้ำหนัก `w`:\\n\\n```tex\\n\\\\begin{align*}\\n\\\\frac{\\\\partial}{\\\\partial x} h_\\\\theta(x) & = \\\\sum\\\\limits_{i = 1}^m |w_i| \\\\\\\\\\n    & = |w_1| + |w_2| + \\\\ldots + |w_i| \\\\\\\\\\n    & = || w ||_1\\n\\\\end{align*} \\\\tag{3}\\n```\\n\\nดังนั้นถ้าเราหารความชันของฟังก์ชันการตัดสินใจด้วย 2 ก็แปลว่าเราต้องคูณ 2 เข้าไปในความกว้างเส้นประเพื่อให้สมการยังคงเป็นจริง ดังนั้นผลที่ได้คือการขยายความกว้างระหว่างเส้นทึบกับเส้นประออกด้านละ 2 เท่า\\n\\nจากข้อเท็จจริงนี้ เราก็จะได้เป้าหมายว่าเราต้องการลด `||w||` เพื่อให้ได้ขอบเขตเส้นแบ่งที่กว้างที่สุดเท่าที่จะเป็นไปได้ อย่างไรก็ตาม ในเวลาเดียวกันเราไม่ต้องการให้ขอบเขตเส้นแบ่งนั้นกว้างเกินไปจนกระทั่งครอบคลุมจุดข้อมูล ดังนั้นเราจึงต้องการให้ฟังก์ชันการตัดสินใจนั้นมีค่ามากกว่า 1 ในด้านที่ผลพยากรณ์เป็น 1 (\"ใช่\") และน้อยกว่า -1 ในด้านที่ผลพยากรณ์เป็น 0 (\"ไม่ใช่\") โดยถ้าเรากำหนดตัวแปร `t^{(i)} = -1` สำหรับด้าน `\\\\hat{y} = 0` และ `t^{(i)} = 1` สำหรับด้าน `\\\\hat{y} = 1` เราก็จะสามารถเขียนข้อจำกัดนี้ได้ว่า `t^{(i)} (w^T x^{(i)} + b) \\\\geq 1` สำหรับทุก Class คำตอบ\\n\\nดังนั้นเราจะได้เป้าหมายการ Optimise ของ SVM algorithm ว่า:\\n\\n```tex\\n\\\\begin{align*}\\n\\\\text{minimise}_{w, b } & \\\\quad \\\\frac{1}{2} w^T w \\\\\\\\\\n\\\\text{subject to } & \\\\quad t^{(i)} (w^T x^{(i)} + b) \\\\geq 1\\n\\\\end{align*} \\\\tag{4}\\n```\\n\\nสังเกตว่าเรา Minimise `\\\\frac{1}{2} ||w||^2` แทนที่จะเป็น `||w||` เพราะ` \\\\frac{1}{2} ||w||^2` สามารถหาอนุพันธ์ได้ผลเป็น `w` ในขณะที่อนุพันธ์ของ `||w||` เท่ากับ 0 ไม่สามารถนำไป Optimise ได้\\n\\nเป้าหมายนี้ใช้สำหรับ Hard margin SVM แต่ถ้าเป็น Soft margin ที่เราต้องการอนุญาตให้พื้นที่เส้นขอบเขตการตัดสินใจนั้นกินบริเวณที่มีจุดข้อมูลอยู่ด้วยได้ ก็ต้องเพิ่มตัวแปรที่เรียกว่า Slack variable `\\\\zeta^{(i)} \\\\geq 0` (อ่านว่า Zeta) โดยระดับของ Slack variable จะถูกกำหนดโดย Hyperparameter C\\n\\nดังนั้นเป้าหมายการ Optimise สำหรับ Soft margin SVM คือ:\\n\\n```tex\\n\\\\begin{align*}\\n\\\\text{minimise}_{w, b } & \\\\quad \\\\frac{1}{2} w^T w + C\\\\sum\\\\limits_{i = 1}^m\\\\zeta^{(i)}\\\\\\\\\\n\\\\text{subject to } & \\\\quad t^{(i)} (w^T x^{(i)} + b) \\\\geq 1 - \\\\zeta^{(i)}\\n\\\\end{align*} \\\\tag{5}\\n```\\n### Kernel\\nSVM algorithm ที่เรานำเสนอไป มีข้อจำกัดคือสามารถสร้างเส้นแบ่งขอบเขตการตัดสินใจแบบเส้นตรงเท่านั้น ซึ่งอาจทำงานได้ไม่ดีถ้าความสัมพันธ์ของข้อมูลนั้นมีความซับซ้อนจนแบ่งด้วยเส้นตรงไม่ได้ วิธีการแก้ปัญหานี้เรียกว่า Kernel\\n\\nKernel คือ \"ทริค\" ทางคณิตศาสตร์ที่ทำให้ Algorithm สามารถ Optimise ค่าตัวแปรแบบ Polynomial ได้ โดยไม่ต้องไปเปลี่ยนรูปแบบและความสัมพันธ์ของ Feature ตั้งต้น แต่ก่อนอื่น เราจะต้องดัดแปลงเป้าหมายการ Optimise ของเราให้อยู่ในรูปแบบที่ Kernel จะทำงานได้ โดยเปลี่ยนจากรูปแบบในสมการที่ (5) ซึ่งเป็นปัญหาแบบ Primal problem ให้กลายเป็นปัญหาแบบ Dual problem (ปัญหาทั้งสองรูปแบบมีรายละเอียดทางคณิตศาสตร์ที่ละเอียดเกินขอบเขตที่จะอธิบายในที่นี้ ในขั้นนี้ขอเพียงให้รู้ว่า SVM algorithm สามารถใช้ทั้งสองวิธีในการ Optimise ได้ผลเหมือนกัน)\\n\\nรูปแบบของเป้าหมาย Optimise แบบใหม่ที่จะใช้ คือ:\\n\\n```tex\\n\\\\begin{align*}\\n\\\\text{minimise}_{\\\\alpha} & \\\\quad \\\\frac{1}{2} \\\\sum\\\\limits_{i = 1}^m \\\\sum\\\\limits_{j = 1}^m \\\\alpha^{(i)} \\\\alpha^{(j)} t^{(i)} t^{(j)} x^{(i)T} x^{(j)} - \\\\sum\\\\limits_{i = 1}^m\\\\alpha^{(i)}\\\\\\\\\\n\\\\text{subject to } & \\\\quad \\\\alpha^{(i)} \\\\geq 0\\n\\\\end{align*} \\\\tag{6}\\n```\\n\\nเมื่อหา Vector` \\\\hat{\\\\alpha}` ที่ทำให้สมการนี้มีค่าน้อยที่สุดได้แล้ว (โดยใช้ Quadratic Programming Solver ซึ่งเป็น Algorithm สำเร็จรูปในการหาคำตอบปัญหาประเภทนี้โดยเฉพาะ) เราก็จะสามารถคำนวนหา Vector `\\\\hat{w}` และ Intercept `\\\\hat{b}` ที่ทำให้สมการ Primal problem (4) หรือ (5) นั้นมีค่าน้อยที่สุด โดยทำดังนี้:\\n\\n```tex\\n\\\\hat{w} = \\\\sum\\\\limits_{i = 1}^m \\\\alpha^{(i)} t^{(i)} x^{(i)} \\\\tag{7}\\n\\n\\\\hat{b} = \\\\frac{1}{n_s} \\\\sum\\\\limits_{i = 1}^m (t^{(i)} - \\\\hat{w}^T x^{(i)}) \\\\tag{8}\\n```\\n\\nเป็นอันว่าเราได้รูปแบบของเป้าหมาย Optimisation ที่พร้อมสำหรับ Kernel แล้ว ต่อไปคือการอธิบาย Kernel\\n\\nสมมุติว่าเราต้องการทำเหมือนกับว่า Hypothesis function ที่ประกอบด้วยตัวแปร `x_1` และ `x_2` นั้นมีรูปร่างฟังก์ชันที่ซับซ้อนขึ้น โดยต้องการให้เป็น Second-degree polynomial เราจะสามารถเขียน Mapping function (`\\\\phi` อ่านว่า Phi) ของสมการนี้ได้ดังนี้:\\n\\n```tex\\n\\\\phi(x) = \\\\phi\\n\\\\begin{pmatrix}\\n    x_1 \\\\\\\\\\n    x_2\\n\\\\end{pmatrix} =\\n\\\\begin{pmatrix}\\n    x_1^2 \\\\\\\\\\n    \\\\sqrt{2} \\\\; x_1 x_2 \\\\\\\\\\n    x_2^2\\n\\\\end{pmatrix}\\n\\\\tag{9}\\n```\\nทีนี้ลองดูว่า ถ้าเราคำนวน Dot product ของ Vector ขนาด 3 มิติ 2 Vector (แบบเดียวกับที่อยู่ในสมการ (6)) โดยสมมุติว่าชื่อ a และ b จะได้ว่า:\\n\\n```tex\\n\\\\begin{align*}\\n\\\\phi(a)^T \\\\phi(b) & =\\n\\\\begin{pmatrix}\\n    a_1^2 \\\\\\\\\\n    \\\\sqrt{2} \\\\; a_1 a_2 \\\\\\\\\\n    a_2^2\\n\\\\end{pmatrix}^T\\n\\\\begin{pmatrix}\\n    b_1^2 \\\\\\\\\\n    \\\\sqrt{2} \\\\; b_1 b_2 \\\\\\\\\\n    b_2^2\\n\\\\end{pmatrix} \\\\\\\\[2ex]\\n& = a_1^2 b_1^2 + 2a_1b_1a_2b_2 + a_2^2 b_2^2 \\\\\\\\\\n& = (a_1b_1 + a_2b_2)^2 \\\\\\\\\\n& = \\\\left[\\\\begin{pmatrix}\\n    a_1 \\\\\\\\\\n    a_2\\n\\\\end{pmatrix}^T\\n\\\\begin{pmatrix}\\n    b_1 \\\\\\\\\\n    b_2\\n\\\\end{pmatrix}\\\\right]^2 \\\\\\\\\\n& = (a^Tb)^2\\n\\\\end{align*}\\n\\\\tag{10}\\n```\\n\\nก็หมายความว่า เราสามารถหา Dot product ของ Vector ที่อยู่ในรูปแบบ Mapping function (คือยังไม่ได้เปลี่ยนรูปจริงๆ) เพียงด้วยการนำเอา Dot product ของ Vector ต้นฉบับมายกกำลังสอง ซึ่งก็หมายความว่าในทางกลับกัน ถ้าเราคำนวนฟังก์ชันเป้าหมาย Optimisation ด้วยการแทนที่ `(a^Tb)` ด้วย `(a^Tb)^2` เราก็จะได้ผลการคำนวนเสมือนว่าเราได้เปลี่ยนรูป Hypothesis function จาก Linear ให้เป็น Second-degree polynomial โดยไม่ต้องไปเปลี่ยนฟังก์ชันจริง\\n\\nดังนั้น เวลาเราใช้ Kernel ก็เพียงเปลี่ยน `x^{(i)T} x^{(j)}` ในสมการที่ (6) ให้เป็น `(x^{(i)T} x^{(j)})^2`\\n\\nเราเรียกฟังก์ชัน `K(a, b) = (a^Tb)^2` นี้ว่า Second-degree polynomial kernel ซึ่งเป็นหนึ่งใน Kernel หลายแบบที่ทำหน้าที่เดียวกัน ต่างตรงที่แต่ละ Kernel จะเปลี่ยนรูปสมการเป็นรูปแบบที่ไม่เหมือนกัน ทำให้เหมาะกับข้อมูลแต่ละประเภท โดย Kernel เป็นที่นิยม เช่น:\\n\\n```tex\\n\\\\text{Linear:} \\\\quad K(a, b) = a^Tb \\\\tag{11}\\n\\n\\\\text{Polynomial:} \\\\quad K(a, b) = (\\\\gamma a^Tb + r)^d \\\\tag{12}\\n\\n\\\\text{Gaussian RBF:} \\\\quad K(a, b) = e^{(-\\\\gamma ||a-b||^2)} \\\\tag{13}\\n\\n\\\\text{Sigmoid:} \\\\quad K(a, b) = \\\\text{tanh} (\\\\gamma a^Tb + r) \\\\tag{14}\\n```\\n\\nทั้งหมดนี้คือหลักการของ SVM และ Kernel อนึ่งคณิตศาสตร์ในส่วนนี้ ส่วนมากอ้างอิงจากหนังสือ Hands-On Machine Learning with Scikit-Learn & TensorFlow โดย Aurélien Géron\\n\\nทีนี้ก็ถึงเวลามาลองโปรแกรมกันเลย\\n\\n### SVM programming\\nเราจะใช้ชุดข้อมูล Iris เหมือนเดิมในการทดลอง SVM โดยจะใช้ข้อมูล Petal length และ Petal width เท่านั้น เพื่อให้ที่สามารถพล็อตกราฟแสดงขอบเขตการตัดสินใจให้ดูได้\\n\\nเริ่มด้วยการโหลดและพล็อตดูข้อมูล:\\n```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn import datasets\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import LinearSVC\\nfrom sklearn.svm import SVC\\n\\n# Load the iris data\\niris = datasets.load_iris()\\nX = iris.data[:, 2:]\\ny = iris.target\\n\\n# Split the data into train and test set\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\n\\n# Plot the data\\nplt.figure(figsize=(7,6))\\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=\\'Paired_r\\')\\nplt.xlabel(iris.feature_names[2])\\nplt.ylabel(iris.feature_names[3])\\nplt.show()\\n```\\nได้ Scatterplot ของความสัมพันธ์ระหว่าง Petal length และ Petal width โดยจำแนกจุดข้อมูลตาม Class ซึ่งได้แก่ `[\\'setosa\\' \\'versicolor\\' \\'virginica\\']`\\nจากนั้นทดลองเทรนด้วย Linear SVC ซึ่งเป็นเวอร์ชั่นที่ใช้ Hypothesis function แบบเส้นตรง โดยเรียก Class `LinearSVC` จากโมดูล `sklearn.svm`:\\n```python\\n# Model 1: Linear SVC version\\n# Create a pipeline\\nclf_linSVC = Pipeline([\\n    (\"linear_svc\", LinearSVC(C=200, loss=\"hinge\", max_iter=100000))\\n])\\n\\n# Train the model\\nclf_linSVC.fit(X_train, y_train)\\n\\n# Evaluate the model\\'s accuracy\\nprint(\"Train set accuracy = \" + str(clf_linSVC.score(X_train, y_train)))\\nprint(\"Test set accuracy = \" + str(clf_linSVC.score(X_test, y_test)))\\n```\\nทดสอบแล้วได้ค่าความแม่นยำดีทีเดียว:\\n```\\nTrain set accuracy = 0.9375\\nTest set accuracy = 0.9736842105263158\\n```\\nทีนี้เราลองพล็อตพื้นที่และเส้นแบ่งการตัดสินใจของโมเดล โดยสร้างฟังก์ชันชื่อ `plot_decision_boundary` ดังนี้:\\n```python\\n# Plot the decision boundaries\\ndef plot_decision_boundary(clf, X, y, cmap=\\'Paired_r\\'):\\n    h = 0.005  # Boundary lines\\' resolution\\n    x_min, x_max = X[:,0].min() - 10*h, X[:,0].max() + 10*h\\n    y_min, y_max = X[:,1].min() - 10*h, X[:,1].max() + 10*h\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\\n                         np.arange(y_min, y_max, h))\\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\\n    Z = Z.reshape(xx.shape)\\n\\n    plt.figure(figsize=(7,6))\\n    plt.contourf(xx, yy, Z, cmap=cmap, alpha=0.25)  # Background\\n    plt.contour(xx, yy, Z, colors=\\'k\\', linewidths=0.2)  # Boundary lines\\n    plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap);  # Data points\\n    plt.xlabel(iris.feature_names[2])\\n    plt.ylabel(iris.feature_names[3])\\n\\nplot_decision_boundary(clf_linSVC, X, y)\\n```\\nจะสังเกตว่าการที่ขอบเขตการตัดสินใจเป็นเส้นตรง ทำให้ไม่สามารถฟิตขอบเขตระหว่าง Class สีแดงกับสีฟ้าได้ดี ส่งผลต่อความแม่นยำของโมเดล ดังนั้น เรามาลองใช้ Kernel เพื่อทำให้ขอบเขตการตัดสินใจนั้นมีความโค้งรับกับความสัมพันธ์ของข้อมูล\\n\\nโดยเราจะใช้ Gaussian RBF kernel เรียกใช้จาก Class `SVC` ในโมดูล `sklearn.svm` โดยกำหนด Argument `kernel=\"rbf\"`:\\n```python\\n# Model 2: Gaussian RBF Kernel version\\n# Create a pipeline\\nclf_SVC = Pipeline([\\n    (\"linear_svc\", SVC(kernel=\"rbf\", gamma=2, C=10, max_iter=10000))\\n])\\n\\n# Train the model\\nclf_SVC.fit(X_train, y_train)\\n\\n# Evaluate the model\\'s accuracy\\nprint(\"Train set accuracy = \" + str(clf_SVC.score(X_train, y_train)))\\nprint(\"Test set accuracy = \" + str(clf_SVC.score(X_test, y_test)))\\n```\\nสังเกตว่ามี Argument gamma ซึ่งก็คือ Hyperparameter `\\\\gamma` ใน Kernel function `K(a, b) = e^{(-\\\\gamma ||a-b||^2)}` โดย gamma นี้กำหนดความแคบของส่วนโค้งซึ่งมีรูปร่างตามการกระจายแบบ Gaussian\\n\\nผลความแม่นยำที่ได้ คือ 100%:\\n```\\nTrain set accuracy = 0.9553571428571429\\nTest set accuracy = 1.0\\n```\\nต่อมาลองพล็อตพื้นที่และเส้นแบ่งการตัดสินใจของโมเดลดู:\\n```python\\n# Plot the decision boundaries\\nplot_decision_boundary(clf_SVC, X, y)\\n```\\nจะเห็นว่า Kernel นั้นแก้ปัญหาของเราได้จริง เพราะทำให้เส้นการตัดสินใจนั้นโค้งรับกับข้อมูล ส่งผลให้ความผิดพลาดลดลง\\n\\nเป็นอันว่าเราได้รู้จัก Support Vector Machines รวมทั้ง Kernel ที่ช่วยให้ Algorithm สามารถเรียนรู้การตัดสินใจข้อมูลที่ซับซ้อนได้ ตอนต่อไปจะมาทำความรู้จักกับอีก Algorithm ที่ใช้งานได้ดีในหลายๆ กรณีเช่นกัน คือ Decision tree\\n\\n## บทที่ 9 Decision Tree\\nDecision tree เป็น Algorithm ที่เป็นที่นิยม ใช้ง่าย เข้าใจง่าย ได้ผลดี และเป็นฐานของ Random Forest ซึ่งเป็นหนึ่งใน Algorithm ที่ดีที่สุดในปัจจุบัน\\n\\nหลักการพยากรณ์ด้วย Decision tree นั้นเข้าใจง่ายมาก ให้นึกว่า Decision tree คือต้นไม้กลับหัว โดยบนสุดคือราก และส่วนล่างลงมาที่ไม่สามารถแตกไปไหนได้แล้วก็คือใบ เราจะเริ่มด้วยการพิจารณาเริ่มแรกบนจุดเริ่มต้นที่เรียกว่า Root node ถ้าข้อมูลที่พบเป็นไปตามเงื่อนไขนั้น การตัดสินใจก็จะวิ่งไปทางซ้ายของ Root node ไปที่จุดที่เรียกว่า Child node ซึ่งถ้าข้อมูลที่มาตามเส้นทางนี้ตรงตามเงื่อนไขของ Child node นี้ ก็จะถือว่าสิ้นสุด เราเรียกว่า Node สิ้นสุดว่า Leaf node\\n\\nย้อนกลับไปยัง Root node ถ้าข้อมูลที่พิจารณาไม่เป็นไปตามเงื่อนไข การตัดสินใจจะวิ่งไปอีกทาง คือทางขวา ไปพบ Child node อีกอันซึ่งก็จะตั้งเงื่อนไขคำถามต่อไป การตัดสินใจก็จะวิ่งไปทางที่ตรงตามเงื่อนไข ทำอย่างนี้ไปเรื่อยๆ จนได้คำตอบ\\n\\nสำหรับกลไกการเทรนโมเดลเพื่อสร้าง Decision tree จะอธิบายด้านล่าง เมื่อได้อธิบายเรื่องค่า Gini\\n\\nก่อนจะไปไกลกว่านั้น จะชี้ให้เห็นว่า Decision tree นั้นไม่ต้องใช้ข้อมูลที่ทำ Feature scaling เพราะไม่ได้มี Optimisation algorithm แบบทั่วไป จึงใช้งานสะดวกมาก\\n\\nเพื่อความเข้าใจมากขึ้น ลองมา Program ด้วย scikit-learn กันเลย โดยใช้ชุดข้อมูล Iris เหมือนเดิม เพื่อความเรียบง่ายจะเลือก Feature เฉพาะ Petal length และ Petal width:\\n```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn import datasets\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.tree import export_graphviz\\n\\n# Load the iris data\\niris = datasets.load_iris()\\nX = iris.data[:, 2:]\\ny = iris.target\\n\\n# Split the data into train and test set\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\n```\\nแล้วก็ลองเทรนโมเดลกันเลย:\\n```python\\n# Train and fit the model\\ntree_clf = DecisionTreeClassifier(max_depth=5).fit(X_train, y_train)\\n\\n# Evaluate the model\\'s accuracy\\nprint(\"Train set accuracy = \" + str(tree_clf.score(X_train, y_train)))\\nprint(\"Test set accuracy = \" + str(tree_clf.score(X_test, y_test)))\\n```\\nสังเกตว่ามี Argument `max-depth` นั่นคือตัวควบคุมว่าจะให้ต้นไม้ของเรามีความลึกกี่ขั้น ถ้าเราพบว่าโมเดลของเรา Overfit ก็ควรลองลดจำนวนชั้นของความลึกลง ดังนั้น `max-depth` จึงเป็น Regularisation hyperparameter ของ Decision trees\\n\\nสำหรับความคลาดเคลื่อน พบว่าได้ความแม่นยำจาก Method `.score` ดังนี้:\\n```\\nTrain set accuracy = 0.9821428571428571\\nTest set accuracy = 1.0\\n```\\nคือแม่นยำ 100% ทีเดียว\\n\\nจากนั้นเรามาลองสร้างกราฟการตัดสินใจ โดยใช้ฟังก์ชัน `export_graphviz` ในโมดูล `sklearn.tree` เพื่อ Export กราฟออกมาด้วย Graphviz ซึ่งเป็น Open-source graph visualisation software:\\n```python\\n# Export graph\\nexport_graphviz(tree_clf, out_file=\"iris_tree.dot\",\\n               feature_names=iris.feature_names[2:],\\n               class_names=iris.target_names,\\n               rounded=True, filled=True)\\n```\\nฟังก์ชันนี้จะ Export กราฟออกมาเป็นไฟล์นามสกุล `.dot` ซึ่งเราต้องใช้คำสั่ง `dot` จาก Graphviz package เพื่อแปลงไฟล์เป็น `.png` โดยเรียกคำสั่งจาก Command line:\\n\\n`$ dot -Tpng iris_tree.dot -o iris_tree.png`\\n\\nถ้าอ่านตามแผนภาพก็น่าจะพอเข้าใจคำอธิบายข้างต้นว่า Decision tree พยากรณ์อย่างไร อย่างไรก็ตาม ในแต่ละ Node มีรายละเอียดที่ควรรู้เพื่อเพิ่มความเข้าใจดังนี้:\\n\\n* `samples` คือจำนวนรายการข้อมูลที่เข้ากันได้กับ Node นั้น ดังนั้น เมื่อการตัดสินใจเคลื่อนลงไปตามความลึกของต้นไม้ จำนวน `samples` ของ Node ในแต่ละชั้นจะมีแนวโน้มที่จะลดลงเรื่อยๆ\\n* `gini` บ่งชี้ความ \"บริสุทธิ์\" ของ Node โดย `gini = 0` หมายความว่าข้อมูลทุกรายการใน Node นั้นอยู่ใน Class เดียวกัน ส่วน `gini = 0.5` ก็แปลว่ารายการข้อมูลใน Node นั้นอยู่ใน 2 Class เท่าๆ กัน โดยแสดงผ่าน `value` เช่น `value = [0, 39, 38]` ใน Child node ด้านขวาของ Root node แปลว่า จากข้อมูล 77 รายการที่เข้าเงื่อนไข Node นี้ มี 39 รายการที่อยู่ใน Class `versicolor` และ 38 รายการอยู่ใน Class `virginica` โดยถ้าหยุดพิจารณาที่ขั้นนี้ ก็จะถือว่าข้อมูลที่เข้าเงื่อนไขของ Node นี้อยู่ใน Class `versicolor` เป็นต้น\\n\\nเพื่อให้เป็นประโยชน์ในการทำความเข้าใจ Algorithm ในส่วนถัดไป เราสามารถบอกได้ว่าค่า `gini` คำนวนตามสูตรนี้:\\n\\n`G_i = 1 - \\\\sum\\\\limits_{k = 1}^{n} p_{i, k}^2 \\\\tag{1}`\\n\\nโดย `p_{i, k}` คือสัดส่วนว่าจากจำนวนรายการข้อมูลใน Node ที่ `i` นั้นอยู่ใน Class `k` กี่รายการ\\n\\nมาถึงจุดนี้เราก็พร้อมที่จะมาทำความเข้าใจว่า Decision tree algorithm นั้นสร้างโมเดลได้อย่างไร โดย scikit-learn จะใช้ Algorithm ที่ชื่อ Classification And Regression Tree (CART) ซึ่งทำงานตามลำดับดังนี้:\\n\\n1) แบ่ง Train set ออกเป็น 2 ส่วนโดยเลือก Class k และเงื่อนไข `t_k` เช่น petal length <= 2.45 โดยค้นหาคู่ของ `k` และ `t_k` ที่จะได้ Node ที่ \"บริสุทธิ์\" ที่สุด นั่นคือมีค่า Gini ต่ำที่สุดนั่นเอง เราสามารถแสดง Cost function ที่สอดคล้องกับเงื่อนไขนี้ได้ดังนี้:\\n\\n`J(k, t_k) = \\\\frac{m_{left}}{m} G_{left} + \\\\frac{m_{right}}{m} G_{right} \\\\tag{2}`\\n- `G_{left/right}` คือ gini ของข้อมูลชุดซ้ายและขวาที่ถูกแบ่ง\\n- `m_{left/right}` คือจำนวนรายการข้อมูลในชุดซ้ายและขวา\\n- `m` คือจำนวนรายการข้อมูลทั้งหมดใน Node นั้น\\n\\n2) แยกข้อมูลแต่ละชุดย่อยออกเป็นสองชุดและทำซ้ำข้อ 1) เรื่อยๆ จนกระทั่งถึงความลึก max-depth ที่กำหนด หรือจนกระทั่งไม่พบค่า `k` และ `t_k` ที่จะลดความไม่บริสุทธิ์ได้อีกต่อไป\\n\\nตอนนี้เราก็เข้าใจแล้วว่า Decision tree สร้างโมเดลอย่างไร สุดท้ายเรามาลองพล็อตเส้นแบ่งการตัดสินใจจากตัวอย่างของเรา:\\n```python\\n# Plot the decision boundaries\\ndef plot_decision_boundary(clf, X, y, cmap=\\'Paired_r\\'):\\n    h = 0.005  # Boundary lines\\' resolution\\n    x_min, x_max = X[:,0].min() - 10*h, X[:,0].max() + 10*h\\n    y_min, y_max = X[:,1].min() - 10*h, X[:,1].max() + 10*h\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\\n                         np.arange(y_min, y_max, h))\\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\\n    Z = Z.reshape(xx.shape)\\n\\n    plt.figure(figsize=(7,6))\\n    plt.contourf(xx, yy, Z, cmap=cmap, alpha=0.25)  # Background\\n    plt.contour(xx, yy, Z, colors=\\'k\\', linewidths=0.2)  # Boundary lines\\n    plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap);  # Data points\\n    plt.xlabel(iris.feature_names[2])\\n    plt.ylabel(iris.feature_names[3])\\n\\nplot_decision_boundary(tree_clf, X, y)\\n```\\nจะเห็นว่า Decision tree สร้างเส้นแบ่งการตัดสินใจที่เป็นเส้น Orthogonal คือแนวระนาบและแนวดิ่งที่ตั้งฉากกันเท่านั้น ดังนั้นในบางกรณีจะเป็นไปไม่ได้เลยที่ Decision tree จะทำนาย Test set ได้แม่นยำ 100%\\n\\nข้อสังเกตนี้สอดคล้องกับธรรมชาติของ CART algorithm ที่ค้นหา Parameter ที่ดีที่สุดจากบนลงล่างโดยตัดสินใจจากเงื่อนไขที่พบในลำดับชั้นปัจจุบันเท่านั้น โดยไม่ได้เช็คว่าการตัดสินใจในชั้นนั้นจะส่งผลให้ลำดับชั้นล่างๆ ลงมามีค่าความไม่บริสุทธิ์น้อยที่สุดหรือไม่ เราเรียกพฤติกรรมแบบนี้ว่า Greedy algorithm ซึ่งพฤติกรรมแบบนี้ทำให้ต้องใช้เวลานานจนแทบเป็นอนันต์จึงจะหาต้นไม้ที่ดีที่สุดได้ โดยใช้เวลาถึง O(e^m) ในทางคณิตศาสตร์เรียกปัญหานี้ว่า ปัญหา NP-Complete ดังนั้นเมื่อเราใช้ Decision tree เราจึงต้องยอมรับผลลัพธ์ที่อาจจะไม่สมบูรณ์แบบ แต่ส่วนมากก็ดีพอสำหรับงานส่วนมาก\\n\\nบทต่อไปเราจะเรียนรู้ Random Forest ซึ่งคือการนำต้นไม้หลายๆ ต้นมารวมกันเป็นป่า เพื่อเพิ่มความแม่นยำในการ Generalise โมเดลให้ทำงานได้ดีขึ้นกับข้อมูลที่โมเดลไม่เคยเห็น\\n\\n## บทที่ 10 Random Forest\\nRandom forest เป็นหนึ่งในกลุ่มของโมเดลที่เรียกว่า Ensemble learning ที่มีหลักการคือการเทรนโมเดลที่เหมือนกันหลายๆ ครั้ง (หลาย Instance) บนข้อมูลชุดเดียวกัน โดยแต่ละครั้งของการเทรนจะเลือกส่วนของข้อมูลที่เทรนไม่เหมือนกัน แล้วเอาการตัดสินใจของโมเดลเหล่านั้นมาโหวตกันว่า Class ไหนถูกเลือกมากที่สุด\\n\\nฟังดูเหมือนไม่น่าจะดี แต่ในความเป็นจริงแล้ว กลไกการรวมการตัดสินใจของผู้ตัดสินใจจำนวนมากเข้าด้วยกันมักจะให้ผลการตัดสินใจที่แม่นยำมากกว่าการพึงพาการตัดสินใจจากแหล่งเดียว ปรากฏการณ์นี้เป็นจริงในหลายมิติ เช่นในทางสังคม เราเรียกว่า \"ปัญญาของฝูงชน\" (Wisdom of the crowd) ซึ่งหากสนใจสามารถค้นคำนี้อ่านได้ทั่วไป\\n\\nการเรียนรู้แบบ Ensemble นี้จะทำงานได้ดีบนเงื่อนไขที่ว่า โมเดลผู้ทำนายแต่ละตัวจะต้องเรียนรู้อย่างเป็นอิสระต่อกันให้มากที่สุด เหมือนกับเงื่อนไขของปัญญาของฝูงชน ว่าคนแต่ละคนจะต้องตัดสินใจด้วยตนเองให้มากที่สุดโดยไม่ได้รับข้อมูลจากคนอื่นหรือนำเอาข้อมูลจากคนอื่นมาเป็นส่วนในการตัดสินใจ\\n\\nใน Machine learning algorithm เรามีวิธีการที่ทำให้การตัดสินใจของแต่ละโมเดลเป็นอิสระต่อกัน โดยการใช้ Algorithm เดียวกัน แต่ให้แต่ละ Instance เรียนรู้จากส่วนของข้อมูลที่ไม่เหมือนกันโดยใช้การสุ่มเลือก กลไกนี้เรียกว่า Bagging และ Pasting โดยสิ่งที่ต่างกันคือ Bagging สามารถสุ่มเลือกข้อมูลรายการเดียวกันได้ แต่ Pasting ไม่อนุญาตให้สุ่มรายการซ้ำกันได้เลย ในทางปฏิบัติ Bagging จะลด Variance ของโมเดลได้ดีกว่า เพราะมีการเลือกรายการข้อมูลซ้ำ ทำให้ได้โมเดลที่เสถียรกว่าและมักจะแม่นยำกว่า Pasting\\n\\nสำหรับ scikit-learn โมเดลแบบ Bagging จะจะใช้วิธีการสุ่มเลือกรายการข้อมูลแบบ Bootstrap โดยแต่ละ Instance จะเลือก 63% ของข้อมูล เหลือ 37% ที่แต่ละ Instance ไม่เห็น เราเรียก 37% นี้ว่า Out-of-bag (oob) instance การที่มี oob instance ทำให้เราสามารถประเมินความแม่นยำเฉลี่ยของทุกๆ Instance ได้ในระหว่างเทรน โดยการเรียก Argument `oob_score=True` ใน Classifier instance\\n\\nเมื่อเข้าใจหลักการทำงานแล้ว มาลองสร้างโมเดลกันเลยดีกว่า โดยครั้งนี้เราจะเลือกชุดข้อมูลที่ยากและซับซ้อนขึ้นเล็กน้อย คือชุดข้อมูล Breast cancer wisconsin (diagnostic) dataset ซึ่งวัดค่าต่างๆ จากตัวอย่างก้อนเนื้อที่เจาะออกมา แล้วจำแนกว่าก้อนเนื้อนั้นเป็นเซลมะเร็งเต้านมหรือไม่\\n\\nข้อมูลมี 30 Feature ได้แก่ `[\\'mean radius\\' \\'mean texture\\' \\'mean perimeter\\' \\'mean area\\' \\'mean smoothness\\' \\'mean compactness\\' \\'mean concavity\\' \\'mean concave points\\' \\'mean symmetry\\' \\'mean fractal dimension\\' \\'radius error\\' \\'texture error\\' \\'perimeter error\\' \\'area error\\' \\'smoothness error\\' \\'compactness error\\' \\'concavity error\\' \\'concave points error\\' \\'symmetry error\\' \\'fractal dimension error\\' \\'worst radius\\' \\'worst texture\\' \\'worst perimeter\\' \\'worst area\\' \\'worst smoothness\\' \\'worst compactness\\' \\'worst concavity\\' \\'worst concave points\\' \\'worst symmetry\\' \\'worst fractal dimension\\']` โดยมีจำนวน 569 รายการ จำแนกเป็น Malignant คือเป็นเซลมะเร็ง 212 รายการ และ Benign คือไม่เป็นมะเร็ง 357 รายการ\\n\\nเราสามารถเรียกข้อมูลชุดนี้จาก `sklearn.datasets` ได้เลย:\\n\\n```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn import datasets\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\n# Load the breast cancer data\\nbreast_ca = datasets.load_breast_cancer()\\n\\n# Explore the data\\nprint(breast_ca.keys())\\nprint(breast_ca[\"DESCR\"])\\nprint(breast_ca[\"feature_names\"])\\nprint(breast_ca[\"target_names\"])\\n\\n# Assign input X and target y\\nX = breast_ca.data\\ny = breast_ca.target\\n```\\n\\nไหนๆ เราก็ศึกษามาหลายโมเดลแล้ว เรามาลองเปรียบเทียบความสามารถของแต่ละโมเดลกันทีเดียวไปเลยดีกว่า โดยเปรียบเทียบระหว่าง SVC with Gaussian RBF Kernel / Decision Tree / Random Forest\\n\\n### SVC with Gaussian RBF Kernel\\nมาลองโมเดลแรกกัน การสร้างโมเดล Support Vector Machine ต้องทำ Feature scaling ก่อน ถ้าอยากรู้ว่าไม่ทำ Feature scaling แล้วเป็นอย่างไรก็สามารถลองกันเองได้:\\n```python\\n# Feature scaling for SVC\\nX_scaler = StandardScaler().fit(X_train)\\nX_train_scaled = X_scaler.transform(X_train)\\nX_test_scaled = X_scaler.transform(X_test)\\n```\\nถ้ายังจำได้ วิธีที่ดีถูกต้องในทางเทคนิคในการทำ Feature scaling คือเรียก `StandardScaler` ให้ฟิตกับข้อมูล `X_train` ก่อนเพื่อสร้าง Instance `X_scaler` ที่จำเอา Parameter ในการ Scale เอาไว้ก่อนโดยไม่นำข้อมูล `X_test` เข้ามาเป็นส่วนหนึ่งของ Parameter จากนั้นใช้ Instance นี้เรียก `.transform` ข้อมูลจริงจากทั้ง `X_train` และ `X_test` เพื่อให้ข้อมูลทั้งสองชุดถูก Scale ด้วย Mean และ Variance แบบเดียวกัน\\n\\nต่อมาก็เทรนโมเดล SVC with Gaussian RBF Kernel:\\n```python\\n# Train and fit the model: SVC with Gaussian RBF Kernel\\nsvc_clf = SVC(kernel=\"rbf\", gamma=\"auto\", C=1, max_iter=-1).fit(X_train_scaled, y_train)\\n\\n# Evaluate the model\\'s accuracy\\nprint(\"For SVC with Gaussian RBF Kernel model:\")\\nprint(\"Train set accuracy = \" + str(svc_clf.score(X_train_scaled, y_train)))\\nprint(\"Test set accuracy = \" + str(svc_clf.score(X_test_scaled, y_test)))\\n```\\nได้ความแม่นยำบน Test set ถึง 97.2%\\n```\\nFor SVC with Gaussian RBF Kernel model:\\nTrain set accuracy = 0.9882629107981221\\nTest set accuracy = 0.972027972027972\\n```\\n### Decision tree\\nโมเดลที่สอง เริ่มกันเลย:\\n```python\\n# Train and fit the model: decision tree\\ntree_clf = DecisionTreeClassifier(max_depth=None).fit(X_train, y_train)\\n\\n# Evaluate the model\\'s accuracy\\nprint(\"For Decision Tree model:\")\\nprint(\"Train set accuracy = \" + str(tree_clf.score(X_train, y_train)))\\nprint(\"Test set accuracy = \" + str(tree_clf.score(X_test, y_test)))\\n\\n# Determine features\\' importances\\nprint(\"\\\\nImportance of each feature:\\\\n\", tree_clf.feature_importances_)\\n```\\nได้ผลดังนี้:\\n```\\nFor Decision Tree model:\\nTrain set accuracy = 1.0\\nTest set accuracy = 0.951048951048951\\n\\nImportance of each feature:\\n [0.         0.         0.         0.         0.         0.\\n 0.         0.70264385 0.         0.         0.         0.\\n 0.         0.01277192 0.00155458 0.         0.         0.01702539\\n 0.         0.         0.0877369  0.12550655 0.         0.03452044\\n 0.00985664 0.         0.         0.00838371 0.         0.        ]\\n```\\nจะเห็นว่าโมเดลที่เทรนด้วย Decision tree มีปัญหา Overfitting ไม่ว่าจะปรับ Hyperparameter อย่างไรก็ไม่สามารถเพิ่มความแม่นยำ Test set ให้เกิน 95.1% ได้ ทั้งๆ ที่โมเดลสามารถฟิตกับ Train set ได้ 100% ปัญหานี้เป็นแรงจูงใจให้เราใช้ Random forest ซึ่งน่าจะให้ผลดีกว่า ซึ่งเดี๋ยวเราจะมาลองกัน\\n\\nสังเกตว่าเราได้คำนวนสิ่งที่เรียกว่า Feature importances ด้วย ซึ่งเป็นความสามารถพิเศษของโมเดลตระกูล Decision tree โดย Feature importances จำชี้วัด \"ความสำคัญ\" ของแต่ละ Feature ในลักษณะคล้ายกับการหา Correlation โดยวัดว่า Node ต่างๆ ที่ใช้ Feature นั้นสามารถลดค่าความไม่บริสุทธิ์ (ลด `gini`) ได้เท่าไหร่ โดยจากตัวอย่างจะเห็นว่า Feature ลำดับที่ 7 และ 21 ดูเหมือนจะมีความสำคัญมากที่สุด อย่างไรก็ตามเดี๋ยวเราจะมาลองดูกันอีกทีตอนเทรน Random forest ว่าจะได้ผลเหมือนกันหรือไม่\\n### Random forest\\nและแล้วก็มาถึงคิวของ Random forest เราสามารถเรียกโมเดลได้จาก Class `RandomForestClassifier` ในโมดูล `sklearn.ensemble`:\\n```python\\n    # Train and fit the model: random forest classifier\\n    rforest_clf = RandomForestClassifier().fit(X_train, y_train)\\n    \\n    # Evaluate the model\\'s accuracy\\n    print(\"For Random Forest model:\")\\n    print(\"Train set accuracy = \" + str(rforest_clf.score(X_train, y_train)))\\n    print(\"Test set accuracy = \" + str(rforest_clf.score(X_test, y_test)))\\n    \\n    # Determine features\\' importances\\n    print(\"\\\\nImportance of each feature:\\\\n\", rforest_clf.feature_importances_)\\n```\\nมาดูผลลัพธ์กันเลย:\\n```\\nFor Random Forest model:\\nTrain set accuracy = 1.0\\nTest set accuracy = 0.972027972027972\\n\\nImportance of each feature:\\n [0.0203572  0.02322627 0.02598635 0.04133143 0.0066621  0.01541918\\n 0.07545279 0.16202326 0.00550662 0.00652847 0.00736119 0.00434392\\n 0.00986888 0.04103056 0.00350675 0.00480023 0.00873174 0.00562927\\n 0.00371145 0.00552231 0.10679671 0.02272615 0.11522445 0.09619906\\n 0.01558007 0.0118116  0.02579207 0.115044   0.00852107 0.00530486]\\n```\\nเราได้โมเดลที่แม่นยำ 97.2% เทียบเท่า SVC with Gaussian RBF Kernel\\n\\nทีนี้ลองพิจารณา Feature importances จะพบว่าโมเดลสามารถให้ค่าความสำคัญได้ทุก Feature ไม่มีการตัดออกเหลือ 0 เหมือน Decision tree โดดๆ เพราะ Random forest ใช้ต้นไม้หลายๆ ต้นมาเฉลี่ยกันคิด ทีนี้ลองดูว่า Feature ไหนมีค่าความสำคัญมากที่สุด พบว่าคือ Feature ลำดับที่ 7, 22, 27, และ 20 ตามลำดับ อย่างไรก็ตาม ทุกครั้งที่เทรนโมเดล ค่าเหล่านี้จะเปลี่ยนไป เพราะโมเดลทำการโดยการสุ่มเลือกรายการข้อมูลซึ่งไม่เหมือนกันเลยในแต่ละครั้ง (หากไม่ได้กำหนด `random_state`)\\n### Decision boundary\\nต่อมาเราจะลองพล็อตเส้นแบ่งการตัดสินใจ โดยค่า Feature importances เป็นข้อมูลให้เราสามารถเลือกได้ว่าเราจะใช้ข้อมูลหัวเรื่องไหนไปพล็อตกราฟที่อธิบายการทำงานและการตัดสินใจของโมเดล โดยต้องเลือกเพียง 2 Feature เท่านั้น เพราะกราฟของเรามี 2 มิติ\\n\\nเริ่มด้วยการสร้าง `X` ใหม่ที่มีแค่ 2 Feature โดยเราจะเลือกลำดับที่ 7 (Mean concave points) และ 20 (Worst radius):\\n```python\\n# Choose only X from column 7 and 20\\nX_selected = breast_ca.data[:, [7,20]]\\nprint(X_selected[:3, :])\\n```\\nจากนั้นเทรนโมเดลตามปกติ โดยเปลี่ยน Input ของโมเดล จาก `X_train` เป็น `X_train_select` ซึ่งถูกคัดแยกด้วย `train_test_split` มาจาก Input `X_selected` ตอนแรก:\\n```python\\n# Split the selected data into train and test set\\nX_train_select, X_test_select, y_train, y_test = train_test_split(X_selected, y, random_state=42)\\n\\n# Train and fit the model: decision tree\\nrforest_clf = RandomForestClassifier(n_estimators=50).fit(X_train_select, y_train)\\n\\n# Evaluate the model\\'s accuracy\\nprint(\"For Random Forest model (with selected X):\")\\nprint(\"Train set accuracy = \" + str(rforest_clf.score(X_train_select, y_train)))\\nprint(\"Test set accuracy = \" + str(rforest_clf.score(X_test_select, y_test)))\\n```\\nหลายคนอาจจะสนใจว่าถ้าเทรนโมเดลด้วย 2 Feature จาก 30 จะได้ผลอย่างไร คำตอบอยู่ที่นี่:\\n```\\nFor Random Forest model (with selected X):\\nTrain set accuracy = 1.0\\nTest set accuracy = 0.916083916083916\\n```\\nก็คือ 91.6% ซึ่งน้อยลงจาก 97.2% พอสมควร แต่ก็ถือว่าดีมาก เพราะเราใช้ข้อมูลเพียง 2 หัวเรื่องมาเทรน\\n\\nลองมานำผลที่ได้ไปพล็อตโดยใช้ฟังก์ชัน `plot_decision_boundary` ที่เคยเขียนไว้:\\n```python\\n# Plot the decision boundaries\\ndef plot_decision_boundary(clf, X, y, cmap=\\'Paired_r\\'):\\n    h = 0.003  # Boundary lines\\' resolution\\n    x_min, x_max = X[:,0].min() - 10*h, X[:,0].max() + 10*h\\n    y_min, y_max = X[:,1].min() - 10*h, X[:,1].max() + 10*h\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\\n                         np.arange(y_min, y_max, h))\\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\\n    Z = Z.reshape(xx.shape)\\n\\n    plt.figure(figsize=(12,9))\\n    plt.contourf(xx, yy, Z, cmap=cmap, alpha=0.25)  # Background\\n    plt.contour(xx, yy, Z, colors=\\'k\\', linewidths=0.2)  # Boundary lines\\n    scatter = plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap);  # Data points\\n    plt.xlabel(breast_ca.feature_names[7])\\n    plt.ylabel(breast_ca.feature_names[20])\\n    plt.legend(*scatter.legend_elements())\\n\\nplot_decision_boundary(rforest_clf, X_selected, y)\\n\\n# Help remind legend names\\ncount = 0\\n\\nfor i in breast_ca[\"target_names\"]:\\n    print(count, \"means\", i)\\n    count += 1\\n```\\nFor loop ในส่วนท้ายเป็นเพียงการแสดงว่าหมายเลข Class ไหนหมายถึงอะไร เวลาอ่านพล็อตจะได้ไม่งง:\\n```\\n0 means malignant\\n1 means benign\\n```\\nสังเกตว่าบริเวณรอยต่อระหว่าง Class 0 กับ 1 ซึ่งมีข้อมูลทั้งสอง Class ปะปนกัน Random forest พยายามอย่างมากที่จะตัดสินใจโดยโหวตจากการตัดสินใจของต้นไม้แต่ละต้นในป่า\\n\\nยังมี Ensemble learning algorithm ที่ฉลาดขึ้นไปอีก โดยใช้หลักการ Boosting ที่ทำงานโดยการให้ Classifier แต่ละ Instance ทำงานแก้ไขข้อผิดพลาดของ Instance ก่อน แทนที่จะทำงานพร้อมกันโดยการสุ่มเลือกรายการข้อมูล เราจะศึกษาและทดลอง Boosting algorithm ในบทต่อไป\\n## บทที่ 11 Boosting\\nBoosting เป็นอีกเทคนิคใน Ensemble learning ที่ใช้ Classifier หลายๆ Instance มาช่วยกันสร้างโมเดลและพยากรณ์\\n\\nการอธิบาย Boosting ให้เข้าใจง่าย น่าจะลองเปรียบเทียบว่ามันต่างกับ Random forest อย่างไร ทั้งคู่เป็น Ensemble learning เหมือนกัน โดย Random forest จะใช้ Classifier หลาย Instance สร้างโมเดลและทำนายพร้อมกัน โดยใช้ \"กฎของจำนวนขนาดใหญ่\" (Law of large numbers) เป็นคุณสมบัติที่ทำให้การทำนายนั้นแม่นยำกว่าการใช้ Classifier เดี่ยวๆ\\n\\nส่วน Boosting นำ Classifier หลายตัวมาทำงานเป็นโซ่ต่อกัน โดยแต่ละตัวจะแก้ไขจุดด้อยของ Classifier ตัวก่อนหน้า พอเทรนเสร็จแล้ว Classifier ทุกตัวจะพยากรณ์ร่วมกัน\\n\\nในบทนี้จะแนะนำ Boosting algorithm ที่เป็นที่นิยมสองตัว คือ AdaBoost และ GradientBoosting\\n#### AdaBoost\\nAdaBoost เป็น Algorithm พื้นฐานของ Boosting method จึงควรลองทำความเข้าใจเพื่อเป็นฐานสำหรับ Algorithm อื่น\\n\\nหลักการทำงานของ AdaBoost คือการใช้ Classifier ที่ไม่ซับซ้อน เช่น Decision tree ที่มีชั้นเดียว (เรียกว่า Decision stump) หลายๆ Instance มาเทรนต่อกันเป็นลูกโซ่ โดยในการเทรนแต่ละรอบจะมีการกำหนดค่าน้ำหนักของข้อมูลแต่ละรายการ Classifier instance จะเรียนรู้จากค่าน้ำหนักเหล่านั้น โดยถ้าพยากรณ์ผิด ค่าน้ำหนักของรายการนั้นจะมีค่ามากขึ้น ส่งผลให้ Classifier instance นั้นได้รับ \"คะแนน\" ต่ำ แต่ในทางกลับกัน ถ้า Instance ไหนพยากรณ์ถูกเป็นสัดส่วนที่มาก ก็จะได้คะแนนมาก\\n\\nการพยากรณ์ของ AdaBoost คือการถ่วงคะแนนเข้ากับคำตอบของแต่ละ Instance แล้วเลือกคำตอบจาก Class ที่ได้รวมแล้วได้ค่าน้ำหนักมากที่สุด\\n\\nขั้นตอนการทำงานอย่างละเอียดของ AdaBoost มีดังนี้:\\n\\n1) กำหนดน้ำหนัก `w^{(i)}` ตั้งต้นของข้อมูลแต่ละรายการเป็น 1/m โดย m คือจำนวนรายการข้อมูลทั้งหมด:\\n\\n`w^{(i)} = \\\\frac{1}{m} \\\\tag{1}`\\n\\nเช่น ถ้ามีข้อมูล 500 รายการ น้ำหนักของ `w^{(1)}`,` w^{(2)}`, `\\\\dots`, `w^{(500)} = 1/500`\\n\\n2) เทรน Classifier instance แรกและคำนวนอัตราความคลาดเคลื่อน `r_1` โดยเปรียบเทียบระหว่างผลรวมของน้ำหนักของทุกๆ รายการที่ทำนายผิด กับผลรวมของน้ำหนักของทุกๆ รายการ:\\n\\n`r_j = \\\\frac{\\\\sum\\\\limits_{i = 1}^m w^{(i)}_{[\\\\hat{y}_j^{(i)} \\\\neq \\\\,y^{(i)}]}}{\\\\sum\\\\limits_{i = 1}^m w^{(i)}} \\\\tag{2}`\\n\\nเราเรียกสมการที่ (2) นี้ว่า Weighted error rate ของ Classifier j\\n\\n3) นำ Weighted error rate นี้มาคำนวนหาน้ำหนัก `\\\\alpha_j` ของ Classifier j ดังนี้:\\n`\\\\alpha_j = \\\\eta \\\\log \\\\frac{1-r_j}{r_j} \\\\tag{3}`\\n* `\\\\eta` อ่านว่า Eta คือ Learning rate hyperparameter ที่ควบคุมอัตราเร็วในการเรียนรู้ ปกติมีค่าเท่ากับ 1\\n* ยิ่ง Classifier j แม่นยำมากเท่าไหร่ แปลว่า Weighted error rate `r_j` มีค่าน้อย ส่งผลให้น้ำหนัก `\\\\alpha_j `ของ Classifier j นั้นมีค่ามาก\\n4) นำน้ำหนัก `\\\\alpha_j` ของ Classifier j ไปอับเดตน้ำหนัก `w^{(i)}` ของข้อมูลแต่ละรายการ ตามเงื่อนไขดังนี้:\\n```tex\\n\\\\text{for } i = 1, 2, \\\\dots ,m\\n\\nw^{(i)} =\\n\\\\begin{cases}\\n    w^{(i)} \\\\quad \\\\text{if } \\\\hat{y}^{(i)} = y^{(i)}\\\\\\\\\\n    w^{(i)} e^{\\\\alpha_j} \\\\quad \\\\text{if } \\\\hat{y}^{(i)} \\\\neq y^{(i)}\\n\\\\end{cases} \\\\tag{4}\\n```\\nแล้ว Normalise น้ำหนัก `w^{(i)}` ของข้อมูลแต่ละรายการ ด้วยการหารด้วยผลรวมของ `w^{(i)}` ทั้งหมด:\\n\\n`w^{(i)} := \\\\frac{w^{(i)}}{\\\\sum\\\\limits_{i = 1}^m w^{(i)}} \\\\tag{5}`\\n\\nจากสมการที่ (4) จะเห็นว่ารายการที่ทำนายผิด จะได้รับการ Boost หรือเสริมค่าน้ำหนัก เพื่อให้ Classifier instance j+1 ในรอบถัดไปเทรนซ้ำในสมการที่ (2) โดยเทรนจากค่าน้ำหนัก `w^{(i)}` ใหม่ของข้อมูลแต่ละรายการ\\n\\nAdaBoosting จะทำกระบวนการที่ (2) ถึง (4) วนซ้ำไปเรื่อยๆ โดยจะหยุดก็ต่อเมื่อ:\\n* วนซ้ำครบจำนวน Classifier instance ที่กำหนด โดยสามารถกำหนดได้ใน Argument `n_estimators` หรือ\\n* ไม่พบรายการที่ทำนายผิดพลาดเลยแม้แต่รายการเดียว\\n5) เมื่อต้องการพยากรณ์ ก็จะคำนวนค่าพยากรณ์ `\\\\hat{y}^{(i)}` ของทุกๆ Classifier instance โดยถ่วงน้ำหนักค่าพยากรณ์แต่ละ Instance ด้วยค่าน้ำหนัก `\\\\alpha_j` ของ Classifier แต่ละ Instance ดังนั้น ค่าพยากรณ์ของ Instance ที่มีความแม่นยำ จะมีค่ามาก ได้รับการพิจารณามาก\\nคำตอบสุดท้ายที่ AdaBoost พยากรณ์ คือคำตอบที่ได้รับเสียงโหวตที่ถ่วงน้ำหนักแล้วมากที่สุด\\n\\nสังเกตว่า AdaBoost ให้ค่ากับ Instance ที่ \"เก่ง\" มากกว่าที่ \"ไม่เก่ง\" ในขณะที่ Ensemble learning แบบ Random forest ให้ค่าน้ำหนักทุก Instance เท่ากัน\\n\\nข้อสังเกตที่สำคัญอีกประการก็คือ AdaBoost ไม่ได้มีเป้าหมายที่ทำให้ Classifier instance ตัวถัดๆ ไปมีความแม่นยำขึ้น แต่ทำงานโดยการเก็บคะแนนความแม่นยำของแต่ละ Instance เอาไว้เพื่อเอามาถ่วงน้ำหนักการตัดสินใจเมื่อต้องการพยากรณ์ในภายหลัง\\n\\nใน scikit-learn เราสามารถเรียกใช้ AdaBoost ได้ จาก `AdaBoostClassifier` class ในโมดูล `sklearn.ensemble`\\n### Gradient boosting\\nGradient boosting เลือกวิธีการในการ Optimise อีกวิธี โดยการพยายามให้ Classifier instance ที่มาใหม่แต่ละตัว มีความแม่นยำขึ้นเรื่อยๆ โดยเรียนรู้จากค่าความคลาดเคลื่อนสะสมที่เกิดจากการทำนายของ Instance ก่อนหน้า\\n\\nการทำงานของ Gradient boosting เข้าใจไม่ยาก สมมุติว่าเราใช้ `DecisionTreeRegressor` เราสามารถจำลอง Algorithm นี้ได้จากโค้ดดังนี้:\\n```python\\nfrom sklearn.tree import DecisionTreeRegressor\\n\\ntree_reg1 = DecisionTreeRegressor(max-depth=2)\\ntree_reg1.fit(X, y)\\n```\\nจากนั้นเราเทรน `DecisionTreeRegressor` instance ที่ 2 จากความคลาดเคลื่อนสะสม ซึ่งก็คือความต่างระหว่าง `\\\\hat{y}` กับ `y`:\\n```python\\ny2 = y - tree_reg1.predict(X)\\ntree_reg2 = DecisionTreeRegressor(max-depth=2)\\ntree_reg2.fit(X, y2)\\n```\\nและทำแบบนี้อีกครั้ง:\\n```python\\ny3 = y2 - tree_reg2.predict(X)\\ntree_reg3 = DecisionTreeRegressor(max-depth=2)\\ntree_reg3.fit(X, y3)\\n```\\nส่วนเวลาพยากรณ์ เราจะเอาคำตอบของการพยากรณ์ทุกๆ Instance มาบวกกัน ซึ่งก็คือ:\\n\\n`h(x) = h_1(x) + h_2(x) + \\\\dots + h_J(x) \\\\tag{6}`\\n\\nโดย J คือจำนวน Instance ทั้งหมด\\n\\nแต่ในความเป็นจริงเราไม่ต้องโค้ดเองแบบนี้ สามารถใช้ `GradientBoostingRegressor` หรือ `GradientBoostingClassifier` class ในโมดูล `sklearn.ensemble` ของ scikit-learn ได้เลย\\n\\nในแต่ละรอบ Classifier instance จะเทรนจาก Input `X` เดียวกัน แต่เปลี่ยน `y` ให้เป็นความต่างของค่าพยากรณ์กับคำตอบจริง ระหว่าง Instance ก่อนหน้ากับ Instance ปัจจุบัน (Residual error)\\n\\nส่วนการ Regularise นั้น Gradient boosting เป็น Algorithm ที่ค่อนข้างไม่ Overfit ง่ายจนเกินไป โดยถ้าหากเราตั้ง `learning_rate` เป็นค่าต่ำๆ จะทำให้ต้นไม้แต่ละต้นมีส่วนในการเรียนรู้น้อยลง นั่นหมายความว่าเราจะต้องใช้ต้นไม้จำนวนมากขึ้นเพื่อฟิต Train set วิธีนี้มักจะช่วยให้ Generalise ไปยังข้อมูลที่โมเดลยังไม่ได้เห็นได้ดีกว่า ดังนั้น สิ่งที่ควรรู้ก็คือ หากเราลดค่า `learning_rate` เราจะต้องเพิ่ม `n_estimators` ด้วย เพื่อลด Bias/underfit\\n\\nภาพแรกแสดง `learning_rate=0.1, n_estimators=3` แสดงให้เห็นว่าโมเดลนั้น Underfit เล็กน้อย ควรแก้ด้วยการเพิ่มจำนวนต้นไม้ `n_estimators`\\n\\nภาพสองแสดง `learning_rate=0.1, n_estimators=200` แสดงให้เห็นว่าเมื่อเพิ่มจำนวนต้นไม้แล้ว โมเดลสามารถฟิตข้อมูลได้ดียิ่งขึ้น แต่ก็ยังไม่ Overfit จนเกินไป\\n\\nโดยสรุปแล้ว Algorithm ตระกูล Gradient boosting ซึ่งรวมทั้ง XGBoost เป็นกลุ่ม Algorithm ที่ยืดหยุ่น แม่นยำ และประสิทธิภาพดีที่สุดกลุ่มหนึ่งในปัจจุบัน\\n\\nมาถึงจุดนี้ เราได้รู้จักกับ Supervised learning algorithm หลักๆ ครบแล้ว (ยกเว้น Neural networks ซึ่งจะเป็นหัวข้อใหญ่อีกกลุ่มหนึ่ง) บทต่อไปเราจะทำความรู้จักกับ Unsupervised learning algorithm เช่น K-Means clustering กัน\\n## บทที่ 12 Clustering\\nที่ผ่านมา 11 บท เราได้ทำความรู้จักกับ Supervised learning ซึ่งมีเป้าหมายในการสร้างโมเดลเพื่อทำนายคำตอบ โดยมีชุดข้อมูลที่รู้คำตอบอยู่แล้วเป็นแหล่งในการเรียนรู้ของ Algorithm ในบทที่ 12 นี้ เราจะมาเรียนรู้ Unsupervised learning ซึ่งเป็นการหาคำตอบจากข้อมูลที่ยังไม่มีใครรู้ว่าคำตอบคืออะไร โดยเราจะมุ่งเน้นที่ Clustering หรือการจัดกลุ่มข้อมูล\\n\\nสมมุติเรามีข้อมูลที่เป็นความสัมพันธ์ระหว่างสองสิ่ง เช่นในชุดข้อมูล Iris เรามี Petal length และ Petal width ของดอก Iris จำนวน 150 รายการ เราลองโหลดและดูข้อมูล 10 รายการแรกกัน:\\n```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn import datasets\\n\\n# Load the iris data\\niris = datasets.load_iris()\\nX = iris.data[:, 2:]\\ny = iris.target\\n\\n# Explore the data\\nprint(X[:10, :])\\n```\\nได้ผลออกมาดังนี้:\\n```\\n[[1.4 0.2]\\n [1.4 0.2]\\n [1.3 0.2]\\n [1.5 0.2]\\n [1.4 0.2]\\n [1.7 0.4]\\n [1.4 0.3]\\n [1.5 0.2]\\n [1.4 0.2]\\n [1.5 0.1]]\\n```\\nสามารถพล็อตข้อมูลเป็น Scatterplot ได้ดังนี้:\\n```python\\n# Plot the data\\nplt.figure(figsize=(7,6))\\nplt.scatter(X[:, 0], X[:, 1], cmap=\\'Paired_r\\')\\nplt.xlabel(iris.feature_names[2])\\nplt.ylabel(iris.feature_names[3])\\nplt.show()\\n```\\nเราจะมาจัดกลุ่มข้อมูลชุดนี้ออกเป็น 3 กลุ่ม โดยสมมุติว่าเราไม่รู้มาก่อนว่า 3 กลุ่มนี้คืออะไรบ้าง (อันที่จริงเรารู้ เพราะข้อมูลชุดนี้เป็น Classification dataset แต่เราไม่ได้นำคำตอบมาใช้ เพื่อจะได้เปรียบเทียบให้ดูภายหลังว่าการแบ่งกลุ่มของ Algorithm เหมือนหรือต่างกับคำตอบจริงแค่ไหน)\\n\\nเราจะใช้ K-Means clustering algorithm ซึ่งเรียกใช้ได้จาก Class `KMeans` ในโมดูล `sklearn.cluster`:\\n```python\\n# Apply K-Means\\nkmeans = KMeans(n_clusters=3).fit(X)\\n```\\nสังเกตว่าเรากำหนดจำนวน Cluster เป็น 3\\n\\nเราสามารถแสดงผลการจัดกลุ่มได้ด้วย Method `.label_` ของ `KMeans` instance:\\n```python\\n# Output labels\\ny_clustered = kmeans.labels_\\nprint(y_clustered)\\n```\\nผลออกมาคือ:\\n```\\n[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\\n 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1\\n 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\\n 1 1]\\n```\\nนี่คือหมายเลข Label ขอบข้อมูลทั้ง 150 รายการที่ K-Means แบ่งให้ เรามาลองพล็อต Scatterplot ที่แบ่งสีตาม Label ใหม่นี้กัน:\\n```python\\n# Plot the clustered data with centroids\\nplt.figure(figsize=(7,6))\\nscatter = plt.scatter(X[:, 0], X[:, 1], c=y_clustered, cmap=\\'tab10\\')\\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, c=\\'red\\')\\nplt.xlabel(iris.feature_names[2])\\nplt.ylabel(iris.feature_names[3])\\nplt.legend(*scatter.legend_elements())\\nplt.show()\\n```\\nจุดสีแดง คือจุดศูนย์กลางของแต่ละ Cluster ซึ่งเราจะอธิบายในส่วน Algorithm ว่าได้มาอย่างไร\\n\\nอย่างที่บอกตอนแรก คือข้อมูลชุดนี้ในความเป็นจริงเรารู้คำตอบ เพื่อความน่าสนใจจะมาลองดูว่าต้นฉบับนั้นจำแนก Label อย่างไร สิ่งที่ต้องทำก็เพียงแค่เปลี่ยน Argument `c=y_clustered` ใน `plt.scatter` เป็น `c=y` เพื่อให้ Map สีเข้ากับข้อมูล `y` ต้นฉบับ ได้ผลคือ:\\n\\nดูคล้ายกันดีมาก ต่างกันเล็กน้อยตรงรอยต่อ\\n\\nถ้าพิจารณาดู `y` ของข้อมูลชุดนี้ คือชื่อสายพันธุ์ของดอก Iris ซึ่งมี 3 Class คือ Setosa, Versicolor, Virginica ข้อมูลที่เก็บมาคือความยาวและความกว้างของกลีบดอก ซึ่งเรารู้ว่าโดยธรรมชาติแล้ว Iris ทั้ง 3 สายพันธุ์มีลักษณะความยาวความกว้างกลีบดอกไม่เหมือนกันดังภาพสุดท้าย การที่เราใช้ Machine จำแนกข้อมูลทั้งสองนี้ออกเป็น 3 Class แล้วได้ผลใกล้เคียงการจำแนกของธรรมชาติ ก็ดูเป็นสิ่งที่น่าทึ่งทีเดียว เพราะ Algorithm เป็นกลไกทางคณิตศาสตร์ เมื่อกลไกคณิตศาสตร์นี้ทำงานแล้วได้ผลเหมือนธรรมชาติ นั่นก็อาจจะแปลว่าธรรมชาติสร้างสิ่งต่างๆ ตามหลักคณิตศาสตร์ก็เป็นได้\\n\\nทีนี้เรามองลองดูว่า K-Means clustering จำแนกข้อมูลเป็นกลุ่มๆ โดยไม่รู้มาก่อนว่าแต่ละกลุ่มคืออะไรได้อย่างไร\\n### K-Means clustering algorithm\\nK-Means clustering มีขั้นตอนการทำงานดังนี้\\n\\nสมมุติว่าต้องการแบ่ง Cluster เป็น 2 Class:\\n\\n1) สุ่มเลือกข้อมูลสองจุดใน Dataset เป็นจุดศูนย์กลางเริ่มต้น เรียกว่า Centroid\\n\\n2) กำหนดให้รายการข้อมูลที่อยู่ใกล้แต่ละ Centroid เป็นสมาชิกของ Centroid นั้น โดยข้อมูลแต่ละรายการจะเป็นสมาชิกได้ Centroid เดียวเท่านั้น:\\n`c^{(i)} = \\\\text{argmin}_k ||x^{(i)}-\\\\mu_{k}||^2 \\\\tag{1}`\\n* `c^{(i)}` คือรหัส Class ที่กำหนดให้ข้อมูล `x^{(i)}`\\n* `\\\\mu_{k}` คือ Centroid k\\n3) คำนวนค่าเฉลี่ยของสมาชิกทุกรายการในแต่ละ Centroid แล้วย้าย Centroid ไปสู่ค่าเฉลี่ยนั้น ขั้นตอนนี้จะทำให้ Centroid ขยับที่เล็กน้อย\\n`\\\\mu_{k} = \\\\frac{1}{n} (x^{(k_1)}+x^{(k_2)}+ \\\\dots +x^{(k_n)}) \\\\tag{2}`\\n4) ทำซ้ำข้อ 2) และ 3) จนกระทั่งได้ Cluster 2 Class โดยมีเงื่อนไขว่าจะจบก็ต่อเมื่อได้รหัสของ Class `c^{(i)}` และ Centroid `\\\\mu_{k}` ที่ทำให้ค่าเฉลี่ยของผลรวมระยะทางระหว่างรายการข้อมูลที่เป็นสมาชิก Centroid กับ Centroid นั้นๆ มีค่าน้อยที่สุด โดยคำนวนจากข้อมูลทุกรายการและ Centroid ทุก Centroid สามารถเขียนเป็น Cost function ได้ดังนี้:\\n`\\\\text{min}_{c, \\\\mu} J(c, \\\\mu) = \\\\frac{1}{m} \\\\sum\\\\limits_{i=1}^m ||x^{(i)}-\\\\mu_{c^{(i)}}||^2 \\\\tag{3}`\\n* `\\\\mu_{c^{(i)}}` คือ Centroid ที่ข้อมูล `x^{(i)}` เป็นสมาชิก\\nจริงๆ ยังมี Clustering algorithm อื่นๆ อีก แต่ K-Means นั้นเป็นที่นิยมมากที่สุด ถ้าสนใจสามารถศึกษาเพิ่มเติมเอกได้จากเอกสารของ scikit-learn\\n\\nตอนต่อไปเราจะเริ่มเรียนรู้การตรวจหาข้อมูลที่ผิดปกติ ไม่เข้าพวก ที่เรียกว่า Anomaly detection\\n## บทที่ 13 Anomaly Detection\\nหากเรามีชุดข้อมูลที่จะเทรนเพื่อสร้างโมเดล แล้วข้อมูลบางรายการมีค่าผิดปกติไปจากพวก Algorithm จะนำความผิดปกติเหล่านั้นไปคำนวนเป็นส่วนหนึ่งของโมเดล ซึ่งหมายความว่าเราอาจจะได้โมเดลที่ไม่ได้เป็นภาพสะท้อนที่แม่นยำของข้อมูลที่น่าจะนำมาใช้พยากรณ์จริง นั่นคือเกิดปัญหา Overfitting หรือ Variance นั่นเอง\\n\\nเราเรียกปัญหานี้ว่าปัญหา Outlier หรือข้อมูล Input ที่ผิดแผกไปจวกพวก ซึ่งสามารถตรวจจับได้ด้วย Unsupervised learning algorithm ประเภท Outlier detection โดยเมื่อตรวจเจอแล้ว เราสามารถพิจารณาจัดการกับข้อมูลเหล่านั้น เช่นการลบออกไป เพื่อจะสามารถสร้างโมเดลที่เป็นตัวแทนที่ดีของข้อมูลส่วนมาก\\n\\nในอีกด้านหนึ่ง หากเรามีชุดข้อมูลที่เรารู้ว่า \"ปกติ\" อยู่ เราอาจต้องการตรวจจับข้อมูลใหม่ที่อยู่นอกกรอบของความ \"ปกติ\" โจทย์แบบนี้มีประโยชน์มากมาย เช่น การตรวจหาความผิดปกติของธุรกรรมการเงินเพื่อเฝ้าระวังการทุจริตคอร์รัปชัน หรือการตรวจการทำงานของเครื่องจักรต่างๆ ในระบบที่มีหลายเครื่อง เพื่อรู้ได้ทันทีเมื่อมีเครื่องที่ผิดปกติ เราเรียกกระบวนการแบบนี้ว่า Novelty detection ซึ่งก็คือการพยากรณ์ว่าข้อมูลใหม่นั้นเป็น Outlier เมื่อเทียบกับข้อมูลที่มีอยู่แล้วหรือไม่นั่นเอง\\n\\nในบทนี้เราจะมาเรียนรู้ Unsupervised learning algorithm สำหรับการตรวจจับความผิดปกติ โดยจะนำเสนอ Elliptic envelope, Isolation forest, และ Local Outlier Factor (LOF) สำหรับ Outlier detection ส่วน Novelty detection จะนำเสนอ One Class SVM\\n### Elliptic envelope\\nสมมุติว่าเรามีข้อมูลที่มีการกระจายตัวแบบ \"มาตรฐาน\" เช่นแบบ Gaussian distribution\\n\\nเราสามารถใช้ Elliptic envelope หาจุดศูนย์กลางของข้อมูล แล้วตรวจหาข้อมูลที่อยู่นอกการกระจายเป็นวงรีแบบ Gaussian โดยจะพิจารณารูปร่างของวงรีตาม Covariance ของการกระจายตัวใน Matrix ข้อมูล\\n\\nเราสามารถเรียกใช้ Elliptic envelope จาก Class `EllipticEnvelope` ในโมดูล `sklearn.covariance`\\n```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.covariance import EllipticEnvelope\\nfrom sklearn.ensemble import IsolationForest\\nfrom sklearn.neighbors import LocalOutlierFactor\\nfrom sklearn.svm import OneClassSVM\\n\\n# Generate 2-D random data\\ncov = np.array([[1, .6],\\n                [.6, 1]])\\nX = np.random.RandomState(0).multivariate_normal(mean=[0, 0],\\n                                                 cov=cov, size=500)\\n\\n# Call and fit the Elliptic Envelope detector\\n\\nee_detector = EllipticEnvelope(contamination=0.1).fit(X)\\nee_detect = ee_detector.predict(X)\\nee_detect\\n```\\nตอนเราเรียก `EllipticEnvelope` instance (รวมทั้ง Anomaly detection algorithm อื่นๆ) เราสามารถกำหนด Argument `contamination` เพื่อระบุว่าเราจะตรวจหา Anomaly เป็นสัดส่วนเท่าใดเมื่อเทียบกับข้อมูลทั้งหมด เช่น 0.1 คือ 10%\\n\\nผลจากการเรียก Method `.predict` จะได้เป็น Array ที่มี Label ของข้อมูลแต่ละรายการ โดย 1 คือปกติ ส่วน -1 คือไม่ปกติ:\\n```\\narray([ 1, -1, -1,  1,  1,  1,  1,  1,  1,  1, -1,  1, -1,  1,  1,  1,  1,\\n        1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1, -1,\\n        1, -1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,\\n        1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\\n        1,  1,  1, -1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1, -1,  1,  1,\\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1, -1,  1,  1,\\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1,  1, -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1, -1,  1, -1,  1,  1,  1,\\n        1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,\\n        1, -1,  1,  1,  1,  1,  1,  1,  1, -1, -1,  1,  1,  1,  1,  1,  1,\\n        1, -1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1, -1, -1, -1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,\\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,\\n       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1,  1, -1,  1, -1,  1,  1,  1,  1,  1,  1, -1,  1,  1,\\n        1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,\\n        1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\\n        1,  1,  1,  1,  1,  1,  1])\\n```\\nเราสามารถพล็อต Scatterplot โดยแยกสีข้อมูลที่ปกติและไม่ปกติ ดังนี้:\\n```python\\n# Plot the data\\nplt.figure(figsize=(9,7))\\nplt.scatter(X[:, 0], X[:, 1], c=ee_detect, cmap=\"flag\", alpha=0.5)\\nplt.title(\"Elliptic Envelope\")\\nplt.show()\\n```\\nจะเห็นว่า Elliptic envelope ได้จำแนกข้อมูล 10% ที่อยู่นอกการกระจายตัวแบบวงรีออกจากข้อมูลทั้งหมด\\n### Isolation forest\\nIsolation forest ใช้เทคนิคแบบ Random forest โดยสุ่มเลือกจุดบนข้อมูล และสร้าง Tree แยกข้อมูลเป็นสาขา โดยมีสมมุติฐานว่า ข้อมูลปกติจะสามารถแตกกิ่งสาขาเป็นจำนวนพอๆ กัน ดังนั้น หากพบ Node ที่แตกสาขาน้อยกว่าค่าเฉลี่ยอย่างมีนัยสำคัญ ก็จะถือว่าเป็นข้อมูลที่ผิดปกติ\\n```python\\n# Call and fit the Isolation Forest detector\\n\\nif_detector = IsolationForest(n_estimators=10, contamination=0.1).fit(X)\\nif_detect = if_detector.predict(X)\\n```\\nสังเกตว่า Hyperparameter ที่กำหนดได้ คือจำนวนต้นไม้ที่จะมาช่วยกันสุ่ม ซึ่งก็คือ Argument `n_estimators`\\n\\nเราสามารถเรียกใช้ Isolation forest จาก Class `IsolationForest` ในโมดูล `sklearn.ensemble`\\n### Local Outlier Factor (LOF)\\nLocal Outlier Factor หรือ LOF เปรียบเทียบความหนาแน่นของข้อมูลจุดต่างๆ แล้วแยกจุดที่มีความหนาแน่นน้อยออกเป็น Anomaly โดยความหนาแน่นจะคำนวนจาก K-Nearest neighbors ซึ่งก็คือระยะห่างระหว่างจุดที่เราสนใจ กับจุด \"เพื่อนบ้าน\" ที่ใกล้ที่สุดจำนวน K จุดตามที่เรากำหนด\\n```python\\n# Call and fit the Local Outlier Factor detector\\n\\nlof_detector = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\\nlof_detect = lof_detector.fit_predict(X)\\n```\\nอนึ่ง LOF สามารถใช้กับ Novelty detection หรือการตรวจหาข้อมูลใหม่ที่ผิดปกติเมื่อรู้ข้อมูลปัจจุบัน โดยใส่ Argument `novelty=True` แล้วใช้ `.fit(X)` เข้ากับข้อมูลปัจจุบัน ตามด้วย `.predict(X_new)` บนข้อมูลใหม่\\n\\nเราสามารถเรียกใช้ Local Outlier Factor จาก Class `LocalOutlierFactor` ในโมดูล `sklearn.neighbors`\\n### One Class SVM\\nOne Class SVM ทำงานโดยการสร้าง Support Vector ด้วย RBF Kernel เพื่อเป็นเส้นขอบเขตการตัดสินใจ ใช้สำหรับ Novelty detection เท่านั้น\\n```python\\n# Call and fit the One Class SVM detector\\nX_new = np.array([[-1, 2],\\n                 [-.1, 1]])\\n\\nsvm_detector = OneClassSVM(tol=0.1, nu=0.1, gamma=0.1).fit(X)\\nsvm_detect = svm_detector.predict(X_new)\\nsvm_detect\\n```\\nทบทวนว่า Novelty detection คือการตรวจหาว่าข้อมูลใหม่ผิดปกติหรือไม่ ดังนั้นเราจะป้อนข้อมูลใหม่ลงไปใน `.predict` ซึ่งจากตัวอย่างมี 2 รายการ คือ [-1, 2] และ [-0.1, 1] ซึ่งได้ผลคือ:\\n```\\narray([-1,  1])\\n```\\nแปลว่าข้อมูลใหม่รายการที่ 1 นั้นผิดปกติ ส่วนรายการที่ 2 นั้นปกติ\\n\\nเรามาลองพล็อตขอบเขตการตัดสินใจดู สังเกตว่าข้อมูลรายการที่ 1 คือจุดสีแดง ส่วนรายการ 2 คือจุดสีดำ:\\n```python\\n# Plot the data with decision boundaries\\ndef plot_decision_boundary(clf, X, y, cmap=\\'Paired_r\\'):\\n    h = 0.005  # Boundary lines\\' resolution\\n    x_min, x_max = X[:,0].min() - 10*h, X[:,0].max() + 10*h\\n    y_min, y_max = X[:,1].min() - 10*h, X[:,1].max() + 10*h\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\\n                         np.arange(y_min, y_max, h))\\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\\n    Z = Z.reshape(xx.shape)\\n\\n    plt.figure(figsize=(9,7))\\n    plt.contourf(xx, yy, Z, cmap=cmap, alpha=0.2)  # Background\\n    plt.contour(xx, yy, Z, colors=\\'k\\', linewidths=0.2)  # Boundary lines\\n    plt.scatter(X[:, 0], X[:, 1], c=\"gray\", cmap=\"flag\", alpha=0.4)\\n    plt.scatter(X_new[:, 0], X_new[:, 1], s=100, c=svm_detect, cmap=\"flag\", alpha=0.8)\\n    plt.xlabel(\"x1\")\\n    plt.ylabel(\"x2\")\\n    plt.title(\"One Class SVM with Decision Boundary\")\\n\\nplot_decision_boundary(svm_detector, X, svm_detect)\\n```\\nข้อด้อยอย่างหนึ่งของ SVM คือมี Hyperparameter ที่ต้องปรับแต่งค่อนข้างมาก เช่นจากตัวอย่าง มี `tol=0.1, nu=0.1, gamma=0.1` อย่างไรก็ตาม ข้อด้อยนี้ก็อาจกลายเป็นข้อดีได้ เพราะเราจะสามารถปรับจูนเส้นขอบเขตการตัดสินใจได้ละเอียดตามที่ต้องการ ความหมายของ Hyperparameter แต่ละตัวสามารถดูได้จากคู่มือของ scikit-learn\\n\\nการเรียกใช้ One Class SVM สามารถเรียกได้จาก Class `OneClassSVM` ในโมดูล `sklearn.svm`\\n\\nบทต่อไปเราจะเริ่มเรียนรู้ Deep learning กัน โดยเริ่มด้วยการทำความเข้าใจ Neural network ซึ่งเป็นหัวใจของ Deep learning\\n## บทที่ 14 Neural Network Algorithm\\nArtificial neural network เป็น Algorithm หลักของการเรียนรู้แบบ Deep learning ซึ่งเป็นแนวทางการเรียนรู้แบบหนึ่งของ Machine learning\\n\\nหลายคนคงเคยได้ยินมาว่า Deep learning ในปัจจุบันนั้นมีความสามารถมาก เช่นสามารถพยากรณ์มะเร็งจากภาพ X-ray ได้แม่นยำกว่าแพทย์ สามารถแยกแยะหน้าคนว่าใครเป็นใคร สามารถอ่านป้ายจราจรจากภาพถ่าย แยกเสียงพูดออกจากเสียงดนตรี เล่นหมากล้อมชนะแชมป์โลก ไปจนถึงการสร้างภาพดาราที่ไม่มีตัวตนจริง จะเห็นว่าความสามารถเหล่านี้ล้วนมีจุดร่วมกันอยู่หนึ่งอย่าง นั่นคือข้อมูล Input ที่ป้อนเข้า Deep learning algorithm มักจะเป็นข้อมูลแบบที่ไม่มีโครงสร้าง หรือที่เรียกว่า Unstructured data เช่น ภาพ ข้อความ เสียง ในขณะที่ Machine learning algorithm ทั่วไปมักจะเหมาะสมกับข้อมูลแบบ Structured data ที่มีโครงสร้างเป็นตาราง\\n\\nอะไรคือสิ่งที่ทำให้ Neural network สามารถเรียนรู้จากข้อมูลที่ไม่มีโครงสร้างได้ดีกว่าวิธีอื่นๆ คำตอบอยู่ที่โครงสร้างการเรียนรู้ (Learning architecture) ของ Neural network ต่างจากโครงสร้างการเรียนรู้ของ Machine learning algorithm แบบทั่วไป\\n\\nเรามาลองทำความเข้าใจโครงสร้างของการเรียนรู้ทั้งสองแบบกันว่าต่างกันอย่างไร\\n\\nใน Machine learning algorithm ทั่วไป เช่น Linear regression และ logistic regression โครงสร้างการเรียนรู้เป็นดังนี้:\\nมี 3 ส่วน คือ Input, Classifier, และ Prediction\\n\\n1) Input คือข้อมูลที่ป้อนเข้า Algorithm เช่นในกรณีนี้มี 3 Feature ได้แก่ `x_1`, `x_2`, และ `x_3`\\n\\n2) Classifier คือฟังก์ชันที่นำเอา Input x มาคำนวนร่วมกับค่าน้ำหนัก `w` ของ `x` แต่ละตัว ผลที่ได้เรียกว่า `z` แล้วป้อน `z` เข้าสู่ฟังก์ชันตัดสินใจ `\\\\sigma` เช่น Sigmoid function เพื่อคำนวนความเป็นไปได้ของคำตอบ\\n\\n3) Prediction คือการตีความคำตอบของ Classifier แล้วนำเสนอผลลัพธ์ให้ผู้ใช้ เช่นคำถาม Binary classification ก็ให้คำตอบว่า \"ใช่\" หรือ \"ไม่ใช่\"\\nในขณะที่ Neural network มีโครงสร้างการเรียนรู้ดังนี้:\\n\\n4 ส่วน คือ Input layer, Hidden layer, Output layer, และ Prediction โดยเราจะอธิบายการทำงานทีละส่วน อนึ่ง เราเรียกกระบวนการที่ทั้ง 4 ส่วนนี้ทำงานต่อเนื่องกันจากด้านซ้ายไปด้านขวาของโครงสร้าง Neural network ว่ากระบวนการแผ่กระจายเดินหน้า หรือ Forward propagation ซึ่งทำงานร่วมกับการแผ่กระจายย้อนกลับ หรือ Backward propagation ซึ่งจะอธิบายภายหลัง\\n\\nแต่ก่อนที่จะรู้จัก Forward propagation หลายคนอาจจะเคยได้ยินหรือเคยตั้งคำถามว่า Artificial neural network นี้มีความเหมือนหรือต่างกับการทำงานของสมองหรือไม่อย่างไร เราลองมาดูภาพ Neuron ในสมอง ซึ่งเป็นหน่วยที่รับ ประมวลผล และส่งข้อมูลไปยังส่วนอื่น ๆ\\n\\nหากจะเปรียบเทียบ โครงสร้างที่เห็นคือ Neuron ซึ่งรับข้อมูลมาจาก Dendrite แล้วส่งข้อมูลออกไปยัง Neuron ตัวถัดไปผ่าน Axon โดย Neuron แต่ละตัวจะรับและส่งข้อมูลกับ Neuron อื่นๆ หลายตัว ปัจจุบันเรายังไม่มีความเข้าใจดีนักว่าโครงสร้างนี้มีความหมายต่อกระบวนการเรียนรู้ของมนุษย์อย่างไร แต่ถ้าจะให้คาดเดา ก็ดูเหมือนว่าโครงสร้างที่มีการเชื่อมโยงกันเป็นเครือข่ายแบบนี้น่าจะทำให้สมองมีความสามารถในการประมวลผลข้อมูลที่รับมา กลายเป็นการตัดสินใจทั้งระดับร่างกายและจิตใจได้ Artificial neural network ได้รับแรงบันดาลใจมาจากโครงสร้างนี้ ถึงแม้กลไกการทำงานไม่จำเป็นต้องเหมือนกันก็ตาม\\n### Forward propagation\\nสมมุติว่า Algorithm มี Parameter w และ b ที่เป็นตัวแทนของข้อมูลเรียบร้อยแล้ว กระบวนการ Forward propagation คือการนำข้อมูล x เข้ามาประมวลผลร่วมกับ Parameter เหล่านั้นเป็นชั้นๆ จนได้คำตอบ\\n\\nอย่างไรก็ตาม ในตอนเริ่มต้น โมเดลจะยังไม่มี Parameter ที่ถูกต้อง เราจึงต้องสุ่มค่าเริ่มต้นของ Parameter ขึ้นมาก่อน เมื่อ Forward propagation ทำงานจบ 1 เที่ยว ก็จะเปรียบเทียบผลการพยากรณ์กับคำตอบที่รู้อยู่แล้ว จากนั้นโมเดลจะใช้กระบวนการตรงกันข้าม คือ Backward propagation ในการปรับค่า Parameter ให้สะท้อนข้อมูลใน Train set มากขึ้น ทำอย่างนี้หลายๆ รอบจนกระทั่งได้ความแม่นยำของโมเดลตามที่ต้องการ อนึ่งเราเรียกการทำงานไป-กลับแต่ละรอบว่า 1 Epoch (ภาษาไทยอาจแปล Epoch ว่า \"กัป\" ซึ่งฟังดูยิ่งใหญ่ดี แต่อาจจะไม่เหมาะกับบริบทนี้ จึงขอใช้ Epoch คำเดิม)\\n\\nForward propagation มีกระบวนการดังนี้:\\n\\n1) Input layer คือข้อมูลขาเข้า ถ้าเป็นข้อมูลแบบมีโครงสร้าง x แต่ละตัวจะแทน Feature หรือคอลัมน์ของข้อมูล เช่น อายุ เพศ รายได้ เป็นต้น\\n\\nแต่ถ้าเป็นข้อมูลแบบไม่มีโครงสร้าง เช่นรูปภาพ x แต่ละตัวจะแทนค่าความสว่างของภาพ 1 Pixel ดังนั้น ถ้าเรามีภาพขาวดำขนาด 64 คูณ 64 Pixel เราจะได้จำนวน Input m = 64 \\\\times 64 = 4096 แต่ถ้าเป็นภาพสี ต้องคูณ 3 เข้าไป เพราะภาพสีแต่ละ Pixel จะมีข้อมูลความสว่าง 3 สี คือ Red, Green, Blue\\n\\n2) Hidden layer คือชั้นประมวลผลที่ซ่อนอยู่ ซึ่งมีได้หลายชั้น (จากตัวอย่างแสดงชั้นเดียว) ใน Hidden layer แต่ละชั้นจะมีหน่วยประมวลผลที่เรียกว่า Neuron โดยในภาพมี 4 Neuron\\n\\nหน้าที่ของ Neuron แต่ละตัว คือการรับข้อมูล Input \"ทุกตัว\" จาก Layer ก่อนหน้า มาประมวลผลโดยใช้ Linear function ร่วมกับค่าน้ำหนัก w ของ Input แต่ละตัว ซึ่งให้ผล z แล้วนำ z ไปคำนวนใน Activation function g ซึ่งอาจจะเป็น Sigmoid, Tanh, หรือ RELU ก็ได้ คำตอบที่ได้เรียกว่า a สังเกตว่ากระบวนการนี้คล้ายกับขั้นตอน Classifier ของ ML algorithm ทั่วไป สิ่งที่ต่างคือชื่อเรียก สัญลักษณ์ที่ใช้แทน และจำนวน Neuron ที่มีมากกว่า 1\\n\\nเราสามารถเขียนขั้นตอนนี้เป็นสมการได้ดังนี้:\\n\\nสำหรับ `x^{(i)}`  แต่ละรายการ:\\n\\n```tex\\nz^{[1](i)} = W^{[1]}x^{(i)} + b^{[1]} \\\\tag{1}\\n\\na^{[1](i)} = g(z^{[1](i)}) \\\\tag{2}\\n```\\n\\nโดยก่อนจะไปไกลกว่านี้ มาทำความคุ้นเคยกับสัญลักษณ์แทนลำดับที่ของสิ่งต่างๆ ในโมเดลกันก่อน:\\n\\n* `[1]` ใน Brackets หมายถึงลำดับที่ของ Layer โดยนับ 1 ที่ Hidden layer แรก\\n* `(i)` ใน Parentheses หมายถึงลำดับรายการข้อมูลที่ i\\n* 1 ที่ห้อยด้านล่าง a คือลำดับที่ของ Neuron ใน Layer นั้นๆ\\n* นำสัญลักษณ์ทั้งหมดมารวมกัน ตัวอย่างเช่น `a^{[1](4)}_3` หมายถึง Activation function ตัวที่ 3 ของ Layer ที่ 1 ที่ทำงานกับข้อมูลรายการที่ 4\\nสิ่งสำคัญที่ต้องรู้ในขั้นนี้ คือ Hidden layer สามารถมีได้หลายชั้น เช่นถ้าหากมีชั้นที่ 2 สมการที่ (1) ก็จะนำ `a^{[1]}` มาแทน x และใช้ `W` และ `b` ของ Layer ที่สอง ดังนั้นเราสามารถเปลี่ยนรูปสมการที่ (1) และ (2) ให้เป็นรูปทั่วไปได้ดังนี้ (Vectorised form):\\n```tex\\nZ^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\\\tag{3}\\n\\nA^{[l]} = g(Z^{[l]}) \\\\tag{4}\\n```\\nโดย [l] คือลำดับที่ของ Layer ซึ่งนับ 1 ที่ Hidden layer ที่ 1\\n\\nคำว่า Deep ใน Deep learning ก็มาจากการที่โมเดลมี Layer หลายชั้นในการประมวลผลนั่นเอง\\n\\n3) Output layer คือชั้นที่ประมวลผล Activation a ทั้งหมดจากชั้นก่อนหน้า โดยถือว่า `a^{[1]}` คือ Input ร่วมกับค่าน้ำหนัก `W^{[2]}` ของ `a^{[1]}` ในชั้นก่อนหน้า (ไม่ใช่ของ `x` ในชั้นแรก) ได้ผลเป็น `z^{[2]}` แล้วนำ `z^{[2]}` ไปคำนวนใน Activation function เช่น Sigmoid function ได้ผลเป็น `a^{[2]}`\\n\\nเขียนเป็นสมการได้ดังนี้ (สังเกตว่าสอดคล้องกับสมการที่ (3) และ (4)):\\n```tex\\nz^{[2](i)} = W^{[2]}a^{[1](i)} + b^{[2]} \\\\tag{5}\\n\\na^{[2](i)} = \\\\sigma(z^{[2](i)}) \\\\tag{6}\\n```\\nหรือเขียนเป็นรูปทั่วไป (Vectorised) คือ:\\n```tex\\nZ^{[L]} = W^{[L]}A^{[L-1]} + b^{[L]} \\\\tag{7}\\n\\nA^{[L]} = \\\\sigma(Z^{[L]}) \\\\tag{8}\\n```\\nโดย `[L]` คือจำนวน Layer ทั้งหมด ดังนั้นจึงหมายถึง Layer สุดท้าย ส่วน `[L-1]` ก็คือ Layer รองสุดท้าย\\n\\nเพื่อความเข้าใจที่ดีขึ้น จะนำเสนอให้ดูว่าข้อมูลและตัวแปรแต่ละตัวใน Neural network มีหน้าตาอย่างไรในรูปแบบ Matrix และ Vector\\n\\nสมมุติว่าเรามีข้อมูลดังนี้: Feature n_0 = 4096 จำนวน m = 10000 รายการ:\\n```tex\\nX =\\n\\\\begin{pmatrix}\\n    x_1^{(1)} & x_1^{(2)} & x_1^{(3)} & \\\\cdots & x_1^{(10000)} \\\\\\\\\\n    x_2^{(1)} & x_2^{(2)} & x_2^{(3)} & \\\\cdots & x_2^{(10000)} \\\\\\\\\\n    x_3^{(1)} & x_3^{(2)} & x_3^{(3)} & \\\\cdots & x_3^{(10000)} \\\\\\\\\\n    \\\\vdots & \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\\n    x_{4096}^{(1)} & x_{4096}^{(2)} & x_{4096}^{(3)} & \\\\cdots & x_{4096}^{(10000)} \\\\\\\\\\n\\\\end{pmatrix}\\n\\\\tag{9}\\n```\\nมิติของ Matrix X คือ `(n_0, m) = (4096, 10000)`\\n\\nเราจะใช้ข้อมูลชุดนี้ใน Neural network ขนาด Layer L = 2 โดยมีจำนวน Neuron ใน Layer แรก `n_1 = 4` Neuron ส่วน Layer ที่สองเป็น Output layer ดังนั้นจึงมี `n_2 = 1`\\n\\nค่าน้ำหนัก `W^{[1]}` จะต้องคูณแบบ Dot product กับ X ได้ ดังนั้นจึงมีมิติ `(n_1, n_0) = (4, 4096)`:\\n```tex\\nW^{[1]} =\\n\\\\begin{pmatrix}\\n    w_1^{[1](1)} & w_1^{[1](2)} & w_1^{[1](3)} & \\\\cdots & w_1^{[1](4096)} \\\\\\\\\\n    w_2^{[1](1)} & w_2^{[1](2)} & w_2^{[1](3)} & \\\\cdots & w_2^{[1](4096)} \\\\\\\\\\n    w_3^{[1](1)} & w_3^{[1](2)} & w_3^{[1](3)} & \\\\cdots & w_3^{[1](4096)} \\\\\\\\\\n    w_4^{[1](1)} & w_4^{[1](2)} & w_4^{[1](3)} & \\\\cdots & w_4^{[1](4096)} \\\\\\\\\\n\\\\end{pmatrix}\\n\\\\tag{10}\\n```\\nเมื่อ `W^{[1]}X` อยู่ในมิติ (n_1, n_0)` \\\\times (n_0, m) = (n_1, m) = (4, 10000)` ดังนั้น `b^{[1]}` จึงจะต้องอยู่ในมิติ (n_1, 1) = (4, 1) จึงจะสามารถบวกเข้าไปสมการได้:\\n```tex\\nb^{[1]} =\\n\\\\begin{pmatrix}\\n    b_1^{[1](1)} \\\\\\\\\\n    b_2^{[1](1)} \\\\\\\\\\n    b_3^{[1](1)} \\\\\\\\\\n    b_4^{[1](1)} \\\\\\\\\\n\\\\end{pmatrix}\\n\\\\tag{11}\\n```\\nเราต้องการ `Z^{[1]} = W^{[1]}X + b^{[1]}` ดังนั้น `Z^{[1]}` จึงต้องมีมิติ (n_1, m) = (4, 10000):\\n```tex\\nZ^{[1]} =\\n\\\\begin{pmatrix}\\n    z_1^{[1](1)} & z_1^{[1](2)} & z_1^{[1](3)} & \\\\cdots & z_1^{[1](10000)} \\\\\\\\\\n    z_2^{[1](1)} & z_2^{[1](2)} & z_2^{[1](3)} & \\\\cdots & z_2^{[1](10000)} \\\\\\\\\\n    z_3^{[1](1)} & z_3^{[1](2)} & z_3^{[1](3)} & \\\\cdots & z_3^{[1](10000)} \\\\\\\\\\n    z_4^{[1](1)} & z_4^{[1](2)} & z_4^{[1](3)} & \\\\cdots & z_4^{[1](10000)} \\\\\\\\\\n\\\\end{pmatrix}\\n\\\\tag{12}\\n```\\nส่วน `A^{[1]}` ก็มีมิติ (n_1, m) เหมือน `Z^{[1]}` เพราะเป็นการนำ `Z^{[1]}` มา Apply activation function ด้วยการ Broadcast ฟังก์ชันเข้าไปใน Matrix:\\n```tex\\nA^{[1]} =\\n\\\\begin{pmatrix}\\n    g(z_1^{[1](1)}) & g(z_1^{[1](2)}) & g(z_1^{[1](3)}) & \\\\cdots & g(z_1^{[1](10000)}) \\\\\\\\\\n    g(z_2^{[1](1)}) & g(z_2^{[1](2)}) & g(z_2^{[1](3)}) & \\\\cdots & g(z_2^{[1](10000)}) \\\\\\\\\\n    g(z_3^{[1](1)}) & g(z_3^{[1](2)}) & g(z_3^{[1](3)}) & \\\\cdots & g(z_3^{[1](10000)}) \\\\\\\\\\n    g(z_4^{[1](1)}) & g(z_4^{[1](2)}) & g(z_4^{[1](3)}) & \\\\cdots & g(z_4^{[1](10000)}) \\\\\\\\\\n\\\\end{pmatrix}\\n\\\\tag{13}\\n```\\n4) Prediction นำเอาผล Activation `a^{[2]}` ของ Output layer มาตัดสินใจใน Decision function เพื่อพยากรณ์ เขียนเป็นสมการได้ดังนี้:\\n```tex\\n\\\\hat{y}^{(i)} = a^{[2](i)} \\\\tag{14}\\n\\ny^{(i)}_{predict} =\\n\\\\begin{cases}\\n    1 \\\\quad \\\\text{if } a^{[2](i)} \\\\geq 0.5 \\\\\\\\\\n    0 \\\\quad \\\\text{if } a^{[2](i)} < 0.5\\n\\\\end{cases} \\\\tag{15}\\n```\\nย้อนกลับมายังคำถามตั้งต้น ว่าทำไม Neural network จึงทำงานได้ดีกับข้อมูลแบบที่ไม่มีโครงสร้าง ลองพิจารณาดังนี้:\\n* ข้อมูลที่ไม่มีโครงสร้าง เช่นภาพ ข้อความ เสียง จะถูกนำเสนอในโมเดล ในลักษณะที่เป็น Feature จำนวนมาก โดยแต่ละ Feature แทนค่าหน่วยของข้อมูลที่บ่งบอกลักษณะของข้อมูลชิ้นนั้น เช่นความสว่างของ Pixel แต่ละจุด การนำเสนอแบบนี้ทำให้มนุษย์ไม่สามารถบอกหรือเตรียมล่วงหน้าได้ว่า Feature ไหนมีความสำคัญมากน้อย เพราะมีจำนวนมากและมีความซับซ้อนเกินกว่าที่สมองมนุษย์จะประมวลผลได้ ไม่เหมือน Feature ของข้อมูลที่มีโครงสร้างที่อยู่ในรูปของคอลัมน์ในตาราง ที่เราสามารถเตรียมเพิ่ม ลด ปรับแต่ง Feature เพื่อให้มีความหมายต่อโมเดล\\n* เมื่อเป็นเช่นนี้ หากเราใช้ Algorithm ทั่วไปในการเทรนข้อมูลที่มี Feature จำนวนมหาศาลแบบนี้ ก็มักจะได้ผลที่ไม่ดี เช่นเกิดปัญหา Variance ซึ่งก็คือการที่ Algorithm พยายามฟิตข้อมูลเข้ากับคำตอบที่รู้อยู่แล้ว แต่ Parameter เดียวกันนี้ไม่สามารถนำไปทำนายข้อมูลใหม่ที่มองไม่เห็นได้ดี เพราะจำนวนและความซับซ้อนของ Feature นั้นเกินกำลังที่ Algorithm ทั่วไปที่คำนวนชั้นเดียวจะสามารถทำความเข้าใจได้ดี\\n* ในขณะที่ Neural network จะแบ่งลำดับขั้นในการเรียนรู้ออกเป็นชั้นๆ โดยชั้นแรกจะเรียนรู้เพื่อแยกแยะ Feature อย่างหยาบๆ เช่น ถ้าเป็นภาพ ก็จะแยกบริเวณที่มีค่าความต่างของสีมาก เช่น ขอบของวัตถุ ออกมาก่อน จากนั้นจึงส่งข้อมูลที่แยกแยะเบื้องต้นแล้ว ไปยัง Layer ชั้นถัดไป ซึ่งจะได้รับข้อมูลที่ประมวลมาแล้ว นำไปแยกแยะค่าน้ำหนักของส่วนที่มีความละเอียดมากขึ้น เช่น ส่วนต่างๆ ของใบหน้า เป็นต้น\\n* การแยกแยะลักษณะของข้อมูลในแต่ละ Layer อาศัย Neuron หลายๆ ตัวทำงานพร้อมกัน โดยแต่ละตัวจะรับข้อมูลทั้งหมดจากชั้นที่แล้วเหมือนกัน แต่ให้ค่าน้ำหนัก w ไม่เท่ากัน การที่ค่าน้ำหนักของข้อมูล x แต่ละชิ้นไม่เท่ากันเลย ส่งผลเท่ากับการที่ Neuron แต่ละตัวช่วยกันประมวลข้อมูลเดียวกันโดยใช้มุมมองที่ต่างกันไปเล็กน้อย ทำให้เกิดผลเป็นการ Regularise ให้ข้อมูลลดโอกาสที่จะ Overfit train set ไปโดยปริยาย\\n\\nทบทวนอีกครั้งว่ากระบวนการทั้งหมดนี้ เรียกว่า Forward propagation ซึ่งจบลงด้วยการได้ค่าพยากรณ์ แต่แน่นอนว่าเมื่อเรายังไม่มี Parameter w และ b ที่ถูกต้อง ค่าที่พยากรณ์ได้ก็จะไม่ตรงกับความจริง ดังนั้นเราจะใช้กระบวนการ Backward propagation ในการปรับแต่ง Parameter ให้เป็นตัวแทนของข้อมูลได้เที่ยงตรงยิ่งขึ้น\\n### Backward propagation\\nบางคนอาจจะเคยได้ยินว่า Backward propagation คือส่วนที่ซับซ้อนและยากที่สุดของ Neural network algorithm ซึ่งอาจจะจริงในส่วนของการคำนวน แต่โดยหลักคิดแล้วไม่ได้ยากขนาดนั้น เรามาลองทำความเข้าใจกันดู\\n\\nทบทวนว่า กระบวนการ Forward propagation จบลงที่เราได้ค่าพยากรณ์ ดังนั้นสิ่งต่อไปที่เราต้องทำ คือการนำค่าพยากรณ์นั้นมาใส่ใน Cost function เพื่อหาความต่างระหว่างค่าพยากรณ์กับค่าจริง โดยสำหรับ Neural network เราจะใช้ Cost function ลักษณะเดียวกันกับ Logistic regression คือ:\\n\\nสมมุติว่า L = 2:\\n```tex\\nJ = -\\\\frac{1}{m} \\\\sum\\\\limits_{i=0}^{m} \\\\left( y^{(i)} \\\\log(a^{[2](i)}) + (1-y^{(i)}) \\\\log(1-a^{[2](i)}) \\\\right) \\\\tag{16}\\n```\\nอนึ่ง หากใครสนใจ เราสามารถเขียนโค้ดของ Cost function โดยใช้ numpy ช่วย Vectorise เพื่อให้สามารถคำนวน Cost function ของรายการข้อมูลทั้งหมดได้อย่างรวดเร็วโดยไม่ต้องใช้ For loop โดยเขียนได้ดังนี้:\\n```python\\nlogprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2),1-Y)\\ncost = (-1/m)*np.sum(logprobs)\\n```\\nสิ่งต่อไปที่เราจะทำ คือเราจะหาว่า Parameter ที่ทำให้ Cost function มีค่าต่ำที่สุด โดยใช้กระบวนการ Gradient descent ซึ่งมีหลักการคือ:\\n\\n1) หาอนุพันธ์ของ Parameter เช่น w เมื่อเปรียบเทียบกับ Cost function J:\\n```tex\\n\\\\frac{\\\\partial J(w)}{\\\\partial w}\\\\tag{17}\\n```\\n2) นำอนุพันธ์ที่ได้ไปลบออกจาก Parameter นั้น โดยควบคุมความเร็วในการลบด้วย Learning rate `\\\\alpha` แล้วนำ Parameter ใหม่ไปคำนวนใน Forward propagation จะได้ Cost function ที่มีค่าลดลง ทำซ้ำขั้นตอนนี้ไปเรื่อยๆ จน Cost function มีค่าต่ำที่สุดที่จะเป็นไปได้:\\n\\nทำซ้ำจนกระทั่งผลลัพธ์ล่าสุดไม่เปลี่ยนแปลงจากผลลัพธ์ครั้งก่อน:\\n```tex\\nw := w - \\\\alpha \\\\frac{\\\\partial}{\\\\partial{w}} J(w)\\\\tag{18}\\n```\\nที่อธิบายไป เป็นภาพรวมของกระบวนการ แต่ใน Neural network เรามี Parameter หลายตัวและหลายชั้น ดังนั้นเราจึงต้องใช้หลายสมการในการหาอนุพันธ์ของ Parameter แต่ละตัว\\n\\nโดย Parameter ที่ต้องหาอนุพันธ์ คือ Parameter ที่ส่งผลต่อค่าพยากรณ์ ได้แก่:\\n* `z` ของแต่ละ Layer ไม่ได้เป็นตัวแปรที่ต้องการ Optimise แต่จำเป็นต้องหาอนุพันธ์เพื่อจะได้ใช้ Chain rule หาอนุพันธ์ของ `w` และ `b` ได้\\n* `w` ของแต่ละ Layer เพราะเป็น Coefficient ของ `x`\\n* `b` ของแต่ละ Layer เพราะเป็น Intercept ของ Linear function `z = wx + b`\\n\\nจะไม่แสดงวิธีการหาอนุพันธ์ของ Parameter แต่ละตัว เพราะยุ่งยากซับซ้อนค่อนข้างมาก แต่จะแสดงให้เห็นเลยว่าอนุพันธ์ของแต่ละ Parameter คืออะไร:\\nสมมุติว่า L = 2:\\n```tex\\n\\\\color{blue}{dz^{[2]}} = a^{[2]}-y \\\\tag{19}\\n\\ndW^{[2]} = \\\\color{blue}{dz^{[2]}} a^{[1]T}\\\\tag{20}\\n\\ndb^{[2]} = \\\\color{blue}{dz^{[2]}} \\\\tag{21}\\n\\n\\\\color{red}{dz^{[1]}} = W^{[2]T} \\\\color{blue}{dz^{[2]}} \\\\tag{22}\\n\\ndW^{[1]} = \\\\color{red}{dz^{[1]}} x^T \\\\tag{23}\\n\\ndb^{[1]} = \\\\color{red}{dz^{[1]}} \\\\tag{24}\\n```\\nทั้งนี้จะไม่แสดงวิธีคิดแบบ Vectorised เพราะอยากเน้นให้เข้าใจหลักการมากกว่า ส่วนในทางปฏิบัติให้ใช้ Framework อย่าง Tensorflow จะสะดวกกว่ามาก\\n\\nสังเกตว่าเราหาอนุพันธ์ของ z เช่น `\\\\color{blue}{dz^{[2]}}` และ `\\\\color{red}{dz^{[1]}}` เพื่อเป็นอนุพันธ์ตั้งต้นให้หาอนุพันธ์ของ `w` และ `b` ได้ตามที่เขียนไว้ข้างต้น\\n\\nเมื่อเราได้อนุพันธ์ทั้ง 6 ตัว (สำหรับโมเดลความลึก 2 ชั้น ถ้า 3 ชั้นก็ต้องเพิ่มอีก 3 ตัว) เราก็จะเอาอนุพันธ์ของ `w` และ `b` ไปอัปเดตค่าตัวแปรทั้งสอง โดยทำดังนี้:\\n```tex\\nW^{[1]} := W^{[1]} - \\\\alpha (dW^{[1]}) \\\\tag{25}\\n\\nb^{[1]} := b^{[1]} - \\\\alpha (db^{[1]}) \\\\tag{26}\\n\\nW^{[2]} := W^{[2]} - \\\\alpha (dW^{[2]}) \\\\tag{27}\\n\\nb^{[2]} := b^{[2]} - \\\\alpha (db^{[2]}) \\\\tag{28}\\n```\\nแล้วนำตัวแปรที่อัปเดตแล้วไปคำนวน Forward propagation ใน Epoch ใหม่ แล้วคิดอนุพันธ์ นำอนุพันธ์มาอัปเดตตัวแปร ทำอย่างนี้ซ้ำไปเรื่อยๆ จนถึงจุดที่ Cost function มีค่าต่ำที่สุด ก็จะได้โมเดลที่ฟิตกับ Train set ที่ดีที่สุดเท่าที่จะเป็นไปได้\\n\\nมาถึงจุดนี้ ก็คงพอเข้าใจแล้วว่า Neural network ทำงานอย่างไร และเรียนรู้อย่างไร เรื่อง Neural network นี้มีรายละเอียดและวิธีการประยุกต์ใช้มากมาย ซึ่งจะค่อยๆ อธิบายในบทอื่นๆ\\n\\nส่วนในบทต่อไป เราจะเริ่มทดลองสร้างโมเดล Neural network และพยากรณ์ โดยใช้ Framework อย่าง Tensorflow และ Keras\\n## บทที่ 15 Neural Network Programming\\nบทนี้เราจะมาลองสร้างโมเดล Neural network อย่างง่ายๆ ด้วยการใช้ Deep learning framework ที่ชื่อ TensorFlow\\n\\nเป้าหมายของเราคือการสร้างโมเดลพยากรณ์ว่าลายมือเขียนตัวเขียน 0-9 จากฐานข้อมูล MNIST นั้น เขียนว่าตัวเลขอะไร\\n\\nปัญหานี้เป็นปัญหาที่เหมาะสมกับ Deep learning เพราะข้อมูล Input เป็นภาพ ซึ่งเป็นข้อมูลที่ไม่มีโครงสร้าง (Unstructured data) และมีจำนวน Feature เยอะมาก โดยในกรณีชุดข้อมูล MNIST นี้ แต่ละภาพมีขนาด 28 x 28 Pixel นั่นคือมี 784 Feature นั่นคือ `x_0` จนถึง `x_{783}`\\n\\nเราจะเริ่มด้วยการ Import library ที่ต้องใช้ ซึ่งนอกจาก numpy และ matplotlib ตามปกติ เราจะใช้ tensorflow.keras เพื่อโหลดชุดข้อมูลจาก `.datasets`, โครงสร้างโมเดลจาก `.models`, และโครงสร้างของ Layer ต่างๆ จาก `.layers`\\n\\nKeras คือ High-level interface ของ TensorFlow ซึ่งเป็น Low-level framework เปรียบเทียบเหมือนกับ TensorFlow เป็นวงจรสวิทช์ไฟ เราอาจควบคุมว่าจะเปิดปิดไฟดวงไหนด้วยการเชื่อมสายไฟในสวิทช์ไฟเข้าด้วยกัน แต่จะใช้เวลาและไม่สะดวก Keras เปรียบเสมือนแผงสวิทช์ไฟที่มีสวิทช์และหน้ากากเรียบร้อย ทำให้เราควบคุมการเปิดปิดไฟได้ง่ายและสะดวกกว่ามาก\\n\\n```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom tensorflow.keras.datasets import mnist\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\\n```\\nClass ต่างๆ ที่เราโหลดมา จะอธิบายภายหลังในส่วนการสร้างโมเดล\\n\\nจากนั้นเราก็โหลดชุดข้อมูล MNIST โดย TensorFlow ได้ฝัง MNIST ไว้ให้แล้ว เพราะเป็นชุดข้อมูลยอดนิยมที่เอาไว้ฝึกและทดสอบโมเดล การโหลดทำได้โดยเรียก:\\n```python\\n# Load the MNIST data\\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\\n\\n# Check data dimension\\nprint(\"X_train data shape is\", X_train.shape)\\nprint(\"y_train data shape is\", y_train.shape)\\nprint(\"X_test data shape is\", X_test.shape)\\nprint(\"y_test data shape is\", y_test.shape)\\n\\n# Check data type\\nprint(\"X_train data type is\", X_train.dtype)\\nprint(\"y_train data type is\", y_train.dtype)\\nprint(\"X_test data type is\", X_test.dtype)\\nprint(\"y_test data type is\", y_test.dtype)\\n```\\nเราควรเช็กมิติข้อมูลทุกครั้งที่โหลดข้อมูลมา พบว่า Train set มี 60,000 รายการ ขนาด 28 x 28 Pixel ส่วน Test set มี 10,000 รายการ ส่วน Data Type เป็น `uint8`\\n```\\nX_train data shape is (60000, 28, 28)\\ny_train data shape is (60000,)\\nX_test data shape is (10000, 28, 28)\\ny_test data shape is (10000,)\\nX_train data type is uint8\\ny_train data type is uint8\\nX_test data type is uint8\\ny_test data type is uint8\\n```\\nจากนั้นมาลองดูตัวอย่างข้อมูลกัน เนื่องจากเรารู้ว่าข้อมูลเป็นภาพ เราสามารถใช้ Method `.imshow` ของ matplotlib แสดงภาพได้ทันที:\\n```python\\n# Visualise sample data\\nimg_index = 999\\nprint(y_train[img_index])\\nplt.imshow(X_train[img_index])\\n```\\nได้ภาพขนาด 28 x 28 สังเกตว่ามีสี เพราะเป็นค่าตั้งต้นของ matplotlib เอง ในความเป็นจริงข้อมูลไม่ได้มีสี\\n\\nต่อมา เราจะ Preprocess ข้อมูลให้พร้อมใช้งานขึ้น สิ่งที่จะทำมีดังนี้:\\n\\n* เปลี่ยนมิติข้อมูลให้เป็น 4 มิติ คือ (จำนวนรายการ, Column, Row, จำนวน Layer สี) โดยข้อมูลของเราเป็นภาพขาวดำขนาด 28 x 28 ดังนั้นมิติคือ (จำนวนรายการ, 28, 28, 1)\\n* กำหนดตัวแปรมิติข้อมูล เพื่อเตรียมใช้ใน Argument `input_shape` ใน Layer แรกของ Model โดยไม่ต้องระบุจำนวนรายการ ดังนั้น มิติข้อมูลสำหรับ Layer แรก คือ (28, 28, 1) อนึ่งเราต้องกำหนดมิติข้อมูลสำหรับ Layer แรกเท่านั้น ส่วน Layer อื่น TensorFlow จะอนุมานให้เอง\\n* แปลง Data type ของข้อมูลให้เป็น Float เพื่อให้ Algorithm สามารถคำนวนและแสดงผลเป็นค่าทศนิยมได้\\n* เลือกข้อมูลแค่บางส่วนมาเทรนและทดสอบ เราจะไม่ใช้ทั้งหมดเพราะจะใช้เวลาเทรนนาน จึงเลือก Train set 2,000 รายการ และ Test set 400 รายการ\\n* Scale ข้อมูลโดยใช้วิธี Normalise ซึ่งก็คือการหารข้อมูลทุกรายการด้วย Range ของค่าความสว่างของแต่ละ Pixel นั่นก็คือ 255 นั่นเอง\\n\\n```python\\n# Reshaping the array to 3-d for Keras\\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\\n\\n# Specify input shape for the mode\\'s 1st layer\\ninput_shape = (28, 28, 1)\\n\\n# Convert data type to float so we can get decimal points after division\\n# Also scope down X_train and X_test to 2000:400 samples\\nX_train = X_train[:2000, :, :, :].astype(\\'float32\\')\\nX_test = X_test[:400, :, :, :].astype(\\'float32\\')\\n\\n# Also scope down y_train and y_test to 2000:400 samples\\ny_train = y_train[:2000]\\ny_test = y_test[:400]\\n\\n# Normalising all data by dividing by max brightness value.\\nX_train /= 255\\nX_test /= 255\\nprint(\\'X_train shape:\\', X_train.shape)\\nprint(\\'X_test shape:\\', X_test.shape)\\nprint(\\'y_train shape:\\', y_train.shape)\\nprint(\\'y_test shape:\\', y_test.shape)\\n```\\nเช็คมิติข้อมูลอีกทีก็จะได้จำนวนรายการและมิติตามที่ต้องการ\\n```\\nX_train shape: (2000, 28, 28, 1)\\nX_test shape: (400, 28, 28, 1)\\ny_train shape: (2000,)\\ny_test shape: (400,)\\n```\\nตอนนี้เราก็พร้อมจะสร้างโมเดลแล้ว ในขั้นนี้ เราต้องออกแบบว่าโมเดลของเราจะมีโครงสร้างอย่างไร ดังนั้น สิ่งที่ต้องตัดสินใจก็คือ:\\n\\n* จำนวน Layer โดยนับ Hidden layer บวก Output layer\\n* คุณสมบัติของแต่ละ Layer เช่น Hidden layer แรก เป็น Dense layer (ทุก Neuron เชื่อมข้อมูลจากทุก Input จาก Layer ก่อนหน้า), จำนวน 100 Neuron, ใช้ RELU เป็น Activation function เป็นต้น เราจะอธิบายเรื่อง Activation function ในบทต่อๆ ไป\\n* Layer พิเศษ เช่น Dropout เป็นวิธีการ Regularise วิธีหนึ่ง ซึ่งจะอธิบายในบทต่อๆ ไป\\n* ตัวเลือกของ Optimisation algorithm ได้แก่ Cost function ที่ใช้ (Argument `loss`), กลไกการ Optimise ที่ใช้ (Argument `optimizer`), และวิธีการชี้วัดความแม่นยำ (Argument `metrics`) โดยตัวเลือกต่างๆ จะอธิบายภายหลังเช่นกัน\\n\\nเมื่อตัดสินใจได้แล้วว่าโมเดลจะมีโครงสร้างและคุณสมบัติอย่างไร ขั้นตอนในการสร้างโมเดลก็คือ:\\n\\n* Instantiate โครงสร้างโมเดล โดยเรียก Class `Sequential` ให้อยู่ใน Instance object โดยเราจะใช้ชื่อ Instance นี้ว่า `model`\\n* เรียก Method `.add` เพื่อสร้าง Layer ทีละชั้น จากซ้ายไปขวา โดยกำหนด Argument ที่สอดคล้องกับโครงสร้าง Layer ที่เราออกแบบไว้\\n* Compile โมเดลตามโครงสร้างที่เรากำหนดไว้ โดยเรียก Method `.compile`\\n```python\\n# Build and compile model\\nmodel = Sequential()\\nmodel.add(Flatten())\\nmodel.add(Dense(100, activation=\\'relu\\', input_shape=input_shape))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(10, activation=\\'softmax\\'))\\n\\nmodel.compile(optimizer=\\'adam\\',\\n              loss=\\'sparse_categorical_crossentropy\\',\\n              metrics=[\\'accuracy\\'])\\n```\\nสังเกต Layer สุดท้าย คือ `Dense(10, activation=\\'softmax\\')` ใช้ Softmax activation function ซึ่งทำให้โมเดลสามารถ Output แบบ Multiclass ได้ โดยเราต้องกำหนดจำนวน Neuron เท่ากับจำนวน Class ที่เป็นไปได้ ซึ่งก็คือ 10 นั่นเอง\\n\\nจากนั้นก็เริ่มเทรนโดยเรียก Method `.fit` พร้อมกำหนด Argument ต่างๆ ซึ่งหลักๆ ก็มี:\\n\\n* `epochs` จำนวนรอบของการเทรน โดยแต่ละ Epoch จะทำให้ Loss ลดลง ในขณะที่ Accuracy เพิ่ม\\n* `batch_size` ขนาดของ Batch ซึ่งก็คือจำนวนรายการข้อมูลที่จะให้ Optimiser คำนวนในหนึ่งครั้ง เช่น ข้อมูลมี 2,000 รายการ ถ้ากำหนด Batch size เป็น 32 แปลว่า Optimiser จะต้องทำงาน 62.5 ครั้ง จึงจะครบทั้ง 2,000 รายการ และถ้ากำหนด Epoch เป็น 20 ก็หมายถึงการทำงาน 62.5 ครั้ง 20 รอบ การกำหนด Batch size จะมีผลโดยตรงกับความเร็วในการคำนวน ยิ่ง Batch ใหญ่ ยิ่งคำนวนเร็ว แต่ Batch ที่ใหญ่เกินไปอาจทำให้ข้อมูลที่คำนวนมีขนาดใหญ่เกินกว่า Memory ของเครื่องเราจะรองรับได้ ทั้งนี้ใน การกำหนด Batch size แบบต่างๆ จะมีชื่อเรียกเฉพาะ เช่น Batch gradient descent แปลว่า Batch size เท่ากับจำนวนรายการข้อมูล ไม่มีการแบ่ง, Mini-batch gradient descent คือการแบ่งข้อมูลออกเป็น Batch, ส่วน Stochastic gradient descent คือ Batch size เท่ากับ 1 เราจะอธิบายเรื่องนี้ละเอียดในภายหลัง\\n* `validation split` เราสามารถกัน Train set ส่วนหนึ่งไว้เป็น Validation set ซึ่งก็คือข้อมูลชุดที่โมเดลไม่เคยเห็น Keras จะคำนวน Loss และความแม่นยำกับ Validation set ทุกครั้งที่คำนวนจบ 1 Epoch และแสดงผลให้เราเห็น อนึ่งการแบ่ง Validation set จะตัดเอาส่วนสุดท้ายของ Train set มาเลยโดยไม่สุ่ม ดังนั้นเราจึงควรสุ่ม Shuffle ข้อมูลให้เรียบร้อยตั้งแต่แรก รายละเอียดเกี่ยวกับการแบ่งชุดข้อมูลออกเป็น Train / Validation / Test set จะอธิบายละเอียดในบทต่อๆ ไป\\n```python\\n# Fit the model to data\\nmodel.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\\n```\\n\\nการเทรนโมเดล Neural network อาจใช้เวลานานมาก ขึ้นอยู่กับจำนวนรายการและ Feature ข้อมูล รวมทั้งโครงสร้าง Layer เช่นจำนวน Layer และจำนวน Neuron โดย TensorFlow จะช่วยเราด้วยการแสดงความก้าวหน้าของการเทรนแต่ละ Epoch เป็น Progress bar และเวลาที่ใช้ เพื่อให้เราสามารถประเมินได้ว่าจะต้องรออีกนานเท่าใด\\n\\nผลที่ได้ออกมาดังนี้:\\n```\\n    Train on 1600 samples, validate on 400 samples\\n    Epoch 1/20\\n    [===] - 1s 491us/sample - loss: 1.4701 - accuracy: 0.5881 - val_loss: 0.7175 - val_accuracy: 0.8275\\n    Epoch 2/20\\n    [===] - 0s 158us/sample - loss: 0.6571 - accuracy: 0.8213 - val_loss: 0.4700 - val_accuracy: 0.8600\\n    Epoch 3/20\\n    [===] - 0s 146us/sample - loss: 0.4681 - accuracy: 0.8744 - val_loss: 0.3726 - val_accuracy: 0.8925\\n    .           .           .           .           .           .           .\\n    .           .           .           .           .           .           .\\n    .           .           .           .           .           .           .\\n    Epoch 18/20\\n    [===] - 0s 233us/sample - loss: 0.0807 - accuracy: 0.9844 - val_loss: 0.2492 - val_accuracy: 0.9375\\n    Epoch 19/20\\n    [===] - 0s 174us/sample - loss: 0.0675 - accuracy: 0.9906 - val_loss: 0.2474 - val_accuracy: 0.9275\\n    Epoch 20/20\\n    [===] - 0s 174us/sample - loss: 0.0638 - accuracy: 0.9881 - val_loss: 0.2490 - val_accuracy: 0.9300\\n```\\n\\nสังเกตว่าแต่ละ Epoch ที่เทรน Loss และ Validation loss จะลดลงเรื่อยๆ สอดคล้องกับ Accuracy และ Validation accuracy ที่เพิ่มขึ้นเรื่อยๆ ซึ่งในกรณีของเราไปหยุดที่ Validation accuracy 93% เราสามารถใช้ผลนี้เป็นตัวบ่งชี้ว่าควรจะปรับ Hyperparameter ต่างๆ ของโมเดลหรือไม่อย่างไร เพื่อทำให้โมเดลมีความแม่นยำมากขึ้น รวมทั้งสามารถวิเคราะห์ปัญหา Bias/variance ด้วยการเปรียบเทียบข้อมูลจากการเทรนกับข้อมูลจาก Validation set\\n\\nเราสามารถ Visualise ความก้าวหน้าของการฝึกโมเดลได้ โดยการใช้ matplotlib พล็อตค่าจาก `model.history.history` ซึ่งเก็บตัวเลขการเทรนไว้ใน Dictionary key:\\n```python\\n# Review model\\'s progress\\nplt.figure(figsize=(9,6))\\nplt.plot(model.history.history[\\'accuracy\\'], label=\\'Train accuracy\\')\\nplt.plot(model.history.history[\\'val_accuracy\\'], label=\\'Validation accuracy\\')\\nplt.ylabel(\\'Value\\')\\nplt.xlabel(\\'No. epoch\\')\\nplt.legend()\\nplt.show()\\n```\\nจะเห็นว่าโมเดลของเราสามารถเพิ่ม Train accuracy ได้อย่างต่อเนื่อง อย่างไรก็ตาม ดูเหมือน Validation accuracy จะไม่ยอมเพิ่มจากจุดสูงสุด คือประมาณ 0.93 ตั้งแต่ Epoch ที่ 8 จากข้อมูลนี้เราสามารถวิเคราะห์ได้ว่าโมเดลมีปัญหา Variance ซึ่งก็คือการที่ไม่สามารถ Generalise ให้พยากรณ์ข้อมูลที่ไม่เคยเห็นได้ดีเท่าที่ควร ดังนั้นข้อมูลนี้จึงบอกเราว่าเราควรลองมาตรการต่างๆ ที่จะช่วยลดปัญหา Variance เช่น การเพิ่มขนาด Validation set การเพิ่มหรือปรับเทคนิคการ Regularisation เป็นต้น\\n\\nต่อมา เราก็นำทดสอบความแม่นยำของโมเดล กับ Test set ซึ่งแยกไว้ตั้งแต่แรกอีกที โดยการใช้ Method `.evaluate`:\\n```python\\n# Evaluate model\\'s accuracy\\nscore = model.evaluate(X_test, y_test, verbose=0)\\nprint(\"Loss is\", score[0])\\nprint(\"Accuracy score is\", score[1])\\n```\\nได้ผลใกล้เคียงกับ Validation accuracy คือ 92.25%\\n```\\nLoss is 0.3142095559835434\\nAccuracy score is 0.9225\\n```\\nตรงจุดนี้อาจมีคำถาม ว่าทำไมเราต้องทดสอบกับ Test set อีก ทั้งๆ ที่ได้ทดสอบกับ Validation set แล้ว คำตอบคือ Validation set มีหน้าที่เป็นชุดทดสอบให้เราปรับแต่งค่าต่างๆ ของโมเดล ซึ่งทำให้เกิดความเสี่ยงที่เราจะปรับค่าเพื่อ \"เอาใจ\" Validation set ส่งผลให้เรา Overfit validation set ทำให้เราไม่รู้ว่าโมเดลของเราจะมีความสามารถในการพยากรณ์ข้อมูลจริงที่ไม่เคยมองเห็นเพียงใด ดังนั้นเราจึงต้องทดสอบโมเดลที่ปรับแต่งเสร็จแล้ว กับ Test set อีกที เพื่อให้ได้ผลที่เที่ยงธรรม ไม่หลอกตัวเอง\\n\\nสุดท้าย เรามาลองพยากรณ์กันเลย โดยป้อนภาพที่มี Index ที่ต้องการจาก `X_test` ลงไปใน Method `.predict` โดยอย่าลืมแปลงมิติข้อมูลให้สอดคล้องกับตอนแรก นั่นก็คือ (จำนวนรายการ, 28, 28, 1) ซึ่งก็คือ (1, 28, 28, 1) นั่นเอง\\n\\nMethod `.predict` จะ Output ออกมาเป็น Array ของความเป็นไปได้ การตีความก็คือเราจะเลือก Index ที่มีความเป็นไปได้สูงสุดเป็นคำตอบ เราสามารถใช้ Method `.argmax` ในการ Output index ที่มีค่าสูงสุดได้:\\n```python\\n# Make a prediction\\nimage_index = 12\\nplt.imshow(X_test[image_index].reshape(28, 28),cmap=\\'Greys\\')\\npred = model.predict(X_test[image_index].reshape(1, 28, 28, 1))\\nprint(\"Prediction probability array is:\")\\n\\ncount = 0\\n\\nfor i in pred.squeeze():\\n    print(count, \":\", i)\\n    count += 1\\n\\nprint(\"From which the max choice is:\", pred.argmax())\\n```\\n```\\nPrediction probability array is:\\n0 : 6.983635e-06\\n1 : 1.8324994e-07\\n2 : 7.03581e-05\\n3 : 3.01667e-05\\n4 : 0.005716639\\n5 : 0.00045127788\\n6 : 9.964272e-07\\n7 : 0.0036463942\\n8 : 3.6717152e-05\\n9 : 0.99004024\\n\\nFrom which the max choice is: 9\\n```\\nจะเห็นว่าภาพที่ 12 ที่เราเลือก เป็นภาพของเลข 9 โมเดลได้พยากรณ์ว่า 9 โดยให้ความเป็นไปได้ 99%\\n\\nการเปรียบเทียบภาพจริงกับค่าที่พยากรณ์ได้ จะช่วยเป็นข้อมูลให้เราวิเคราะห์ข้อผิดพลาดของโมเดลได้ว่าเป็นอย่างไรและน่าจะเกิดจากอะไร โดยจะอธิบายเรื่อง Error analysis เพิ่มเติมในบทต่อๆ ไป\\n\\nบทต่อไป เราจะมาเจาะลึกเกี่ยวกับโครงสร้างของโมเดลและตัวเลือกของ Optimisation algorithm\\n### บทที่ 16 Neural Network Vanishing Gradients\\nหลังจากที่เราสามารถสร้างโมเดล Deep learning แบบพื้นฐานแล้ว เราจะมาสำรวจปัญหาต่างๆ ในการเทรน Neural network และแนวทางแก้ไข โดยในบทนี้จะพูดถึงปัญหา Vanishing gradients\\n\\nในการเทรน Deep neural network เราอาจพบว่าโมเดลเทรนจบจนได้ Loss ที่ต่ำที่สุด แต่ก็ไม่ต่ำเท่าที่ควร นั่นคือเกิดปัญหา Bias ทั้งๆ ที่เราได้วางโครงสร้าง Layers, จำนวน Neuron, และจำนวนรายการเนื้อหาเหมาะสมเพียงพอแล้ว\\n\\nบ่อยครั้งที่ Bias นี้ เกิดจากปัญหาที่เรียกว่า Vanishing gradients นั่นคือการที่เมื่อเราเทรนโมเดลโดยการอัปเดต Parameter ซ้ำๆ ในกระบวนการ Gradient descent อนุพันธ์ `\\\\delta w` บางตัวจะมีค่าน้อยลงจนเหลือน้อยมากๆ โดยเฉพาะในตัวที่อยู่ใน Layer ชั้นแรกๆ ที่ใกล้กับ Input layer ส่งผลให้การอัปเดต Parameter ครั้งถัดไป Loss จะเปลี่ยนแปลงน้อยลงมากจนไม่เกิดผลในการ Optimise หรือต้องใช้เวลา (จำนวนรอบ) สูงมากๆ ในการเทรน อนึ่ง ในทางกลับกัน หากอนุพันธ์เพิ่มขึ้นสูงเกินไป ก็จะเกิดปัญหาที่เรียกว่า Exploding gradients ซึ่งก็ส่งผลให้ Optimise ไม่สามารถ Converge หาคำตอบที่ดีได้เช่นกัน\\n\\nปัญหา Vanishing gradients นี้มีแนวทางในการแก้ 3 แนวทางด้วยกัน ได้แก่:\\n\\n* การเปลี่ยนวิธีการตั้งค่าตั้งต้นของ Parameter\\n* การเปลี่ยน Activation function\\n* การใช้เทคนิค Batch normalisation\\n\\nซึ่งเราจะอธิบายทีละแนวทาง\\n### Xavier and He initialisation\\nถ้าเราตั้งค่าตั้งต้นให้กับน้ำหนัก w เป็น 0 เราจะแทบไม่สามารถเทรนอะไรได้เลย เพราะทุก Neuron ใน Layer แรกจะส่งค่า Activation เดียวกันออกไปยัง Layer ถัดไป ดังนั้นเราจึงต้องตั้งค่าตั้งต้นน้ำหนัก w ให้ไม่ใช่ 0 และไม่ใช่ตัวเลขที่เท่ากัน\\n\\nแต่ถ้าเราตั้งค่าตั้งต้นของ w เป็นตัวเลขสุ่มทั่วไป จะพบว่าบางตัวเลขอาจทำให้เกิดปัญหา Vanishing gradients ดังนั้น เราจึงต้องมีวิธีที่ดีในการสุ่มค่าตั้งต้นของ w\\n\\nวิธีดังกล่าว เรียกว่า Xavier initialisation มีหลักการคือจะต้องสุ่มค่าที่ทำให้ Variance ของ Output เท่ากับ Variance ของ Input ใน Layer เดียวกัน และต้องทำให้อนุพันธ์ `\\\\delta w` ในแต่ละ Layer มี Variance เท่ากันทั้งขาเข้าและขาออกเมื่อทำ Backward propagation ด้วย อย่างไรก็ตามข้อกำหนดนี้เป็นไปไม่ได้ที่จะทำจริง เพราะจะทำได้ก็ต่อเมื่อแต่ละ Layer มีจำนวนการเชื่อมต่อขาเข้าเท่ากับขาออก ดังนั้นวิธีที่ดีที่สุดในการจำลองผลลัพธ์นี้ให้เกิดขึ้นได้ คือการใช้สมการต่อไปนี้:\\n\\n```tex\\nr = \\\\sqrt{\\\\frac{6}{n_i + n_{i+1}}} \\\\tag{1}\\n```\\n\\nโดย n_i คือจำนวนการเชื่อมต่อขาเข้าของ Layer นั้น ส่วน `n_{i+1}` คือจำนวนการเชื่อมต่อที่ออกจาก Layer นั้น\\n\\nวิธี Initialisation นี้เรียกว่า Xavier initialisation ตามชื่อของผู้คิดค้น ถูกออกแบบให้ใช้กับ Tanh activation function ซึ่งจะอธิบายในส่วนถัดไป\\n\\nอย่างไรก็ตาม ในปัจจุบันเรามักใช้ Activation function ตระกูล ReLU แทน Sigmoid และ Tanh ดังนั้นเราจึงต้องเปลี่ยนแปลงวิธี Initialise ของ Xavier ให้อยู่ในรูปนี้:\\n\\n```tex\\nr = \\\\sqrt{\\\\frac{6}{n_i}} \\\\tag{2}\\n```\\n\\nวิธีนี้เรียกว่า He initialisation ตามชื่อผู้คิดค้น ความแตกต่างกับ Xavier คือวิธีของ He พิจารณาเฉพาะจำนวนการเชื่อมต่อขาเข้าเท่านั้น\\n\\nทั้งสองวิธีเป็นวิธีสุ่มจาก Uniform distribution คือกระจายตัวออกเท่าๆ กันตลอด Range ของข้อมูล เราสามารถจะเลือกสุ่มแบบ Normal distribution คือกระจายการสุ่มให้กระจุกตัวเยอะตรงกลางๆ ของ Range ก็ได้ โดยการเปลี่ยนแปลงสมการเล็กน้อย ทั้งนี้ใน TensorFlow เราสามารถกำหนดวิธีการ Initialise ค่าน้ำหนักได้โดยใช้ Argument kernel_initializer ใน Layer เช่น:\\n```python\\nmodel.add(Dense(64,\\n                kernel_initializer=\\'he_uniform\\',\\n                bias_initializer=\\'zeros\\'))\\n```\\nอนึ่ง ค่าน้ำหนักของ b สามารถเริ่มด้วย 0 ได้ ไม่มีปัญหาอะไร\\n\\n**ปัจจุบัน แนะนำให้ใช้ He initialiser** เพราะออกแบบมาให้ใช้กับ Activation function ตระกูล ReLU และ ELU ซึ่งแนะนำให้ใช้แทน Sigmoid หรือ Tanh โดยจะพูดถึงเรื่องนี้ในส่วนถัดไป\\n### Activation functions\\n\\nทบทวนอีกครั้ง ว่าการคำนวนของแต่ละ Neuron ประกอบด้วยการรับ Input `a^{[l-1]}` มาถ่วงน้ำหนักกับ `W^l` และ `b^l` จะได้ Linear function `z^l` จากนั้นจะนำ `z^l` ไปเป็น Input ของ Activation function `g` ซึ่งได้ผลเป็น `a^l` เขียนรวมได้ทั้งหมดดังนี้:\\n\\n```tex\\nz^{[l](i)} = W^{[l]}a^{[l-1](i)} + b^{[l]} \\\\tag{3}\\n\\na^{[l](i)} = g(z^{[l](i)}) \\\\tag{4}\\n```\\n\\nหน้าที่ของ Activation function g คือการควบคุม Output ของ Neuron ให้อยู่ใน Range ที่ Neuron ชั้นถัดไปจะคำนวนได้ง่าย และถ้าหาก Activation นั้นอยู่ใน Hidden layer ชั้นสุดท้าย ก็จะมีหน้าที่ควบคุม Range ของ Output ให้สามารถตีความเป็นคำตอบได้ง่าย\\n\\nยกตัวอย่างเช่นเมื่อใช้ Sigmoid function แทน g ตามสมการด้านล่าง ค่า Activation ที่ได้จะอยู่ในช่วง 0 ถึง 1 เท่านั้น ซึ่งสะดวกในการตีความแบบ Classification (มากกว่า 0.5 คือ \"ใช่\" น้อยกว่าคือ \"ไม่ใช่\"):\\n\\n```tex\\n\\\\sigma(z) = \\\\frac{1}{1+e^{(-z)}} \\\\tag{5}\\n```\\n\\nSigmoid function หาก z สูงมากๆ Activation ก็จะได้ผลเข้าใกล้ 1 ในทางกลับกัน หาก z น้อยมากๆ Activation ก็จะได้ผลเข้าใกล้ 0\\n\\nนอกจาก Sigmoid ยังมี Activation function ที่คล้ายกัน คือ Tanh ย่อมาจาก Hyperbolic Tangent ซึ่งอยู่ในรูปนี้:\\n\\n```tex\\n\\\\text{tanh}(z) = \\\\frac{1-e^{-2z}}{1+e^{-2z}} \\\\tag{6}\\n```\\n\\nTanh ต่างจาก Sigmoid ตรงที่ Output จะอยู่ใน Range -1 และ 1 ซึ่งส่งผลให้การตีความแตกต่างออกไป โดยมีจุดตัดการตัดสินใจที่ 0 ไม่ใช่ 0.5\\n\\nสังเกตว่า Output range ของ Tanh คือ 2 สูงกว่า Range ของ Sigmoid ที่เท่ากับ 1 อยู่ 2 เท่า\\n\\nทั้ง Sigmoid และ Tanh ต่างมีปัญหาหลักอยู่ 2 อย่าง ดังนี้:\\n\\n* ใช้ Exponential ในการคำนวน ทำให้เปลืองพลังการประมวลผล\\n* เป็นหนึ่งในปัจจัยที่ทำให้เกิดปัญหา Vanishing gradients เพราะถ้าหาก Activation ของ Sigmoid / Tanh ออกมาเป็นค่าน้อยๆ หรือมากๆ อนุพันธ์ (ความชัน) ของ Sigmoid function ก็จะมีค่าน้อยมากเข้าใกล้ 0 เราต้องใช้อนุพันธ์ของ Sigmoid function ในการคำนวนอนุพันธ์ของ `z^{[1]}` ซึ่งจะถูกนำไปคำนวนอนุพันธ์ของ `w^{[1]}` และ `b^{[1]}` อีกต่อหนึ่ง นั่นหมายความว่า Neuron ที่ให้ค่า Activation สูงหรือต่ำเกินไป จะส่งผลให้การอัปเดต Parameter รอบถัดไปมีการเปลี่ยนแปลงน้อยลงกว่าที่ควรจะเป็น ซึ่งส่งผลให้กระบวนการ Optimisation ไม่สามารถหา Loss ที่ต่ำที่สุดได้\\n\\nปัญหาทั้งสองนี้แก้ได้โดยการเปลี่ยน Activation function เป็น ReLU\\n\\nReLU ย่อมาจาก Rectified Linear Unit เป็นฟังก์ชันที่เรียบง่ายมาก เรียกว่า Ramp function คือถ้า Input มีค่าตั้งแต่ 0 ขึ้นไป Output ก็จะมีค่าเท่ากัน ส่วน Input ที่น้อยกว่า 0 จะกำหนด Output เป็น 0 ตลอด\\n\\nสามารถเขียน ReLU เป็นสมการได้ดังนี้:\\n```tex\\n\\\\text{relu}(z) =\\n\\\\begin{cases}\\n    z \\\\quad \\\\text{if } z \\\\geq 0 \\\\\\\\\\n    0 \\\\quad \\\\text{if } z < 0\\n\\\\end{cases}\\\\tag{7}\\n```\\nหรือเขียนได้อีกอย่างว่า:\\n```tex\\n\\\\text{relu}(z) = \\\\text{max}(0, x)\\\\tag{8}\\n```\\nดูเผินๆ เหมือนว่า ReLU ไม่น่าจะช่วยให้โมเดลทำงานได้ดี เพราะ Output เป็น Linear function ที่แทบไม่ได้ดัดแปลงแก้ไขอะไร แต่ในความเป็นจริงพบว่า การใช้ ReLU ใน Hidden layer นอกจากจะทำให้การเทรนทำได้รวดเร็วกว่า Sigmoid หลายเท่าแล้ว ยังช่วยแก้ปัญหา Vanishing gradients ได้เป็นอย่างมาก เพราะความชันของฟังก์ชันจะเป็นค่าคงตัว คือ 1 เสมอ\\n\\nจะเห็นว่าการทำงานของ ReLU คือการ \"ส่งต่อ\" Linear function ออกไป โดยเลือกปิด Neuron บางตัวออกไปเลยหาก Neuron นั้นมีค่าน้อย ซึ่งพฤติกรรมนี้อาจส่งผลให้ Optimiser ไม่สามารถ Converge หา Loss ที่ต่ำเท่าที่ควร แต่ในความเป็นจริง ปัญหานี้ไม่ได้เกิดขึ้นตลอดเวลา เพราะ Activation function ใน Neuron หนึ่งๆ คือผลรวมของ Input ทุก Feature ดังนี้:\\n\\n```tex\\na_1^{[1](1)} = g(w_1^{[1](1)} x_1^{[1](1)} + w_2^{[1](1)} x_2^{[1](1)} + \\\\dots + w_n^{[1](1)} x_n^{[1](1)} + b_1^{[1](1)})  \\\\tag{9}\\n```\\n\\nนั่นแปลว่าต่อให้มี `wx` บางคู่ที่ให้ผลลบ แต่โดยรวมแล้ว `wx` ทุกคู่บวกรวมกัน ก็มีโอกาสที่จะเป็นบวกสูงกว่าเป็นลบมาก ดังนั้นจึงมีโอกาสน้อยที่ ReLU จะให้ผลออกมาเป็น 0\\n\\nแต่ก็ไม่ใช่ว่าจะไม่มีโอกาสที่ ReLU จะให้ผลลบเลย ในบางโมเดล Neuron หลายสิบเปอร์เซ็นถูกปิดเพราะ ReLU ทางแก้คือใช้ Activation function ที่ดัดแปลงส่วนแบนของ ReLU ให้มีความลาดเอียงเล็กน้อย เรียกว่า Leaky ReLU:\\n```tex\\n\\\\text{relu}(z) = \\\\text{max}(0.01, x)\\\\tag{10}\\n```\\nหน้าตาของกราฟจะคล้าย ReLU แต่ส่วนบนจะลาดลงเล็กน้อย เช่นถ้า Input เท่ากับ -1 จะส่งผลให้ Output เท่ากับ -0.01 เป็นต้น\\n\\nล่าสุด ในปี 2015 มีคณะนักวิจัยเสนอ Activation function ใหม่ที่ชื่อ ELU หรือ Exponential Linear Unit ที่ทดสอบแล้วได้ผลเหนือกว่า ReLU และ Leaky ReLU ทุกด้าน เช่น เวลาที่ใช้เทรนโดยรวมสั้นกว่า และได้ความแม่นยำสูงกว่า ELU มีสมการดังนี้:\\n```tex\\n\\\\text{ELU}_{\\\\alpha}(z) =\\n\\\\begin{cases}\\n    \\\\alpha(e^z-1) \\\\quad \\\\text{if } z < 0 \\\\\\\\\\n    z \\\\quad \\\\text{if } z \\\\geq 0\\n\\\\end{cases}\\\\tag{11}\\n```\\n\\nELU มีคุณสมบัติที่อุดช่องว่างของ Activation function อื่นๆ ที่ผ่านมา ช่วยลดปัญหา Vanishing gradients เพราะอนุพันธ์ของฟังก์ชันยังมีค่าบวกอยู่ถึงแม้ Input จะมีค่าน้อยกว่า 0 ไปจนถึง -2 โดยเราสามารถปรับค่า Hyperparameter `\\\\alpha` เพื่อกำหนดว่า Output จะมีค่าเท่าใดเมื่อ Input มีค่าติดลบมากๆ โดยปกติตั้งไว้ที่ 1\\n\\nข้อเสียของ ELU คือต้องใช้พลังประมวลผลมาก แต่ก็แลกกับการที่ทำให้ Optimiser นั้น Converge เร็วขึ้น อย่างไรก็ตามในเวลาทดสอบกับ Test set หรือเวลาใช้โมเดลพยากรณ์จริง (Forward propagation ขาเดียว) ELU จะใช้เวลานานกว่า ReLU ดังนั้นใน Application ที่ความเร็วในการได้ผลลัพธ์เป็นสิ่งสำคัญ ก็ควรพิจารณาให้ดีว่ายอมรับความช้าลงได้หรือไม่\\n\\n**ปัจจุบันแนะนำให้ใช้ ELU เป็นอันดับแรก** ตามด้วย Leaky ReLU, ReLU, Tanh, และ Sigmoid\\n\\nใน TensorFlow เราสามารถเลือกใช้ Activation function ใดก็ได้ เพียงแค่เปลี่ยนค่าของ Argument activation ใน Model layer\\n\\nต่อมาเป็นเทคนิคสุดท้ายเพื่อลดปัญหา Vanishing gradients นั่นคือ Batch normalisation\\n\\n### Batch normalisation\\nเวลาเทรนโมเดลหลายๆ ครั้ง ค่าเฉลี่ยและการกระจายตัวของ Input สำหรับแต่ละ Neuron ในแต่ละ Layer จะมีแนวโน้มที่จะเปลี่ยนไปเรื่อยๆ ซึ่งทำให้ Gradient descent ทำงานได้ไม่เต็มประสิทธิภาพ เช่น ใช้เวลานานในการคำนวน และต้องใช้จำนวนรอบมากกว่าจะ Converge ได้ เรียกปัญหานี้ว่า Internal covariate shift ดังนั้นเมื่อพิจารณาว่าข้อมูล Input เริ่มแรกต้องถูก Scale ให้เรียบร้อยเสียก่อน ทำไมเราจึงไม่ Scale และ Normalise ข้อมูล Input ของแต่ละ Layer ระหว่างการเทรนไปด้วยเสียเลย\\n\\nBatch normalisation คือการปรับ Mean และ Variance ของ Input สำหรับแต่ละ Layer ให้เป็นมาตรฐาน แล้วปรับ Scale และตำแหน่งของ Input ทั้งชุดให้ เทคนิคนี้เพิ่งถูกเสนอเมื่อปี 2015 นี้เอง การทำ Batch normalisation นอกจากจะทำช่วยลดปัญหา Vanishing gradient ลงเกือบทั้งหมดแล้ว ยังทำให้เราสามารถตั้ง Learning rate ได้เร็วขึ้น หรือลดจำนวน Epoch ที่ต้องเทรน (เพราะ Input ไม่ใหญ่เกินไป) ส่งผลให้เทรนโมเดลได้เร็วขึ้น นอกจากนี้ยังให้ผลในการ Regularise ด้วยระดับหนึ่ง\\n\\nขั้นตอนในการทำ Batch normalisation มีดังนี้:\\n\\n1) หา Mean `\\\\mu_B` และ Variance `\\\\sigma_B^2` ของ Mini-batch B:\\n```tex\\n\\\\mu_B = \\\\frac{1}{m_B} \\\\sum\\\\limits_{i=1}^{m_B} x^{(i)} \\\\tag{12}\\n\\n\\\\sigma_B^2 = \\\\frac{1}{m_B} \\\\sum\\\\limits_{i=1}^{m_B} (x^{(i)} - \\\\mu_B)^2 \\\\tag{13}\\n```\\n2) Normalise z ด้วยการนำ Input z ไปลบกับ Mean ของ Mini-batch และหารส่วนเบี่ยงเบนมาตรฐานของ Mini-batch:\\n```tex\\nz_{norm}^{(i)} = \\\\frac{z^{(i)}-\\\\mu_B}{\\\\sqrt{\\\\sigma_B^2 + \\\\epsilon}} \\\\tag{14}\\n```\\nโดย `\\\\epsilon` อ่านว่า Epsilon คือจำนวนขนาดเล็กมากๆ ที่บวกเข้าไปเพื่อป้องกันไม่ให้ตัวหารของสมาการเป็น 0 โดยปกติตั้งเป็น `10^{-5}`\\n\\n3) Scale และปรับตำแหน่งชุดข้อมูลโดยการนำ `z_{norm}^{(i)}` ไปคำนวนร่วมกับ Scaling parameter `\\\\gamma` และ Shifting parameter `\\\\beta`:\\n\\n```tex\\n\\\\tilde{z}^{(i)} = \\\\gamma z_{norm}^{(i)} + \\\\beta \\\\tag{15}\\n```\\n\\nโดย Scaling parameter \\\\gamma และ Shifting parameter \\\\beta จะถูกใส่เข้าไปใน Backward propagation เพื่อให้ Algorithm ปรับค่าทั้งสองนี้ระหว่างเทรน\\n\\nมีข้อสังเกตว่าถ้าเราทำ Batch normalisation บน Hidden layer แรก ก็ไม่มีความจำเป็นต้อง Normalise input ตั้งแต่แรก\\n\\nอย่างไรก็ตาม Batch norm ทำให้การคำนวน Forward propagation ช้าลง ดังนั้นจึงควรพิจารณาว่าการใช้ He initialisation บวกกับ ELU activation function นั้นให้ผลดีเพียงพอแล้วหรือไม่\\n\\nใน TensorFlow เราสามารถทำ Batch normalisation โดยการสร้าง Layer ชื่อ BatchNormalization ได้เลย\\n\\nเป็นอันว่าเราเรียนรู้เทคนิคที่จะลดปัญหา Vanishing gradients ได้ถึง 3 แนวทาง บทต่อไปเราจะมาทำความรู้จักกับ Optimiser ต่างๆ ที่จะช่วยเพิ่มความเร็วในการเทรนโมเดล\\n## บทที่ 17 Neural Network Optimisers\\nมาจนถึงตอนนี้ เรารู้ว่ากลไกที่นำเอาอนุพันธ์ของตัวแปรต่างๆ มาอัปเดตเพื่อลด Loss ใน Cost function นั้นเรียกว่า Gradient descent ซึ่งทำงานโดยการนำเอาอนุพันธ์ไปถ่วงน้ำหนักด้วย Learning rate `\\\\alpha` แล้วลบออกจาก Parameter ดังนี้:\\n```tex\\nW^{[l]} := W^{[l]} - \\\\alpha (dW^{[l]}) \\\\tag{1}\\n\\nb^{[l]} := b^{[l]} - \\\\alpha (db^{[l]}) \\\\tag{2}\\n```\\nโดย [l] คือลำดับที่ของ Layer\\n\\nกลไกการ Optimise แบบนี้ มักเกิดปัญหาว่าถ้าหากอนุพันธ์นั้นมีค่าน้อย (คือมีความชันน้อย) ก็จะทำให้ Parameter ถูกอัปเดตแต่ละรอบช้ามาก ส่งผลให้ใช้เวลานานกว่าโมเดลจะเทรนจนถึงจุดที่ Loss ค่ำที่สุดเท่าที่จะเป็นไปได้\\n\\nปัจจุบัน มี Algorithm ชั้นสูงเพื่อเพิ่มประสิทธิภาพการ Optimise ให้เลือกใช้ โดย Optimiser เหล่านี้ล้วนมีหลักการคล้ายกัน คือนำค่าอนุพันธ์ล่าสุดไปเพิ่มน้ำหนักให้แก่การ Optimise ในรอบถัดไป เพื่อเร่งความเร็วของการอัปเดตให้มากขึ้น ในบทนี้เราจึงจะมาเรียนรู้ Optimiser ชั้นสูง 3 ตัว ได้แก่ Momentum, RMSProp, และ Adam โดยเราสามารถเรียกใช้ Optimiser เหล่านี้ได้โดยการกำหนด Argument ใน Method .compile เช่น:\\n```python\\nmodel.compile(optimizer=\\'adam\\',\\n              loss=\\'sparse_categorical_crossentropy\\',\\n              metrics=[\\'accuracy\\'])\\n```\\n### Momentum\\n\\nMomentum optimiser ใช้อนุพันธ์ล่าสุดมาทำให้การเคลื่อนที่ของ Cost function นั้นพุ่งไปข้างหน้าอย่างนุ่มนวล หัวใจหลักของความสามารถนี้คือการใช้ **Exponentially weighted moving average (EMA) ของอนุพันธ์** มาอัปเดต Parameter\\n\\nวิธีการคำนวน Momentum มีดังนี้:\\n\\n1) คำนวน Vector ของ \"น้ำหนักเร่ง\" v โดยใช้ EMA:\\n```tex\\nv_{dW^{[l]}} = \\\\beta v_{dW^{[l]}} + (1-\\\\beta)dW^{[l]} \\\\tag{3}\\n\\nv_{db^{[l]}} = \\\\beta v_{db^{[l]}} + (1-\\\\beta)db^{[l]} \\\\tag{4}\\n```\\n    `v_{dW^{[l]}}` และ `v_{db^{[l]}}` คือ Vector ของน้ำหนักเร่งของ dW และ db ตัวที่แล้ว โดยเริ่ม Initialise ด้วย 0\\n    `\\\\beta` คือค่าน้ำหนักของ EMA ปกติตั้งที่ 0.9 ซึ่งแปลว่า \"ค่าน้ำหนักเร่งใหม่ มาจากค่าน้ำหนักเร่งเก่า 9 ส่วน และ dW 1 ส่วน\"\\n\\n2) นำค่าน้ำหนักเร่ง `v_{dW^{[l]}}` และ `v_{db^{[l]}}` ที่ได้ ไปถ่วง Learning rate `\\\\alpha` และลบออกจาก `W^{[l]}` และ `b^{[l]}`:\\n```tex\\nW^{[l]} := W^{[l]} - \\\\alpha (v_{dW^{[l]}}) \\\\tag{5}\\n\\nb^{[l]} := b^{[l]} - \\\\alpha (v_{db^{[l]}}) \\\\tag{6}\\n```\\nสังเกตว่าสมการที่ (5) และ (6) นั้นก็คือการอัปเดต Parameter ตามปกติ เพียงเปลี่ยนตัวแปรที่เอาไปถ่วงน้ำหนักและลบออก จากเดิมใช้ `dW^{[l]`  และ `db^{[l]}` เป็นค่าน้ำหนักเร่ง `v_{dW^{[l]}}` และ `v_{db^{[l]}}`\\n### RMSProp\\n\\nRMSProp ย่อมาจาก Root Mean Square Propagation มีคุณสมบัติคล้ายกับ Momentum แต่แทนที่จะใช้ EMA ของอนุพันธ์ตัวที่ผ่านๆ มาในการอัปเดต Parameter เปลี่ยนไปใช้ EMA ของยกกำลังสองของอนุพันธ์แทน โดยมีวิธีการคำนวนดังนี้:\\n\\n1) คำนวน Vector ของ \"น้ำหนักเร่ง\" s โดยใช้ EMA ของยกกำลังสองของอนุพันธ์:\\n```tex\\ns_{dW^{[l]}} = \\\\beta s_{dW^{[l]}} + (1-\\\\beta)dW^{[l]2} \\\\tag{7}\\n\\ns_{db^{[l]}} = \\\\beta s_{db^{[l]}} + (1-\\\\beta)db^{[l]2} \\\\tag{8}\\n```\\n2) นำค่าน้ำหนักเร่ง `s_{dW^{[l]}}` และ `s_{db^{[l]}}` ที่ได้ ไปประกอบกับ `dW^{[l]}` และ `db^{[l]}` และถ่วงกับ Learning rate `\\\\alpha` ตามสมการด้านล่าง แล้วลบออกจาก `W^{[l]}` และ `b^{[l]}`:\\n```tex\\nW^{[l]} := W^{[l]} - \\\\alpha \\\\left(\\\\frac{dW^{[l]}}{\\\\sqrt{s_{dW^{[l]}}} + \\\\epsilon}\\\\right) \\\\tag{9}\\n\\nb^{[l]} := b^{[l]} - \\\\alpha \\\\left(\\\\frac{db^{[l]}}{\\\\sqrt{s_{db^{[l]}}} + \\\\epsilon}\\\\right) \\\\tag{10}\\n```\\nโดย `\\\\epsilon` คือค่าคงที่ขนาดเล็กๆ ที่ป้องกันไม่ให้ตัวหารเป็น 0\\n\\nสังเกตว่า Term ที่เอาไปถ่วงกับ Learning rate นั้นไม่เหมือน Momentum กล่าวคือเป็นการนำ Square root ของค่าน้ำหนักเร่งไปหารออกจากอนุพันธ์ วิธีการนี้จะช่วยจำกัดความแปรผันในทิศที่ตั้งฉากกับทิศทางที่ไปสู่จุดต่ำสุดของ Cost function ในขณะที่ Momentum สนใจในการสร้างแรงที่มุ่งไปสู่ทิศต่ำสุดของ Cost function อย่างเดียว\\n### Adam\\n\\nAdam ย่อมาจาก Adaptive Moment Estimation เป็นการรวม Momentum และ RMSProp เข้าด้วยกัน ถือว่าเป็น Optimiser ที่ดีที่สุดในปัจจุบัน มีวิธีการคำนวนดังนี้ (แสดงเฉพาะสำหรับ w):\\n\\n1) คำนวน Vector ของ \"น้ำหนักเร่ง\" v โดยใช้ EMA ตามแบบ Momentum และ Vector ของ \"น้ำหนักเร่ง\" s โดยใช้ EMA ตามแบบ RMSProp แล้วแก้ Bias ที่เกิดจากการที่ v และ s ถูก Initialise ด้วย 0 (ถ้าไม่แก้จะทำให้ v และ s รอบแรกๆ มีค่าต่ำกว่าที่ควรจะเป็น):\\n```tex\\nv_{dW^{[l]}} = \\\\beta_1 v_{dW^{[l]}} + (1-\\\\beta_1)dW^{[l]} \\\\tag{11}\\n\\nv_{dW^{[l]}}^{corrected} = \\\\frac{v_{dW^{[l]}}}{1-\\\\beta_1^t} \\\\tag{12}\\n\\ns_{dW^{[l]}} = \\\\beta_2 s_{dW^{[l]}} + (1-\\\\beta_2)dW^{[l]2} \\\\tag{13}\\n\\ns_{dW^{[l]}}^{corrected} = \\\\frac{s_{dW^{[l]}}}{1-\\\\beta_2^t} \\\\tag{14}\\n```\\n2) นำค่าน้ำหนักเร่ง `v_{dW^{[l]}}^{corrected}` และ `s_{dW^{[l]}}^{corrected}` ไปประกอบกันในลักษณะเดียวกับ RMSProp คือ `v_{dW^{[l]}}^{corrected}` เป็นตัวตั้ง หารด้วย Square root ของ `s_{dW^{[l]}}^{corrected}` เพื่อจำกัดความแปรผันในทิศที่ตั้งฉากกับทิศทางที่ไปสู่จุดต่ำสุดของ Cost function แล้วถ่วงกับ Learning rate `\\\\alpha` แล้วลบออกจาก `W^{[l]}` และ `b^{[l]}`:\\n```tex\\nW^{[l]} := W^{[l]} - \\\\alpha \\\\left(\\\\frac{v_{dW^{[l]}}^{corrected}}{\\\\sqrt{s_{dW^{[l]}}^{corrected}} + \\\\epsilon}\\\\right) \\\\tag{15}\\n```\\nในทางปฏิบัติ แนะนำให้ใช้ Adam ไปเลย เว้นแต่จะมีปัญหาก็สามารถลองใช้ Momentum ได้\\n\\nในบทต่อไปจะพูดถึงเทคนิคการ Regularise deep neural network ซึ่งรวมทั้ง Dropout layer\\n## บทที่ 18 Neural Network\\nRegularisation คือเทคนิคในการลดปัญหา Variance ของโมเดล นั่นก็คือการที่โมเดลถูกเทรนแล้วฟิตกับข้อมูลชุด Train set ได้ดี แต่พยากรณ์ไม่แม่นยำเมื่อนำไปใช้กับ Test set หรือข้อมูลจริงที่โมเดลไม่เคยเห็น บางทีเราเรียกปัญหานี้ว่า Generalisation error\\n\\nปัญหา Generalisation error เกิดขึ้นจากการที่โมเดลฟิต Parameter เข้ากับ Train set อย่างรัดกุมเกินไป จนกระทั่งไม่สามารถพยากรณ์ข้อมูลที่มองไม่เห็นได้อย่างเที่ยงตรงเท่าที่ควร\\n\\nซึ่งถ้าหากเราดำเนินการเพื่อลด Generalisation error ก็จะได้โมเดลที่มีความซับซ้อนน้อยลง ทำให้สามารถ Generalise ไปยังข้อมูลชุดใหม่ๆ ได้ดียิ่งขึ้น\\n\\nเทคนิคการลด Generalisation error ที่จะนำเสนอ คือ l^2 regularisation, Dropout regularisation, และ Early stopping\\n### L2 regularisation\\nแนวทางพื้นฐานในการแก้ปัญหา Generalisation error คือการเพิ่ม Regularisation term เข้าไปใน Cost function และในการอัปเดต Parameter เพื่อกดค่า w ให้น้อยลง\\n\\nค่าของ Regularisation term นี้ จะแปรผันตามความซับซ้อนของโมเดล ด้วยการใช้ `l^2` norm ของ W เป็นพื้นฐาน\\n\\nCost function ที่รวม Regularisation term แล้ว มีสมการรูปทั่วไปดังนี้:\\n```tex\\nJ = \\\\frac{1}{m} \\\\sum\\\\limits_{i=1}^m L(\\\\hat{y}^{(i)}, y^{(i)}) + \\\\frac{\\\\lambda}{2m} \\\\sum\\\\limits_{l=1}^L \\\\Vert{W^{[l]}}\\\\Vert_F^2 \\\\tag{1}\\n```\\nโดย `\\\\Vert{W^{[l]}}\\\\Vert_F^2` คือ Frobenius norm ของ Matrix W คำนวนจากผลรวมของ w ยกกำลังสองทุกตัวของทุก Layer ตั้งแต่ Hidden layer ไปจนถึง Output layer สามารถแทนเป็นสมการดังนี้:\\n```tex\\n\\\\Vert{W^{[l]}}\\\\Vert_F^2 = \\\\sum\\\\limits_{i=1}^{n^{l-1}} \\\\sum\\\\limits_{l=1}^{n^l} (W_{i,j}^{[l]})^2 \\\\tag{2}\\n```\\nโดย W เป็น Matrix ขนาด `n^{l-1}` คูณ `n^{l}`\\n\\nสิ่งที่ Regularisation term นี้ทำ ก็คือการกดดันให้ค่าน้ำหนัก w มีค่าน้อยลง โดยการทำงานคู่กับการที่เราบวก `\\\\frac{\\\\lambda}{m} W^{[l]}` เข้าไปในอนุพันธ์ของ w ทำให้ `dW^{[l]}` มีค่ามากขึ้น ส่งผลให้ `W^{[l]}` มีค่าน้อยลง ดังนี้:\\n```tex\\ndW^{[l]} = \\\\text{from backprop} + \\\\frac{\\\\lambda}{m} W^{[l]} \\\\tag{3}\\n\\nW^{[l]} := W^{[l]} - \\\\alpha(dW^{[l]}) \\\\tag{4}\\n```\\nโดยการลดลงของ `w` นี้ จะแปรผันตามขนาดที่แท้จริงของ W เราเรียกพฤติกรรมนี้ว่าการถดถอยน้ำหนัก หรือ Weight decay\\n\\nสังเกตว่าการทำ `l^2` regularisation นี้ เราต้องกำหนด Hyperparameter \\\\lambda ที่ควบคุมว่า Regularisation term จะมีผลต่อการถดถอยของน้ำหนักเพียงใด\\n\\nใน TensorFlow เราสามารถทำ `l^2` regularisation โดยการกำหนด Argument ใน Layer ดังนี้:\\n```python\\nfrom keras import regularizers\\nmodel.add(Dense(64, input_dim=64,\\n                kernel_regularizer=regularizers.l2(0.01)))\\n```\\nการ Regularise ด้วยวิธีนี้ เป็นวิธีทั่วไปที่ใช้ในสถิติและ Machine learning algorithm อื่นๆ อย่างไรก็ตาม เมื่อใช้กับ Deep neural network `l^2` regularisation มีปัญหาอันหนึ่ง คือปรากฏการณ์ Co-adaptation กล่าวคือมันทำให้ค่าน้ำหนักที่มีค่ามาก ยิ่งขยายขนาด ส่วนค่าน้ำหนักที่มีค่าน้อย ยิ่งลดขนาด นั่นก็คือการขยายความต่างนั่นเอง ปรากฏการณ์นี้ทำให้เราไม่สามารถใช้ประโยชน์จากโมเดลที่มีความซับซ้อนมากขึ้นได้เต็มที่ เพราะเมื่อเราทำให้โมเดลซับซ้อนขึ้น ผลที่ได้กลับเป็นโมเดลที่เน้นสิ่งที่มีค่ามาก และกดสิ่งที่มีค่าน้อยลงอย่างเป็นระบบ ดังนั้นเราจึงมีวิธีการ Regularise พิเศษสำหรับ Neural network ที่เรียกว่า Dropout\\n### Dropout regularisation\\nหลักคิดของ Dropout นั้นเรียบง่ายมาก เราจะ \"ปิด\" Neuron บางตัวในระบบ ไม่ให้รับข้อมูลใดๆ ดังนั้น Neuron ที่ถูกปิดก็จะไม่ส่งข้อมูลออกไปยัง Neuron ตัวอื่นเช่นเดียวกัน\\n\\nในระบบนี้ แสดงการกำหนด Dropout เท่าเท่ากับ 0.25 บน Layer ที่ 2 และ 3 นั่นคือ ให้ปิด Neuron 25% ของ Layer ดังกล่าว\\n\\nการ \"ปิด\" Neuron บางตัวในเครือข่ายแต่ละชั้น เป็นการเลือกปิดแบบสุ่ม ซึ่งฟังเผินๆ อาจจะดูเหมือนเป็นความคิดที่ไม่ดี แต่ในความจริงเทคนิคนี้ใช้งานได้ดี เพราะเราไม่ได้ไปปิด Input x แต่ไปปิดหน่วยประมวลผลบางตัว การเลือกปิดแบบสุ่มนี้จะป้องกันการเกิด Co-adaptation และได้ผลในการลด Overfitting เพราะทำให้โมเดลนั้น \"ง่าย\" ลง แต่ไม่ได้จงใจกดข้อมูลลักษณะใดลักษณะหนึ่งลง เป็นการสุ่มกดเพื่อลดความซับซ้อนของทั้งระบบ\\n\\nในทางเทคนิค วิธีการทำ Dropout คือการสร้าง Boolean matrix มิติเท่ากับ Matrix ของ Activation ที่ต้องการใช้ Dropout โดยสุ่มค่า Yes/No ตามสัดส่วนที่เรากำหนด เช่น ถ้ากำหนดให้ปิด 25% ก็คือให้ทั้ง Matrix มีค่า True 75% ค่า False 25% จากนั้นนำ Dropout matrix นี้ไปคูณแบบ Element-wise เข้ากับ Activation matrix ก็จะได้ Activation matrix ใหม่ที่บาง Element มีค่าเท่ากับศูนย์ในตำแหน่งเดียวกับที่ใน Dropout matrix มีค่าเป็น False จากนั้นนำสัดส่วนการปิดไปหารแบบ Element-wise ออกจาก Activation matrix ใหม่ เพื่อทำการ Normalise\\n\\nแสดงวิธีการทำ Dropout ใน iPython ได้ดังนี้:\\n```python\\nIn [1]: keep_prob = 0.75\\n\\nIn [2]: import numpy as np\\n\\nIn [3]: d = np.random.rand(3,4) < keep_prob\\n\\nIn [4]: d\\nOut[4]:\\narray([[False,  True,  True,  True],\\n       [ True,  True,  True,  True],\\n       [ True, False,  True,  True]])\\n\\nIn [5]: a = np.random.randn(3,4)\\n\\nIn [6]: a\\nOut[6]:\\narray([[-0.37998077, -0.96594641,  0.54147094, -0.35392546],\\n       [-0.34891998, -0.53428796, -1.03811021, -2.36410892],\\n       [-0.26496699,  0.21605195, -1.11321207,  0.53065364]])\\n\\nIn [7]: a_dropped = np.multiply(a,d)\\n\\nIn [8]: a_dropped\\nOut[8]:\\narray([[-0.        , -0.96594641,  0.54147094, -0.35392546],\\n       [-0.34891998, -0.53428796, -1.03811021, -2.36410892],\\n       [-0.26496699,  0.        , -1.11321207,  0.53065364]])\\n\\nIn [9]: a_dropped /= keep_prob\\n\\nIn [10]: a_dropped\\nOut[10]:\\narray([[-0.        , -1.28792855,  0.72196126, -0.47190062],\\n       [-0.46522664, -0.71238394, -1.38414694, -3.15214523],\\n       [-0.35328932,  0.        , -1.48428276,  0.70753819]])\\n```\\nใน TensorFlow เราสามารถทำ Dropout ได้อย่างง่ายดาย โดยการเพิ่ม Dropout layer ลงไปก่อนหน้า Layer ที่เราต้องการทำ Dropout ดังนี้:\\n```python\\nmodel.add(Dropout(0.25)) # Dropout 25%\\n```\\nในปัจจุบัน แนะนำให้ใช้ Dropout ก่อน สำหรับ Deep neural network\\n### Early stopping\\nEarly stopping คือการหยุดการเทรนก่อนที่ Optimiser จะ Converge หา Loss ที่ต่ำที่สุด โดยมีสมมุติฐานว่ายิ่งเทรนไป โมเดลจะยิ่งซับซ้อนขึ้น จนอาจจะเกินจากที่จะสามารถ Generalise ได้ดี\\n\\nจะเห็นว่า ตามปกติ Optimiser จะเทรนเพื่อให้ได้ Train error ต่ำที่สุด แต่บางครั้ง Test error จะเริ่มวกกลับไปมีค่าสูงขึ้นก่อนที่จะพบจุดต่ำสุดของ Cost function ดังนั้น เราควรจะหยุดการเทรนที่จุดที่ Test error ต่ำที่สุด ไม่ใช่ Train error ต่ำที่สุด\\n\\nวิธีการในทางปฏิบัติ คือเมื่อเราเทรนโมเดลด้วยการเรียก Method `.fit` ให้เราแยกข้อมูลเทรนส่วนหนึ่งออกมาเป็น Validation set เพื่อให้ Optimiser ทดสอบความแม่นยำทุกๆ Epoch ดังนี้:\\n```python\\n# Fit the model to data\\nmodel.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\\n```\\nระหว่างเทรน จะได้ผลออกมาดังนี้:\\n```\\nTrain on 1600 samples, validate on 400 samples\\nEpoch 1/20\\n[===] - 1s 491us/sample - loss: 1.4701 - accuracy: 0.5881 - val_loss: 0.7175 - val_accuracy: 0.8275\\nEpoch 2/20\\n[===] - 0s 158us/sample - loss: 0.6571 - accuracy: 0.8213 - val_loss: 0.4700 - val_accuracy: 0.8600\\nEpoch 3/20\\n[===] - 0s 146us/sample - loss: 0.4681 - accuracy: 0.8744 - val_loss: 0.3726 - val_accuracy: 0.8925\\n.           .           .           .           .           .           .\\n.           .           .           .           .           .           .\\n.           .           .           .           .           .           .\\nEpoch 18/20\\n[===] - 0s 233us/sample - loss: 0.0807 - accuracy: 0.9844 - val_loss: 0.2492 - val_accuracy: 0.9375\\nEpoch 19/20\\n[===] - 0s 174us/sample - loss: 0.0675 - accuracy: 0.9906 - val_loss: 0.2474 - val_accuracy: 0.9275\\nEpoch 20/20\\n[===] - 0s 174us/sample - loss: 0.0638 - accuracy: 0.9881 - val_loss: 0.2490 - val_accuracy: 0.9300\\n```\\nให้สังเกต Validation accuracy ซึ่งจะเพิ่มขึ้นเรื่อยๆ แต่ถึงจุดหนึ่ง จะวกกลับเป็นมีค่าลดลง เช่น Epoch 18 ได้ 93.75% ส่วน Epoch 19 กลับแม่นยำลดลงเป็น 92.75% ดังนั้น เราอาจเลือกหยุดการเทรนที่ Epoch 18 แทนที่จะเป็น Epoch 20 เป็นต้น\\n\\nเราสามารถเลือกใช้เทคนิคการ Regularise แบบต่างๆ นี้ได้ตามสถานการณ์ อย่างไรก็ตาม ก่อนที่จะใช้เทคนิคการ Regularisation เพื่อลด Generalisation error ขอให้พิจารณาดูก่อนว่า Validation set หรือ Test set ที่เราเอามาทดสอบ มีขนาดเล็กเกินไป หรือมาจาก Distribution ที่ต่างจาก Train set หรือไม่ ซึ่งเป็นสิ่งที่ควรจะแก้ไขก่อน\\n\\nตอนนี้เรารู้จักหลักการและเทคนิคพื้นฐานของ Deep learning แล้ว ในบทต่อไปจะพูดถึงโครงสร้าง Deep learning model สำหรับงานเฉพาะด้าน เช่นการจำแนกภาพ โดยใช้ Convolutional Neural Network หรือ CNN\\n## บทที่ 19 Convolutional Neural Network\\nNeural network ทั่วไปอาจทำงานได้ดีกับข้อมูลที่ไม่ซับซ้อนและถูกเตรียมมาให้มีมาตรฐานเดียวกัน เช่นภาพขาวดำขนาดเล็กที่วัตถุอยู่กลางภาพ แต่พอข้อมูลมีความซับซ้อน ขนาด และความหลากหลายมากขึ้น จะทำงานได้ไม่ดีนัก โดยมักจะเกิดปัญหา Variance ซึ่งก็คือการที่โมเดลไม่สามารถทำนายข้อมูลที่ไม่เคยเห็นได้ดีเท่าที่ควร\\n\\nในบทนี้เราจะมาทำความรู้จัก Convolutional Neural Network หรือ CNN ซึ่งเป็นโครงสร้าง Neural network แบบพิเศษ ที่มีความสามารถในการจำแนกข้อมูลประเภทรูปภาพได้ดีกว่า Neural network ทั่วไปมาก\\n\\nไอเดียหลักของ CNN คือการที่ใช้ Layer ชนิดพิเศษ ที่เรียกว่า Convolution layer ซึ่งทำหน้าที่สกัดเอาส่วนต่างๆ ของภาพออกมา เช่น เส้นขอบของวัตถุต่างๆ เพื่อให้โมเดลสามารถเรียนรู้ลักษณะของภาพได้อย่างมีประสิทธิภาพและแม่นยำ\\n\\nใน CNN จะใช้ Convolution layer มาประกอบกับ Layer ชนิดอื่น เช่น Pooling layer แล้วนำกลุ่ม Layer ดังกล่าวมาซ้อนต่อๆ กัน โดยอาจเปลี่ยน Hyperparameter บางอย่าง เช่นขนาดของ Filter layer (ซึ่งเป็นส่วนหนึ่งของ Convolution layer) และจำนวน Channel ของ layer วิธีการนำเอาส่วนต่างๆ มาประกอบกันนี้ เรียกว่าเป็นโครงสร้าง (Architecture) ของ CNN ซึ่งมีหลายแบบ เช่น LeNet, AlexNet, VGG, ResNet, Inception Network เป็นต้น ซึ่งเราจะพูดถึงโครงสร้างแบบต่างๆ นี้ในบทถัดไป แต่ตอนนี้เรามีทำความเข้าใจส่วนประกอบต่างๆ ของ CNN ซึ่งเป็นพื้นฐานที่จะนำมาประกอบกันเสียก่อน โดยเริ่มที่ Convolution layer\\n### Convolution layer\\n\\nเพื่อความเข้าใจ ให้ลองทำความเข้าใจก่อนว่า Convolution คืออะไร\\n\\nสมมุติเรามี Matrix ซ้ายมือ ขนาด 6x6 และมี Matrix ตรงกลาง ซึ่งเรียกว่า Filter หรือ Kernel ขนาด 3x3 เราจะนำเฉพาะ 3x3 ช่องแรกของ Matrix แรก มาคูณแบบ Element-wise กับ Filter matrix แล้วนำผลที่ได้แต่ละค่า (ซึ่งมีทั้งสิ้น 9 ค่า) มาบวกกัน แล้วนำไปสู่ในแถวแรกคอลัมน์แรกของ Matrix ที่สามซึ่งเป็นผลลัพธ์ ผลลัพธ์ที่ว่า เท่ากับ -1\\n\\nถัดมา เราจะเลื่อนกรอบขนาด 3x3 ใน Matrix แรกไปทางขวา 1 ช่อง แล้วทำแบบเดิม ผลลัพธ์ที่ได้ นำไปใส่ในแถว 1 ช่อง 2 ของ Matrix ผลลัพธ์ ทำไปเรื่อยๆ จนสุดทาง แล้วเลื่อนกรอบ 3x3 ลงมาด้านล่าง 1 ช่อง (ชิดขอบด้านซ้ายมือ) แล้วทำแบบเดิม จนกระทั่งเติมค่าใน Matrix ผลลัพธ์จนเต็ม\\n\\nกระบวนการนี้ เรียกว่า Convolution ซึ่งแสดงสัญลักษณ์ด้วย * ส่วน Neural network ที่มี Layer ที่ใช้กระบวนการ Convolution นี้อย่างน้อย 1 Layer เราก็เรียกว่า Convolutional neural network\\n\\nถ้าหากข้อมูล Input เป็นภาพสี RGB แปลว่าจะมีค่าสีที่เป็น Matrix ขนาด 6x6 จำนวน 3 ชั้น ดังนั้น Filter ของเราก็จะมี 3 ชั้นตามไปด้วย ส่วนการคำนวน Convolve ก็ให้ทำทีละชั้น นั่นคือ Input ชั้นที่ 1 คูณ Element-wise กับ Filter ชั้นที่ 1 ส่วน Input ชั้นที่ 2 คูณ Element-wise กับ Filter ชั้นที่ 2 เป็นต้น เสร็จแล้วนำผลที่ได้ของทั้ง 3 ชั้นมาบวกกันทั้งหมด กลายเป็นตัวเลข Scalar ตัวเดียว แล้วนำไปใส่ในช่องแรกของ Matrix ผลลัพธ์\\n\\nในกระบวนการนี้ Matrix แรกด้านซ้ายมือที่เป็น Input ก็คือ `a^{[l-1]}` ส่วน Filter matrix คือ Matrix ของค่าน้ำหนัก `W^{[l]}` ซึ่งมีค่าที่ต้องเรียนรู้จากกระบวนการ Backward propagation ผลที่ได้จากการ Convolve 2 Matrix นี้เข้าด้วยกัน คือ Matrix ผลลัพธ์ นั่นคือ `W^{[l]}a^{[l-1]}` นั่นเอง จากนั้นเราก็บวก Intercept `b^{[l]}` เข้าไป ประกอบกันกลายเป็นผลลัพธ์สมการเส้นตรง `Z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}` แล้วก็นำ `Z^{[l]}` นี้ไปป้อนเข้า Activation function `g(Z^{[l]})` ก็จะได้ผลเป็น Activation output ของ Layer นั้น นั่นก็คือ `A^{[l]}`\\n\\nสังเกตว่า Convolution layer นี้ มีคุณสมบัติพิเศษอยู่ 2 ประการ คือ:\\n\\n1) ทั้ง Layer ใช้ค่าน้ำหนัก W ร่วมกัน: กระบวนการ Convolution ใช้ Filter ค่าน้ำหนักตัวเดียวกัน มาคำนวนหา Wa โดยการ Convolve กับ Input ในตำแหน่ง Grid ต่างๆ หลายๆ ครั้งจนครบ ดังนั้น จำนวนของค่าน้ำหนัก w ใน Matrix ค่าน้ำหนัก W จะมีจำนวนน้อยกว่า Neural network แบบทั่วไปมาก ซึ่งนอกจากจะส่งผลให้การคำนวนทำได้เร็วขึ้นมากแล้ว การใช้ค่าน้ำหนักชุดเดิมซ้ำๆ ในหลายๆ ส่วนของภาพจะเป็นการตรวจจับคุณลักษณะบางประการในหลายๆ ส่วนของภาพ\\n\\nตัวอย่างเช่น หาก Filter matrix มีแถวซ้ายเป็นค่าบวก แถวขวาเป็นค่าลบ ดังนี้:\\n```tex\\nf =\\n\\\\begin{pmatrix}\\n    1 & 0 & -1 \\\\\\\\\\n    1 & 0 & -1 \\\\\\\\\\n    1 & 0 & -1\\n\\\\end{pmatrix} \\\\tag{1}\\n```\\nถ้าเรานำ Filter นี้ไป Convolve กับ Input matrix ที่ด้านซ้ายกับขวาสว่างไม่เท่ากัน เช่น:\\n```tex\\nA =\\n\\\\begin{pmatrix}\\n    10 & 10 & 10 & 0 & 0 & 0\\\\\\\\\\n    10 & 10 & 10 & 0 & 0 & 0\\\\\\\\\\n    10 & 10 & 10 & 0 & 0 & 0\\\\\\\\\\n    10 & 10 & 10 & 0 & 0 & 0\\\\\\\\\\n    10 & 10 & 10 & 0 & 0 & 0\\\\\\\\\\n    10 & 10 & 10 & 0 & 0 & 0\\n\\\\end{pmatrix} \\\\tag{2}\\n```\\nจะส่งผลให้ Linear function ผลลัพธ์ มีค่ามากตรงแถวกลางๆ ดังนี้:\\n```tex\\nA * f =\\n\\\\begin{pmatrix}\\n    0 & 30 & 30 & 0\\\\\\\\\\n    0 & 30 & 30 & 0\\\\\\\\\\n    0 & 30 & 30 & 0\\\\\\\\\\n    0 & 30 & 30 & 0\\n\\\\end{pmatrix} \\\\tag{3}\\n```\\nค่ามากหมายถึงมีความสว่างมาก นั่นแปลว่า Filter นี้ใช้ Detect ส่วนของภาพที่เป็นเส้นขอบแนวดิ่งนั่นเอง\\n\\n2) การเชื่อมต่อข้าม Layer แบบ Sparse: ค่าของ Output ในแต่ละ Layer เป็นผลมาจากการคำนวนค่า Input เพียงบางส่วน ซึ่งต่างกับ Neural network ทั่วไปที่มีการเชื่อมต่อแบบ Dense หมายถึงค่าของ Output แต่ละ Neuron มาจากการคำนวน Input ทุก Neuron การเชื่อมต่อแบบ Sparse ส่งผลให้การคำนวนนั้นเร็วขึ้นมาก\\n### Padding\\nกระบวนการ Convolution มักจะทำให้ Matrix ผลลัพธ์มีมิติเล็กลง ซึ่งถ้าหากเราทำ Convolution หลายๆ ชั้น ภาพสุดท้ายที่ออกมาก็จะเล็กลงมาก นอกจากนั้น Convolution ยังมีแนวโน้มที่จะทำให้ข้อมูลที่อยู่ตามขอบของภาพนั้นไม่ถูกนำไปคำนวนอย่างเต็มที่เหมือนข้อมูลที่อยู่ส่วนกลางของภาพ เพราะ Filter มีโอกาสจับข้อมูลตามขอบภาพน้อยกว่าตรงกลางภาพ\\n\\nทางออกต่อปัญหา 2 ข้อนี้ คือการทำ Padding ซึ่งก็คือการขยายขอบของข้อมูล Input ออกทุกด้านเท่าๆ กัน เช่น ถ้า Input มีขนาด 6x6 การทำ Padding p = 1 จะทำให้ Input มีขนาดเป็น 8x8\\n\\nเราสามารถคำนวนได้ว่าเมื่อทำ Padding แล้ว มิติของ Output จะเป็นเท่าใด แต่ก่อนอื่น ลองเปรียบเทียบกับแบบธรรมดา\\n\\n1) \"Valid\" convolution คือไม่มี Padding\\n\\nมิติของ Output คือ:\\n```tex\\n(n-f + 1), (n-f + 1) \\\\tag{4}\\n```\\nโดย n คือมิติของ Input ส่วน f คือมิติของ Filter\\n\\n2) \"Same\" convolution คือการ Pad เพื่อให้มิติของ Output เท่ากับมิติของ Input\\n\\nมิติของ Output คือ:\\n```tex\\n(n + 2p -f + 1), (n + 2p -f + 1) \\\\tag{5}\\n\\np = \\\\frac{f-1}{2} \\\\tag{6}\\n```\\nโดย p คือขนาด Pixel ของ Padding ในแต่ละด้าน.\\n\\nตัวอย่างเช่น ถ้า Filter มีขนาด 5x5 เราจะต้อง Pad เป็นจำนวนดังนี้:\\n```tex\\np = \\\\frac{5-1}{2} = 2 \\\\text{ pixel} \\\\tag{7}\\n```\\nอนึ่ง f ควรจะเป็นเลขคี่ เพื่อให้เกิดความสมมาตรและมีจุดกึ่งกลาง\\n###  Stride\\n\\nStride แปลว่า \"ย่างก้าว\" หมายถึงจำนวนช่องที่จะเลื่อนไปในกระบวนการ Convolution แต่ละครั้ง เช่นถ้าเลื่อนทีละช่อง Stride s = 1 แต่ถ้าเลื่อนทีละ 2 ช่อง ก็คือ s = 2\\n\\nสูตรในการคำนวนขนาด Output เมื่อใช้ Stride คือ:\\n```tex\\n\\\\lfloor\\\\frac{n + 2p -f}{s} + 1\\\\rfloor, \\\\lfloor\\\\frac{n + 2p -f}{s} + 1\\\\rfloor \\\\tag{8}\\n```\\nสังเกตว่าเราจะปัดเศษลง ไม่ใช่ปัดขึ้น\\n### Pooling layer\\n\\nหลังจากที่ข้อมูลผ่าน Convolution layer แล้ว บ่อยครั้งที่จะถูกส่งเข้า Layer อีกแบบหนึ่งที่เรียกว่า Pooling layer\\n\\nหน้าที่ของ Pooling layer คือการสกัดเอาส่วนที่สำคัญที่สุดของข้อมูล และเพิ่มประสิทธิภาพการประมวลผลให้รวดเร็วยิ่งขึ้น กลไกของ Pooling layer นั้นเรียบง่ายมาก คือการสกัดเอาเฉพาะค่าสูงสุดของ Grid เก็บไว้ใน Output เช่น Pooling layer ขนาด 2x2 โดยมีค่า Stride s = 2\\n\\nPooling layer ที่สกัดเอาเฉพาะค่าสูงสุดของ Grid เก็บไว้ เรียกว่า Max pooling ซึ่งเป็นรูปแบบที่ใช้บ่อยที่สุด นอกจากนั้นยังมี Average pooling ซึ่งหาค่าเฉลี่ยของ Grid เก็บไว้ แต่ใช้น้อยกว่า Max pooling มาก\\n### ตัวอย่าง Convolutional Network\\nตอนนี้เรารู้จักองค์ประกอบของ Convolutional neural network แล้ว เราจะจบบทนี้ด้วยการแสดงตัวอย่างว่าองค์ประกอบเหล่านี้ จะถูกนำมาต่อกันเป็น CNN อย่างไร\\n####  Input Layer\\nสมมุติว่าเรามีภาพตั้งต้นขนาด 32x32 pixel โดยเป็นภาพสี RGB นั่นหมายความว่าภาพนี้มี 3 Layer คือแทนสี Red, Green, และ Blue โดยเราต้องการจำแนกภาพนี้ออกเป็น 10 ประเภท:\\n```\\nInput layer a[0]: (32, 32, 3)\\n```\\n**1st Layer**\\n\\nใน Layer แรก เราจะใช้ Convolution layer ขนาด 5x5 โดยมี Stride เท่ากับ 1 และไม่มี Padding โดยจะใช้ Convolution layer บบนี้ทั้งสิ้น 6 Layer นั่นก็คือ `f^{[1]} = 5, s^{[1]} = 1, p^{[1]} = 0, n_C^{[1]} = 6` เราจะได้มิติของ `a^{[1]}` จากการคำนวนตามสมการที่ (4) (n-f + 1) ดังนี้\\n```\\nConvolution layer a[1]: (28, 28, 6)\\n```\\nจากนั้นเราจะใช้ Max pooling ขนาด `f^{[1]} = 2, s^{[1]} = 2` ก็จะได้มิติของ Activation function ของ Layer ที่ 1 จากการคำนวนตามสมการที่ (8) `\\\\lfloor\\\\frac{n + 2p -f}{s} + 1\\\\rfloor` ดังนี้:\\n```\\nMax-pooling layer a[1]: (14, 14, 6)\\n```\\n**2nd Layer**\\n\\nต่อมาใน Layer ที่สอง เราจะใช้ Convolution layer ขนาด 5x5 โดยมี Stride เท่ากับ 1 และไม่มี Padding โดยจะใช้ Convolution layer บบนี้ทั้งสิ้น 16 Layer นั่นก็คือ `f^{[2]} = 5, s^{[2]} = 1, p^{[2]} = 0, n_C^{[2]} = 16` จะได้มิติดังนี้:\\n```\\nConvolution layer a[2]: (10, 10, 16)\\n```\\nจากนั้นเราจะใช้ Max pooling ขนาด `f^{[2]} = 2, s^{[2]} = 2` ก็จะได้มิติของ Output ของ Layer ที่ 2 ดังนี้:\\n```\\nMax-pooling layer a[2]: (5, 5, 16)\\n```\\nเสร็จแล้วเราจะปิดท้าย Layer ที่สอง ด้วยการรวมเอา Output ทั้งหมดมา Activate ใน Dense layer นั่นก็คือ Layer ที่เชื่อมต่อทุก Neuron แบบปกติ ซึ่งจะมีขนาดเท่ากับ Output นั่นก็คือ 5x5x16 = 400 เราเรียก Layer ชนิดนี้ว่า Dense หรือ Flatten layer:\\n```\\nFlatten (dense) layer a[2]: (400)\\n```\\n**3rd Layer**\\n\\nจากนั้นเราจะนำ Dense layer ไปเชื่อมเข้ากับ Dense layer อีกชั้น แต่คราวนี้ลดขนาดลงเป็น 120:\\n```\\nFlatten (dense) layer a[3]: (120)\\n```\\n**4th Layer**\\n\\nแล้วก็นำ Dense layer ไปเชื่อมเข้ากับ Dense layer อีกชั้นหนึ่งซึ่งเล็กลงไปอีก คราวนี้มีขนาด 86:\\n```\\nFlatten (dense) layer a[4]: (86)\\n```\\n**5th Layer**\\n\\nสุดท้ายก็ Output ออกไปยัง Softmax layer ขนาด 10 neuron เพื่อจำแนก Classification ออกเป็น 10 กลุ่ม:\\n```\\nSoftmax layer a[5]: (10)\\n```\\nคงพอจะเห็นภาพว่าเราสามารถนำส่วนประกอบต่างๆ มาประกอบกันเป็น Convolutional neural network ได้ตามตัวอย่าง ซึ่งอันที่จริงเป็นโครงสร้างที่เรียกว่า LeNet-5 ซึ่งถูกตีพิมพ์แผนแพร่เมื่อปี 1998 และถูกนำมาใช้ในการอ่านตัวเลขบนเช็คโดยธนาคารหลายแห่ง ถือเป็นหนึ่งในโครงสร้าง CNN แบบคลาสสิค\\n##  ภาคผนวก 1: ทำไมต้อง Machine Learning\\nย้อนไปในช่วงสามปีที่เรียนม.ปลาย ผู้เขียนใช้เวลาไปกับการอ่านนิยายวิทยาศาสตร์ชุดสถาบันสถาปนา (Foundation Series) ของ Isaac Asimov นิยายชุดนี้เป็นส่วนสำคัญที่ทำให้ผู้เขียนเลือกเรียนเศรษฐศาสตร์ และทำงานด้านนวัตกรรมทางสังคมมาจนถึงทุกวันนี้\\n\\nแกนหลักของพล็อตเรื่องในสถาบันสถาปนา คือการที่มีนักคณิตศาสตร์ผู้หนึ่งได้พัฒนาศาสตร์การพยากรณ์ ที่เรียกว่า Psychohistory และใช้ข้อมูลที่ได้จากการพยากรณ์ด้วยศาสตร์นี้ มาลดระยะเวลาและผลกระทบของวิกฤติที่มนุษยชาติกำลังเผชิญ (ในบริบทว่าในอนาคต มนุษย์ได้แผ่ขยายตั้งรกรากไปในดาวต่างๆ ทั่วทั้งกาแล็กซี่มาแล้วเป็นเวลานับหมื่นปี จนลืมไปแล้วว่ามนุษย์ถือกำเนิดบนดาวเคราะห์โลกดวงนี้)\\n\\nจุดมุ่งหมายของการพยากรณ์นั้นไม่ใช่เป็นไปเพื่อการรู้เหตุการณ์ล่วงหน้าโดยตัวมันของเอง แต่คือการรู้สิ่งที่รู้ได้ แต่ปกติมนุษย์รู้ได้ยากเพราะข้อจำกัดของการรับรู้ การประมวลผล และญาณทัศนะของมนุษย์ ข้อจำกัดเหล่านี้เป็นปัจจัยสำคัญที่เป็นอุปสรรคขวางกั้นไม่ให้มนุษย์สามารถแก้ปัญหาอย่างชาญฉลาด หรือพัฒนาตนเองและสังคมให้ก้าวหน้าขึ้นไปได้ ตั้งแต่อดีตเมื่อมนุษย์เริ่มพัฒนามาจนถึงปัจจุบัน เราอยู่กับข้อจำกัดนี้ โดยปรับตัวให้เข้ากับข้อจำกัดและสร้างอารยธรรมที่อยู่บนพื้นฐานของข้อจำกัดนี้ ดังที่เราเห็นว่าสังคมส่วนมากล้วนให้ค่ากับประสบการณ์และความรู้ฝังลึกที่สะสมในตัวบุคคลและวัฒนธรรม\\n\\nดังนั้น การพัฒนาปัญญาประดิษฐ์ โดยเฉพาะ Machine learning นี้ ผู้เขียนเห็นว่ากำลังจะเป็นปัจจัยสำคัญที่จะผลักดันให้มนุษย์ก้าวเข้าสู่ยุคที่สอง คือยุคที่การเข้าใจและการตัดสินใจนั้นอยู่บนพื้นฐานของความจริง ไม่ใช่ความเห็นหรือการคาดเดา การที่เราสามารถรู้ได้ว่าอะไรจริงและตัดสินใจบนพื้นฐานของความจริง จะปลดปล่อยเราให้เป็นอิสระจากความไม่รู้ ความไม่แน่ใจ ความหลงผิด และในขณะเดียวกัน ก็สามารถใช้ความรู้นี้มาเสริมสร้างคุณลักษณะและกิจกรรมอันเป็นเอกลักษณ์ของมนุษย์ นั่นก็คืออุดมคติ ความคิดสร้างสรรค์ ศิลปะ และการพัฒนาด้านจิตวิญญาณ อันเป็นขอบเขตที่เครื่องจักรยังไม่อาจแทนที่ได้\\n\\n### ความจริงที่ไม่ต้องคาดเดา\\n\\nหากจะพูดกันเฉพาะในประเทศไทย เรามีปัญหาและความท้าทายต่างๆ มากมายที่ดูเหมือนจะใช้เวลามานานก็ยังไม่สามารถแก้ไขหรือป้องกันได้ ยังไม่นับปัญหาใหม่ๆ ที่เกิดขึ้นอย่างรวดเร็ว รุนแรง และกำลังกลายเป็นความเสี่ยงต่อคน สังคม และสิ่งแวดล้อมเป็นอย่างมาก เป้าหมายในการทำงานของผู้เขียนคือการหาทางแก้ไขปัญหาเหล่านี้ด้วยวิธีการใหม่ๆ ดังนั้นจึงพอจะเห็นว่าแนวทางการใช้ Machine ช่วยทำความเข้าใจและตัดสินใจนั้นมีศักยภาพสูงที่จะช่วยแก้ไขปัญหาเหล่านี้ได้\\n\\nยกตัวอย่างเช่น ในเรื่องฝุ่นขนาดเล็ก PM2.5 ที่คนไทยกำลังเผชิญอย่างหนัก ที่ผ่านมาภาครัฐล้มเหลวอย่างสิ้นเชิงที่จะแก้ปัญหา ซึ่งสาเหตุหนึ่งของความล้มเหลวคือการขาดความรู้และข้อเท็จจริงว่า ฝุ่นละอองขนาดเล็กที่เกิดขึ้น มาจากปัจจัยใดมากน้อย อย่างไรก็ตาม วิทยาศาสตร์ข้อมูลและ Machine learning ได้ให้คำตอบต่อเรื่องนี้อย่างชัดแจ้ง ว่า[การเผาในที่โล่งนั้นเป็นปัจจัยที่มีส่วนเกี่ยวข้องมากที่สุดต่อปัญหาฝุ่นละอองขนาดเล็ก](https://towardsdatascience.com/identifying-the-sources-of-winter-air-pollution-in-bangkok-part-ii-72539f9b767a#--responses) สิ่งที่เหลือที่ต้องทำ คือการนำเอาข้อเท็จจริงนี้มาปฏิบัติให้เป็นนโยบายโดยผู้มีส่วนเกี่ยวข้อง (ซึ่งทำได้หรือไม่นั้นเป็นอีกปัญหาหนึ่ง)\\n\\nหรือในเรื่องการดูแลและส่งเสริมพัฒนาการเด็กปฐมวัย ปัจจุบันเริ่มมีความตื่นตัวในการเสริมสร้างพัฒนาการเด็กตั้งแต่ยังเล็ก โดยมีหลักฐานและการวิจัยมากมายที่บ่งชี้ว่าการแทรงแซง (Intervention) ที่เหมาะสมจะทำให้เด็กเติบโตมามีความสามารถในการเรียนรู้และมีความสุขในระยะยาวมากกว่าการไม่ทำอะไรเลย ปัญหาอยู่ที่ว่า หน่วยงานที่มีหน้าที่ส่งเสริมพัฒนาการเด็ก เช่นศูนย์พัฒนาเด็กเล็กที่กระจายอยู่ตามองค์การบริหารส่วนตำบลทั่วประเทศ ไม่สามารถรู้ได้อย่างทันท่วงทีว่าเด็กคนไหนจำเป็นต้องเสริมพัฒนาในด้านใด หรือกว่าจะรู้ก็ต้องใช้เวลาหลายเดือนหรือเป็นปี เป็นการสูญเสียนาทีทองของพัฒนาการไปอย่างน่าเสียดาย ในเรื่องนี้ Machine learning สามารถเข้ามาช่วยพยากรณ์ได้ว่าเด็กคนไหนมีพัฒนาการด้อยด้านใด และจำเป็นแค่ไหนที่จะต้องเสริมในด้านต่างๆ โดยสามารถรู้ได้ตั้งแต่วันแรกที่รับเด็กเข้ามาในศูนย์ฯ จากการเก็บข้อมูลและหาความสัมพันธ์ระหว่างปัจจัยต่างๆ ในเด็กแต่ละคน เช่นสุขภาพ การเลี้ยงดู สภาพครอบครัว ฯลฯ\\n\\nส่วนในเรื่องสุขภาพจิต ปัจจุบันมีผู้มีปัญหาด้านจิตใจที่ต้องการคำปรึกษาเป็นจำนวนมาก และหลายคนก็มีปัญหาหนักที่เป็นความเสี่ยงในการฆ่าตัวตาย จำนวนผู้ต้องการคำปรึกษาที่มากมาย ทำให้บริการให้คำปรึกษาต่างๆ นั้นไม่สามารถให้บริการได้ทัน ทำให้ผู้ที่มีปัญหาจำนวนมากไม่ได้รับการปรึกษา หรือได้รับช้าเกินไป ในเรื่องนี้ เราสามารถใช้เทคโนโลยีอย่างแชทบอท ผนวกกับ Machine learning ในการตรวจหาได้ว่าผู้ขอรับคำปรึกษาคนไหนมีความเสี่ยงในการฆ่าตัวตาย เพื่อที่จะส่งเคสดังกล่าวให้ผู้ให้คำปรึกษาและช่วยเหลือที่เป็นมนุษย์มาดูแลได้ทันที เป็นเครื่องมือในการคัดกรองความเสี่ยงตามหลักการแพทย์\\n\\nและด้วยเทคนิคเดียวกัน เราสามารถตรวจหาสิ่งผิดปกติจากฐานข้อมูล Open data เพื่อตรวจหาการทุจริตคอร์รัปชันประเภทต่างๆ เช่นการจัดซื้อจัดจ้างภาครัฐ โดยการออกแบบกลไกการตีความข้อมูลที่เป็นข้อมูลสาธารณะอยู่แล้ว\\n### Machine learning สำหรับทุกคน\\n\\nเช่นเดียวกับที่คนส่วนมากในยุคปัจจุบันสามารถใช้คอมพิวเตอร์และอุปกรณ์ต่างๆ เพื่อทำงานสำนักงาน ค้นหาข้อมูล และสื่อสารกันได้ ผู้เขียนมีความเห็นว่าคนในอนาคตควรจะมีความเข้าใจและความสามารถเบื้องต้นในการทำให้ Machine นั้นเรียนรู้และให้คำตอบที่เป็นข้อเท็จจริงแก่เรา ทฤษฎี แนวคิด และการปฏิบัติเรื่องวิทยาศาสตร์ข้อมูลและ Machine learning อาจดูเข้าถึงได้ยาก เข้าใจยาก และทำได้ยากสำหรับคนทั่วไปในปัจจุบัน แต่ก็เหมือนกับที่ยุคหนึ่ง คนทั่วไปรู้สึกว่าอินเทอร์เน็ตเป็นพื้นที่ลึกลับ เข้าถึงได้ยาก หรือย้อนไปอีกในยุคที่การศึกษาในระบบยังไม่แพร่หลาย คนส่วนมากก็รู้สึกว่าการคำนวนทศนิยม เศษส่วน อัตราเร็ว เป็นเรื่องยากเกินเข้าใจ สิ่งที่ยากในยุคหนึ่ง หากได้รับการพิสูจน์ว่ามีประโยชน์อย่างกว้างขวาง ก็จะพัฒนากลายเป็นสิ่งที่เข้าใจและปฏิบัติกันทั่วไปในยุคต่อมา\\n\\nการสร้างความเข้าใจเรื่องปัญญาประดิษฐ์ วิทยาศาสตร์ข้อมูล และ Machine learning สำหรับคนทั่วไป มีการดำเนินการแล้วในประเทศฟินแลนด์ โดยเปิด[คอร์สออนไลน์ฟรีสำหรับทุกคน](https://www.elementsofai.com/) ซึ่งผู้จัดทำมีเป้าหมายให้ประชาชนฟินแลนด์ 1% ได้เรียนคอร์สนี้\\n\\nหนึ่งในปัจจัยสำคัญที่ทำให้คนทั่วไปเรียนรู้ Machine learning ได้ง่ายขึ้น คือความแพร่หลายของภาษาโปรแกรมระดับ High-level ที่เข้าใจง่าย ใช้ง่าย อย่าง Python และ Library ต่างๆ ที่เป็น Framework สำหรับการสร้าง Machine learning system เช่น scikit-learn และ TensorFlow ที่ได้จัดการเรื่องการวางสูตรคำนวนที่ซับซ้อนและละเอียดอ่อนให้เราแทบทั้งหมด ผู้เขียนเองถึงแม้จะมีความรู้พื้นฐานและประสบการณ์ด้านคอมพิวเตอร์มาบ้าง แต่ไม่เคยได้ศึกษาการเขียนโค้ดจริงๆ จังๆ ก็สามารถใช้เวลาเพียงแค่ไม่ถึง 6 เดือนในการเรียนรู้ภาษา Python และ Machine learning จนมีความรู้และความมั่นใจในการนำมาประยุกต์ใช้\\n\\nแต่เมื่อเครื่องมือนั้นพร้อมและใช้งานง่ายขึ้น ก็เป็นหน้าที่ของเราที่จะต้องมีความเข้าใจในหลักการและเทคนิคต่างๆ เพื่อให้สามารถเลือกใช้เครื่องมือต่างๆ นั้นได้เหมาะสมกับโจทย์และเป้าหมายที่ต้องการ นี่เป็นที่มาและแรงบันดาลใจให้ผู้เขียนได้พยายามถ่ายทอดหลักการของ Machine learning อย่างค่อนข้างลึกซึ้ง ประกอบกับการแนะนำวิธีการในการปฏิบัติจริง เพื่อช่วยให้ผู้อ่านสามารถเริ่มต้นเส้นทางในการเรียนรู้เรื่องนี้ได้ และในขณะเดียวกัน ก็เป็นการฝึกทบทวนความเข้าใจของผู้เขียนเอง จากสิ่งที่ได้ศึกษาและปฏิบัติมา ให้แม่นยำชัดเจนขึ้นด้วย\\n\\n## ภาคผนวก 2: พื้นฐาน Python อย่างสั้นที่สุด\\nบทนี้จะแนะนำ Python สำหรับผู้ที่ไม่มีพื้นฐานการเขียนโค้ดเลย โดยจะอธิบายเฉพาะเรื่องพื้นฐานที่สำคัญจริงๆ อย่างสั้นและกระชับที่สุด เพื่อให้สามารถเริ่มต้นได้และสามารถใช้ Machine learning framework เช่น scikit-learn และ TensorFlow\\n\\nขอย้ำอีกรอบ ว่าสิ่งที่แนะนำนี้ไม่ได้ครบถ้วนสมบูรณ์ตามหลักวิชาการ แต่เน้นกระชับ สั้น ง่าย ให้เริ่มต้นเรียนรู้ได้เท่านั้น\\n\\nPython เป็นภาษาคอมพิวเตอร์แบบ High-level นั่นหมายความว่าเราสามารถสั่งให้ Python ทำสิ่งต่างๆ ได้ในลักษณะที่คล้ายกับภาษาคน โดยไม่ต้องไปสนใจเรื่อง \"หลังบ้าน\" ของคอมพิวเตอร์มากนัก เช่นการจัดการหน่วยความจำ การระบุประเภทตัวแปร เป็นต้น ทำให้เขียนง่าย อ่านง่าย เข้าใจง่าย\\n\\nนอกจากนั้น Python ยังเป็นภาษาแบบ Interpret หมายความว่าเวลาเราเรียก Script หรือโปรแกรมที่เขียนด้วย Python เครื่องก็จะตีความ คำนวน และแสดงผลทันที โดยไม่ต้องสั่งแปลงเป็นภาษาเครื่องก่อนจะรันโปรแกรม (เรียกว่าการ Compile) ทำให้สะดวกรวดเร็วในการพัฒนาโปรแกรม แต่แลกมากับการที่ความเร็วและประสิทธิภาพในการคำนวนจะด้อยกว่าภาษาที่ต้อง Compile เช่น C++ เป็นต้น\\n\\nปัจจุบัน Python เป็นภาษา \"มาตรฐาน\" สำหรับงานวิทยาศาสตร์ข้อมูล, AI, Machine learning นอกจากนั้นยังใช้ในอีกหลายวงการ เช่นการสร้างเว็บไซต์ ผ่าน Framework เช่น Django และ Flask เป็นต้น\\n\\n### การติดตั้งและใช้งาน Python\\nขั้นแรกให้โหลด Python มาติดตั้งก่อน โดยเข้าไปที่[เว็บไซต์ของ Python](https://www.python.org/) ไปที่หน้า Downloads และทำตามคำแนะนำได้เลย\\n\\nอย่างไรก็ตาม ถ้าจะใช้ Python ในงานวิทยาศาสตร์ข้อมูล, AI, Machine learning แนะนำวิธีติดตั้งอีกแบบ คือการติดตั้ง Python และ Library มาตรฐานที่มักจะได้ใช้แบบรวดเดียวจบ ผ่าน [Anaconda](https://www.anaconda.com/distribution/) ก็จะได้ Library พื้นฐาน เช่น numpy, pandas, matplotlib มาเลย ส่วน scikit-learn และ TensorFlow ก็สามารถติดตั้งได้ง่ายๆ โดยใช้คำสั่ง `conda` ใน Commandline เช่น `conda install scikit-learn` และ `conda install tensorflow` เป็นต้น รายละเอียดเรื่องการติดตั้งและจัดการ Package จะไม่อธิบายมาก สามารถอ่านและศึกษาได้โดยตรง\\n\\nการเขียนโค้ดและเรียกโปรแกรมที่เขียนด้วย Python มีหลายวิธี ดังนี้\\n\\n* iPython: เรียก `ipython` จาก Commandline จะสามารถเขียนโค้ดและรันทีละบรรทัดๆ เหมาะสำหรับการทำลองไอเดียสั้นๆ เร็วๆ หรือใช้ Python เป็นเครื่องคิดเลข\\n* IDE: ใช้ Python ผ่านโปรแกรมที่เป็นสภาพแวดล้อมสำหรับการเขียนโค้ดโดยเฉพาะ เช่น PyCharm หรือ Spyder โดยสามารถติดตั้งและเรียกใช้ IDE เหล่านี้ได้เอง\\n* Jupyter Lab: ใช้ Python ผ่าน Jupyter Notebook (ปัจจุบันพัฒนาเป็นรุ่นใหม่ เรียกว่า Jupyter Lab) โดยการเรียก `jupyter lab` จาก Commandline จะเปิดเว็บขึ้นมาในลักษณะ Web-based IDE นั่นเอง สำหรับงาน Data science/ML แนะนำวิธีนี้ เพราะสามารถแยกโค้ดเป็น Cell ย่อยๆ และสามารถใส่ Cell ที่เป็นข้อความธรรมดาเพื่ออธิบายโค้ดของเราควบคู่ไปได้\\n### การคำนวน\\nเมื่อเรียกใช้ Python ผ่านช่องทางที่ถนัดแล้ว ก็มาเริ่มกันเลย\\n\\nการคำนวนและการแสดงผล ทำได้ง่ายมาก เช่น:\\n```\\n2*0.5/3\\n```\\n\\nจะได้คำตอบเป็น `0.3333333333333333` สังเกตว่าถ้าเราคำนวนหลายๆ อย่างใน Term เดียว เราอาจจะไม่แน่ใจว่า Python จะคำนวนสัญลักษณ์ไหนก่อน ซึ่งจริงๆ มีลำดับที่ชัดเจน แต่ถ้าไม่อยากจำ แนะนำว่าให้ใส่วงเล็บให้ชัดเจนไปเลย เช่น:\\n```\\n(2*0.5)/3\\n ```   \\n\\nนอกจากบวก `+` ลบ `-` คูณ `*` หาร `/` ยังมี Operator ที่น่ารู้ เช่น ยกกำลัง `**` Floor division `//` (หารไม่เอาเศษ) และ Modulus `%` (หารแล้วเอาแค่เศษ) สามารถดูเพิ่มเติมได้จากเว็บต่างๆ เช่น[ที่นี่](https://www.w3schools.com/python/python_operators.asp)\\n### Variables: การกำหนดตัวแปร\\nเวลาเราเขียนโค้ด เรามักจะต้องการเก็บข้อมูลบางอย่างเอาไว้ก่อนเพื่อเรียกใช้ภายหลัง เราเรียกที่เก็บข้อมูลนี้ว่าตัวแปร หรีอ Variable\\n\\nตัวอย่างเช่น:\\n```python\\nage = 50\\nname = \"John\"\\nprint(age)\\nprint(name)\\n```   \\n\\nจะได้คำตอบคือ:\\n```\\n    50\\n    John\\n```\\n\\nสังเกตว่า ค่าของตัวแปรที่เป็นตัวเลข สามารถใส่ได้เลย ส่วนถ้าเป็นตัวหนังสือ ต้องใช้ Quotation ครอบไว้ จะใช้ `\"` หรือ `\\'` ก็ได้\\n\\nเราใช้ฟังก์ชัน `print()` เพื่อแสดงผลสิ่งที่อยู่ในวงเล็บ เราเรียกสิ่งที่อยู่ในวงเล็บว่า Argument โดยในที่นี้เราจะใส่ตัวแปรของเราเป็น Argument ทำให้ Python print ค่าของตัวแปรของเราออกมา อนึ่ง Argument อาจมีได้หลายอัน ขึ้นอยู่กับว่า Function นั้นรองรับ Argument อะไรบ้าง โดยเราใช้ Comma `,` คั่นระหว่างแต่ละ Argument ซึ่งเดี๋ยวเราจะเห็นตัวอย่างเมื่ออธิบาย Function อื่นๆ ต่อไป\\n\\nทีนี้เราจะลองใช้ประโยชน์จากตัวแปร โดยเราต้องการที่จะบอกว่า John อายุ 50 ปี:\\n```python\\nprint(name + \\' is \\' + str(age) + \\' years old.\\')\\n``` \\n\\nได้คำตอบว่า:\\n```\\n    John is 50 years old.\\n```\\n\\nลองมาดูกันว่าเกิดอะไรขึ้นบ้าง:\\n\\n* เครื่องหมาย `+` ในที่นี้ คือการเอาข้อความ (เรียกว่า String) มาต่อกัน (เรียกการกระทำแบบนี้ว่า Concatenate)\\n* การ Concatenate จะต้องทำระหว่างข้อมูลประเภทเดียวกันเท่านั้น เช่น String กับ String ดังนั้น เราจึงต้องแปลงตัวแปร `age` จากตัวเลขจำนวนเต็ม (Integer หรือ int) ให้เป็นข้อความ (String หรือ str) ด้วยการเรียกฟังก์ชัน `str()`\\n* สังเกตว่าเราเว้นวรรคบางจุด เช่น ก่อนและหลัง `is` และก่อน `years old` เพื่อให้ข้อความที่ออกมามีการเว้นวรรคที่ถูกต้อง เพราะการ Concatenate ไม่ได้เว้นวรรคให้เรา\\n\\nทีนี้ถ้าเรากำหนดตัวแปรใหม่ เช่น `age = 35` แล้วเรียก `print` เหมือนเดิมอีกรอบ Output ที่ได้ก็จะเปลี่ยนตามตัวแปรใหม่ นั่นก็คือ `John is 35 years old.`\\n### Data types: ประเภทของข้อมูล\\nตัวแปรสามารถเก็บข้อมูลได้หลายประเภท เราจะทำความรู้จักประเภทข้อมูลพื้นฐานที่สำคัญและใช้บ่อยดังนี้:\\n\\n* Text `str`\\n* Numerical `int`, `float`\\n* Sequence `list`, `tuple`\\n* Mapping `dict`\\n* Boolean `bool`\\n\\n#### String\\n\\nเรารู้จัก String กันแล้วก่อนหน้านี้ String มีคุณสมบัติที่น่าสนใจบางอย่าง เช่น:\\n\\n1) String เป็น Array หมายความว่าตัวอักษรแต่ละตัวจะถูกมองว่าเป็นข้อมูล 1 ชิ้น ดังนั้นเราจึงสามารถจัดการตัวอักษรใน String ได้หลากหลาย เช่น:\\n\\n**หาความยาว** โดยใช้ฟังก์ชัน len()\\n```python\\na = \"Hello, World!\"\\nprint(len(a))\\n```\\nได้คำตอบคือ `13`\\n\\n**ตัดเอา Space ว่างท้าย String ออก** โดยใช้ Method `.strip()` (Method คือฟังก์ชันที่ใช้ได้กับตัวแปร โดยการนำไปต่อท้ายตัวแปรได้ทันที เพื่อกระทำการบางอย่างกับข้อมูลในตัวแปรนั้น ลองดูตัวอย่างข้างล่าง)\\n```python\\na = \" Hello, World! \"\\nprint(a.strip())\\n```\\nได้คำตอบว่า `Hello, World!` โดยไม่มี Space หลัง `!`\\n\\n**แยกคำออกจากกัน** โดยใช้ Method `.split()`\\n```python\\na = \"My name is John.\"\\nprint(a.split())\\n```\\nจะเป็นการแยกประโยคออกเป็นคำๆ โดยแยกที่ Space ได้ผลคือ `[\\'My\\', \\'name\\', \\'is\\', \\'John.\\']` ทั้งนี้สามารถกำหนดได้ว่าให้แยกที่พบอักษรอะไร เช่น `a.split(\",\")` คือให้แยกเมื่อพบ Comma เป็นต้น\\n\\nสังเกตว่า `[\\'My\\', \\'name\\', \\'is\\', \\'John.\\']` เป็นข้อมูลประเภท List ซึ่งจะอธิบายภายหลังว่าคืออะไร ตอนนี้เพียงให้รู้ว่า List นี้มีข้อมูล 4 รายการ คือคำแต่ละคำนั่นเอง คั่นด้วย Comma\\n\\n**เลือกเฉพาะบางตัวอักษรตามลำดับที่** เป็นความสามารถที่เรียกว่า Slicing ซึ่งใช้ได้กับทุกอย่างที่มีลักษณะเป็น Array หลักการคือ Python จะกำหนดให้ข้อมูลชิ้นแรกมีรหัส Index 0 และเพิ่มขึ้นทีละ 1 ต่อไปเรื่อยๆ ดังนั้น:\\n```python\\na = \"Hello, World!\"\\nprint(a[1])\\n```\\nจะได้ `e`\\n\\nเราสามารถเลือก Slicing เป็น Range ได้ โดยใช้ Colon `:` คั่น โดยจะนับ Index เริ่มต้นเป็นชิ้นแรก (Inclusive) แต่ Index ตัวขบขะไม่นับตัวมันเอง (Exclusive) ตัวอย่างเช่น:\\n```python\\na = \"Hello, World!\"\\nprint(a[2:5])\\n```\\nจะได้ `llo`\\n\\nสุดท้าย ถ้าใน String มีเครื่องหมายที่ปกติจะถูกตีความว่ามีความหมายพิเศษ เช่น `\"` เราจะไม่สามารถใส่เครื่องหมายนั้นลงไปได้ตรงๆ ถ้าอยากใส่ ต้องใช้ Backslash `\\\\` ไว้ก่อน เราเรียก `\\\\` ว่า **Escape character** ตัวอย่างเช่น:\\n```python\\nprint(\"My name is \\\\\"John\\\\\".\")\\n```\\nจะได้ `My name is \"John\".`\\n\\nแถม Escape character ที่มีประโยชน์มาก คือ `\\\\n` คือการขึ้นบรรทัดใหม่ ส่วน `\\\\t` คือ Tab\\n\\nอ่านเพิ่มเติมเกี่ยวกับ String ได้[ที่นี่](https://www.w3schools.com/Python/python_strings.asp)\\n#### Int และ Float\\n\\nInt (Integer) คือจำนวนเต็ม เช่น -1, 0, 5, 100 ส่วน Float คือจำนวนที่มีทศนิยม เช่น -0.12, 5.816497 เป็นต้น เราสามารถดูได้ว่าตัวเลขของเราคือประเภทอะไร โดยใช้ฟังก์ชัน `type()` เช่น:\\n```python\\nx = 1\\ny = 2.5\\n\\nprint(type(x))\\nprint(type(y))\\n```\\nจะได้:\\n```\\n<class \\'int\\'>\\n<class \\'float\\'>\\n```\\nเราสามารถแปลงระหว่าง `int` กับ `float` ได้ โดยใช้ฟังก์ชัน `int()` และ `float()`\\n#### List\\nList เป็นข้อมูลแบบ Array ที่มีสมาชิกหลายตัว สามารถผสมสมาชิกประเภทต่างๆ กันได้ และสามารถสลับตำแหน่ง เปลี่ยนแปลงข้อมูลได้ จึงใช้งานได้หลายวัตถุประสงค์ยืดหยุ่นมาก เราสามารถสร้าง List โดยการใช้ Bracket [] เช่น:\\n\\n```python\\nemptylist = []\\nprint(emptylist)\\n```\\n\\nก็จะได้ List ว่างๆ คือ [] ส่วน List ที่มีสมาชิก ก็ใส่สมาชิกนั้นลงไปเลย คั่นแต่ละตัวด้วย Comma เช่น:\\n\\n```python\\nfruitlist = [\"apple\", \"banana\", \"cherry\"]\\nprint(fruitlist)\\n```\\n\\nก็จะได้ `[\\'apple\\', \\'banana\\', \\'cherry\\']`\\n\\nสมมุติเราอยากเลือกแค่ banana กับ cherry ก็ใช้ Slicing ได้ เช่น:\\n\\n```python\\nfruitlist = [\"apple\", \"banana\", \"cherry\"]\\nprint(fruitlist[1:])\\n```\\n\\nก็จะได้ `[\\'banana\\', \\'cherry\\']` สังเกตว่าตอน Slice ถ้าเราไม่ใส่ Index ตัวจบ ก็แปลว่าให้เลือกไปจนถึงสมาชิกตัวสุดท้าย โดยรวมตัวสุดท้ายด้วย ในทางกลับกัน ถ้าไม่ใส่ Index ตัวเริ่ม ก็คือให้เริ่มที่สมาชิกตัวแรกเลย\\n\\nเราสามารถเปลี่ยนสมาชิกใน List ง่ายๆ เช่น:\\n\\n```python\\nfruitlist = [\"apple\", \"banana\", \"cherry\"]\\nfruitlist[1] = \"pineapple\"\\nprint(fruitlist)\\n```\\n\\nจะได้ `[\\'apple\\', \\'pineapple\\', \\'cherry\\']` ทันที\\n\\nส่วนถ้าจะเพิ่มสมาชิก ก็ทำได้โดยใช้ Method .append() เช่น:\\n\\n```python\\nfruitlist = [\"apple\", \"banana\", \"cherry\"]\\nfruitlist.append(\"orange\")\\nprint(fruitlist)\\n```\\n\\nจะได้ `[\\'apple\\', \\'banana\\', \\'cherry\\', \\'orange\\']`\\n\\nนอกจากนี้ยังมีการแทรกสมาชิก ใช้ `.insert(index, object)` โดยใส่ Index ที่ต้องการแทรกลงไปเป็น Argument ก่อนจะใส่ Object ที่ต้องการแทรก ส่วนการลบสมาชิก ก็สามารถใช้ `.remove(object)` ซึ่งใส่ชื่อ Object เป็น Argument หรือไม่ก็ `.pop(index)` ซึ่งใช้หมายเลข Index เป็น Argument (ถ้าไม่ใส่จะหมายถึงให้เอารายการสุดท้ายออก)\\n\\nถ้าต้องการลบสมาชิกทั้งหมดใน List ให้เหลือ List ว่างๆ ก็ใช้ `.clear()`\\n\\nสุดท้าย เราสามารถเอา List มาต่อกันได้ โดยการใช้เครื่องหมาย + เช่น:\\n\\n```python\\nlist1 = [\"a\", \"b\" , \"c\"]\\nlist2 = [1, 2, 3]\\n\\nlist3 = list1 + list2\\nprint(list3)\\n```\\n\\nจะได้ `[\\'a\\', \\'b\\', \\'c\\', 1, 2, 3]`\\n\\nสุดท้าย เราสามารถนับจำนวนสมาชิกใน List ได้ โดยใช้ฟังก์ชัน len() เช่นเดียวกับการนับจำนวนตัวอักษรใน String\\n\\nList จะมีประโยชน์มากเมื่อใช้คู่กับ For loop ซึ่งจะอธิบายในส่วนถัดๆ ไป\\n\\nสามารถอ่านเรื่อง List เพิ่มเติมได้[ที่นี่](https://www.w3schools.com/Python/python_lists.asp)\\n#### Tuple\\nTuple เป็นข้อมูลแบบ Array ที่มีสมาชิกหลายตัวคล้ายกับ List ต่างกันที่ Tuple นั้นไม่สามารถเปลี่ยนแปลงข้อมูลข้างในได้ เราสามารถสร้าง Tuple โดยใช้วงเล็บ () เช่น:\\n\\n```python\\nemptytuple = ()\\nprint(emptytuple)\\n```\\n\\nก็จะได้ List ว่างๆ คือ () ส่วนคุณสมบัติอื่นๆ ก็เหมือนกับ List ต่างตรงที่ไม่สามารถเพิ่ม ลบ สิ่งที่อยู่ใน Tuple ได้\\n\\nอ่านเพิ่มเกี่ยวกับ Tuple ได้[ที่นี่](https://www.w3schools.com/Python/python_tuples.asp)\\n#### Dictionary\\nDictionary เป็นประเภทข้อมูลที่เก็บข้อมูลแต่ละตัวเป็นคู่ ซึ่งประกอบด้วยรหัส (Key) กับค่า (Value) เรียกว่า Key-value pair เราสามารถสร้าง Dictionary ด้วยการใช้วงเล็บปีกนก `{}` ตัวอย่างเช่น:\\n```python\\nprofiledict = {\\n    \"name\": \"John\",\\n    \"age\": 50,\\n    \"sex\": \"male\"\\n}\\n\\nprint(profiledict)\\n```\\nจะได้ `{\\'name\\': \\'John\\', \\'age\\': 50, \\'sex\\': \\'male\\'}` สังเกตว่าข้อมูลแต่ละคู่จะใช้ Comma แบ่ง\\n\\nเราสามารถ**เรียก Value** ของ Key-value pair โดยการเรียก Key ที่ต้องการ โดยเรียกใน Bracket เช่น:\\n```python\\nprint(profiledict[\"name\"])\\n```\\nก็จะได้ Value ของ Key `\"name\"` นั่นก็คือ `\"John\"`\\n\\nอีกวิธีในการเรียก Value คือการใช้ Method `get(key)` ก็ได้ผลเหมือนกัน\\n\\nเราสามารถ**เปลี่ยน Value** ได้ โดยการเรียก Key แล้วกำหนด Value ใหม่โดยใช้เครื่องหมาย `=` เช่น:\\n```python\\nprofiledict = {\\n    \"name\": \"John\",\\n    \"age\": 50,\\n    \"sex\": \"male\"\\n}\\n\\nprofiledict[\"age\"] = 35\\n\\nprint(profiledict)\\n```\\nก็จะได้ `{\\'name\\': \\'John\\', \\'age\\': 35, \\'sex\\': \\'male\\'}` ตามที่ต้องการ\\n\\nการ**เพิ่ม Key-value pair** ใหม่ใน Dict ให้ทำเหมือนกับการเปลี่ยน Value โดยกำหนด Key และ Value ตามที่ต้องการได้เลย เช่น:\\n```python\\nprofiledict = {\\n    \"name\": \"John\",\\n    \"age\": 50,\\n    \"sex\": \"male\"\\n}\\n\\nprofiledict[\"address\"] = \"Bangkok\"\\n\\nprint(profiledict)\\n```\\nจะได้ `{\\'name\\': \\'John\\', \\'age\\': 50, \\'sex\\': \\'male\\', \\'address\\': \\'Bangkok\\'}`\\n\\nส่วนการ**ลบ Key-value pair** ก็ใช้ Method `.pop(key)`\\n\\nการ Loop สมาชิกใน Dictionary โดยปกติจะได้ Key ออกมา แต่ถ้าอยากได้ Value ก็สามารถทำได้ โดยจะอธิบายในส่วน For loop\\n\\nอ่านเพิ่มเกี่ยวกับ Dictionary ได้[ที่นี่](https://www.w3schools.com/Python/python_dictionaries.asp)\\n#### Boolean\\nBoolean คือข้อมูลที่มีสองค่าเท่านั้น คือ True หรือ False โดย Python จะพิจารณาจาก Expression ที่ใส่ เช่น:\\n\\n```python\\nprint(10 > 9)\\nprint(10 == 9)\\nprint(10 < 9)\\n```\\n\\nคำตอบคือ:\\n\\n```\\nTrue\\nFalse\\nFalse\\n```\\n\\nสังเกตว่า เวลาเปรียบเทียบความเท่ากัน ใช้ == ไม่ใช่ = เพราะ = เป็นเครื่องหมายสำหรับการกำหนดค่าให้กับตัวแปร (Assignment) ไม่ใช่การเปรียบเทียบว่าจริงหรือไม่ (Validation)\\n\\nสำหรับรายละเอียดอื่นๆ อ่านเพิ่มได้[ที่นี่](https://www.w3schools.com/Python/python_booleans.asp)\\n\\n### Conditions: การกำหนดเงื่อนไข\\n\\nบ่อยครั้งที่เราต้องการให้โค้ดของเรามีความฉลาด เช่นสามารถทำสิ่งหนึ่งภายใต้เงื่อนไขบางอย่าง หรือให้ทำสิ่งเดิมวนซ้ำๆ จนกระทั่งบรรลุถึงเงื่อนไขที่ต้องการ ความสามารถแบบนี้เรียกว่า Conditions หรือบางทีก็เรียกว่า Flow control เป็นหัวใจของการเขียนโค้ด ควรจะเรียนรู้ไว้ให้คล่อง\\n\\nเราจะทำความรู้จัก Conditions 3 รูปแบบ คือ:\\n\\n```\\nIf...Else\\nFor loop\\nWhile loop\\n```\\n\\n**If...Else**\\n\\nเราสามารถกำหนดให้โปรแกรม เรียกโค้ดบรรทัดหนึ่งๆ ก็ต่อเมื่อเงื่อนไขบางอย่างเป็นจริง\\n\\nIf: การกำหนดเงื่อนไข ทำได้โดยการเขียน if เว้นวรรค ตามด้วย Statement ที่เป็นเงื่อนไข ตามด้วย Colon : เช่น:\\n\\n```python\\na = 10\\nb = 25\\n\\nif b > a:\\n  print(\"b is greater than a\")\\n```\\n\\nเนื่องจากในกรณีนี้ b มากกว่า a ตามเงื่อนไขใน if (statement): Python ก็จะเรียกโค้ดที่อยู่บรรทัดถัดไป โดยจะเรียกเฉพาะบรรทัดที่ถูก Indent เท่านั้น การ Indent ทำได้โดยการกด Tab หรือเคาะ Space 2 หรือ 4 ที\\n\\nElif: ถ้าเราต้องการสร้างเงื่อนไขต่อไป ในกรณีที่เงื่อนไขแรกไม่เป็นจริง ก็ใช้ elif ซึ่งย่อมาจาก Else if เช่น:\\n\\n```python\\na = 25\\nb = 25\\n\\nif b > a:\\n    print(\"b is greater than a\")\\nelif b == a:\\n    print(\"b equals a\")\\n```\\n\\nElse: แต่ถ้าเราต้องการเพียงแค่จะดักเงื่อนไขที่ไม่เป็นจริงจาก If ด้านบน ก็เพียงใช้ else ดังนี้:\\n\\n```python\\na = 50\\nb = 25\\n\\nif b > a:\\n    print(\"b is greater than a\")\\nelse:\\n    print(\"b is not greater than a\")\\n```\\n\\nAnd/Or: ใน Statement เราสามารถใช้ Operator and หรือ or เชื่อม 2 Statement เข้าด้วยกัน เพื่อสร้างเงื่อนไขที่เจาะจงขึ้นได้ โดย and แปลว่าจะต้องเป็นจริงทั้ง 2 Statement ส่วน or เป็นจริง Statement เดียวก็ถือว่าจริง\\n\\n**For loop**\\n\\nFor loop ใช้สำหรับการวนโค้ดซ้ำๆ ตามสมาชิกของข้อมูลแบบ Text, Sequence, และ Mapping นั่นก็คือ str, list, tuple, และ dict สำหรับผู้เริ่มต้นอาจจะงง จึงให้ลองดูตัวอย่างดังนี้:\\n\\n```python\\nfor i in \"banana\":\\n    print(i)\\n```\\n\\nจะได้ Output ว่า:\\n\\n```\\nb\\na\\nn\\na\\nn\\na\\n```\\n\\nหลักการทำงานของ For loop คือการกำหนดตัวแปรขึ้นมาตัวหนึ่ง เช่น i (นิยมใช้ เพราะย่อมาจาก Iteration แปลว่าการทำซ้ำเป็นรอบ) แล้วกำหนดค่า i ให้เป็นสมาชิกแต่ละตัวตั้งแต่ตัวแรกไปจนถึงตัวสุดท้าย โดยให้กระทำกับ i ตามโค้ดที่ถูก Indent ไว้ เช่นในที่นี้ ก็เพียงให้พิมพ์ค่า i ออกมา โดยการวนแต่ละรอบ จะ Output ออกหนึ่งบรรทัด พอวนรอบถัดไป ก็ Output ออกบรรทัดใหม่ ทำอย่างนี้ไปเรื่อยๆ จนถึงสมาชิกตัวสุดท้าย\\n\\nลองมาดู For loop สำหรับ List กัน:\\n```python\\nfruitlist = [\"apple\", \"banana\", \"cherry\"]\\n\\nfor i in fruitlist:\\n    print(i)\\n```\\nจะได้:\\n```\\napple\\nbanana\\ncherry\\n```\\nหลายคนอาจสงสัยว่า จะทำอย่างนี้ไปทำไม คำตอบคือมันจะมีประโยชน์เมื่อไปผสมกับอย่างอื่นที่ทำให้ Output นั้นมีความหมาย ตัวอย่างเช่น:\\n```python\\nfruitlist = [\"apple\", \"banana\", \"cherry\"]\\ncount = 1\\n\\nfor i in fruitlist:\\n    print(\"Fruit number \" + str(count) + \" is \" + i + \".\")\\n    count = count + 1\\n\\nprint(\"There are \" + str(count-1) + \" fruit types.\")\\n```\\nOutput ที่ได้คือ:\\n```\\nFruit number 1 is apple.\\nFruit number 2 is banana.\\nFruit number 3 is cherry.\\nThere are 3 fruit types.\\n```\\nอธิบายดังนี้:\\n\\n* เรามีข้อมูลตั้งต้นเป็น List ของประเภทผลไม้ เราต้องการให้โปรแกรมระบุลำดับที่ของประเภทผลไม้ พร้อมกับบอกว่าเป็นประเภทอะไร นอกจากนั้น เรายังอยากให้โปรแกรมสรุปว่าสุดท้ายผลไม้ใน List มีทั้งหมดกี่ประเภท\\n* เมื่อได้โจทย์แล้ว ก็ต้องวางแผน จะเห็นว่าเราต้องการนับลำดับที่ของผลไม้ โดยให้ตัวเลขเพิ่มขึ้นเรื่อยๆ ดังนั้น เราจะกำหนดตัวแปรสำหรับลำดับที่ที่จะแสดงผล โดยกำหนดไว้เป็น 1 ก่อน ด้วยการกำหนด `count = 1`\\n* จากนั้นก็เริ่มวน For loop โดยใน Loop ให้พิมพ์ `count` และ `i` ซึ่งก็คือชื่อผลไม้ใน List นั่นเอง\\n* ตอนเราจะจบ Loop แต่ละรอบ เราจะต้องเปลี่ยนตัวแปร `count` ให้เพิ่มขึ้นรอบละ 1 ซึ่งทำได้โดยการกำหนด `count = count + 1` หรือจะย่อว่า `count += 1` ก็ได้\\n* ดังนั้น พอผ่านรอบแรกมาถึงจุดนี้ `count` จะเท่ากับ 2 เมื่อ For loop วนรอบสอง ก็จะแสดงค่า `count` ของรอบนี้ เป็น 2 ตามที่ต้องการ ซึ่งจับคู่กับ `banana` ทำอย่างนี้ไปเรื่อยๆ จนจบ List\\n* สุดท้าย เราจะสรุปจำนวนประเภทผลไม้ ซึ่งก็คือ `count` แต่ต้องลบ 1 เพราะรอบสุดท้าย คือรอบที่ 3 ได้จบรอบด้วยการบวก `count` ไปอีก 1 ทำให้ `count` ตอนจบเท่ากับ 4 ตอนพิมพ์จำนวนสรุปตอนจบนี้ ต้องทำ \"นอก Loop\" นั่นคือให้กลับไปสร้าง Statement ที่ไม่ได้อยู่ใน Indent นั่นเอง เพราะถ้ายังอยู่ใน Indent โปรแกรมก็จะเรียกโค้ดบรรทัดนี้ทุกครั้งที่วน Loop ซึ่งไม่ใช่สิ่งที่เราต้องการ\\n* อนึ่ง วิธีการสรุปจำนวนรายการใน List ที่ง่ายกว่านั้น คือการใช้ฟังก์ชัน `len(object)` เช่น `print(\"There are \" + str(len(fruitlist)) + \" fruit types.\")` สังเกตว่าเราใช้ `str()` คลุมรอบ `len()` อีกที เพราะ `len()` ให้ผลเป็นตัวเลข ซึ่งเอามา Concatenate กับ Text ไม่ได้ ต้องเปลี่ยนตัวเลขให้เป็น `str` เสียก่อน\\n\\nเราสามารถ**หยุด For loop** ได้ด้วยการใช้ `break` และ `continue`\\n\\n1) `break` จะหยุด For loop ก่อนที่จะวนครบทุกรายการ โดยจะรันรายการล่าสุดเป็นรายการสุดท้าย:\\n\\n```python\\nfruits = [\"apple\", \"banana\", \"cherry\"]\\nfor i in fruits:\\n    print(i)\\n    if i == \"banana\":\\n        break\\n```\\n\\nจะได้:\\n```\\napple\\nbanana\\n```\\n2) continue จะข้ามรายการล่าสุด แล้วไปต่อที่รายการถัดไป:\\n```python\\nfruits = [\"apple\", \"banana\", \"cherry\"]\\nfor i in fruits:\\n    if i == \"banana\":\\n        continue\\n    print(i)\\n```\\nจะได้:\\n```\\napple\\ncherry\\n```\\n\\n**While loop** เป็นการวนซ้ำตราบใดที่เงื่อนไขยังคงเป็นจริง เช่น:\\n```python\\ni = 1\\nwhile i < 6:\\n    print(i)\\n    i += 1\\n```\\nจะได้:\\n```\\n1\\n2\\n3\\n4\\n5\\n```\\nการใช้ While loop ให้ระวังอย่าลืมบวก i เพิ่ม เพื่อให้มีจุดที่ Loop จะวนจนไปชนเงื่อนไขที่ไม่เป็นจริง มิฉะนั้น หากเงื่อนไขเป็นจริงเสมอ While loop ก็จะวนตลอดกาล\\n\\nการหยุด While loop สามารถใช้ break และ continue ได้เช่นเดียวกับ For loop\\n### Function: ฟังก์ชัน\\n\\nFunction คือกลุ่มโค้ดที่จะรันต่อเมื่อถูกเรียกใช้เท่านั้น การสร้างฟังก์ชัน ให้ใช้ def เว้นวรรค ตามด้วยชื่อฟังก์ชันที่ต้องการ ตามด้วย Colon : ตัวอย่างเช่น:\\n```python\\ndef greeting():\\n    print(\"Hello!\")\\n```\\nพอจะเรียกฟังก์ชัน ก็เพียงเรียก greeting() ก็จะได้:\\n```\\nHello!\\n```\\nสังเกตว่าที่ผ่านมา เรารู้จักกับฟังก์ชันที่มาพร้อมกับ Python หลายตัวแล้ว เช่น `str()` หรือ `len()` ฟังก์ชันเหล่านี้ก็เหมือนกับฟังก์ชันที่เราสร้างขึ้นเอง ต่างกันที่ Python เตรียมมาให้เราแล้วเท่านั้นเอง\\nArgument\\n\\nเราสามารถใส่ข้อมูลเข้าไปในฟังก์ชัน เพื่อให้ฟังก์ชันทำอะไรบางอย่างกับข้อมูล แล้วให้ผลลัพธ์ออกมาได้ โดยผลลัพธ์ก็สามารถออกมาเป็นข้อมูลได้เช่นกัน ข้อมูลที่เราใส่ในฟังก์ชัน เรียกว่า Argument (บางทีก็เรียกว่า Parameter เวลาพูดถึงตัวแปรที่อยู่ในวงเล็บตอนสร้างฟังก์ชัน) โดยใส่ในวงเล็บที่ต่อท้ายชื่อฟังก์ชันตอนเราสร้างฟังก์ชัน ตัวอย่างเช่น:\\n```python\\ndef greeting(name):\\n    print(\"Hello, \" + name + \"!\")\\n```\\nเวลาเรียกฟังก์ชัน ก็ใส่ Argument ที่เราต้องการลงไป เช่น:\\n```python\\ngreeting(\"John\")\\n```\\nก็จะได้:\\n```\\nHello, John!\\n```\\nถ้าเรากำหนด Argument กี่ตัวตอนสร้าง Function ตอนเรียกเราก็ต้องใส่ Argument จำนวนเท่ากัน เข่น:\\n```python\\ndef greeting(fname, lname):\\n    print(\"Hello, \" + fname + \" \" + lname + \"!\")\\n```\\nเวลาเรียกฟังก์ชัน ก็ใส่ Argument ทั้งสองลงไปตามลำดับ เช่น:\\n```python\\ngreeting(\"John\", \"Doe\")\\n```\\nก็จะได้:\\n```\\nHello, John Doe!\\n```\\nหากเราไม่รู้ว่าตอนเรียกฟังก์ชัน จะมี Argument ที่ป้อนเข้าไปกี่ตัว เราสามารถทำดังนี้ตอนสร้างฟังก์ชัน:\\n\\n1) ใช้ Arbitary argument หรือ `*args` โดยการใส่ `*` ก่อนชื่อ Parameter ตอนสร้างฟังก์ชัน วิธีนี้จะเป็นการส่ง Tuple เข้าไปในในฟังก์ชัน โดยไม่ต้องระบุจำนวน Argument ที่แน่นอน ตัวอย่างเช่น:\\n```python\\ndef fruitlist(*fruit):\\n    print(\"The last fruit is \" + fruit[-1] + \".\")\\n\\nfruitlist(\"apple\", \"banana\", \"cherry\")\\n```\\nก็จะได้ The last fruit is cherry. สังเกตว่าในฟังก์ชัน เราเลือกผลไม้รายการสุดท้ายโดยการ Slice ด้วย [-1] ซึ่งหมายถึงรายการสุดท้าย\\n\\n2) ใช้ Keyword argument หรือ kwargs โดยการใส่ Key-value เข้าไปในฟังก์ชัน โดยตอนเรียกให้เรียก Key ก็จะได้ Value วิธีนี้ต้องระบุจำนวน Argument ที่แน่นอน เช่น:\\n```python\\ndef fruitlist(fruit1, fruit2, fruit3):\\n    print(\"The last fruit is \" + fruit3 + \".\")\\n\\nfruitlist(fruit1 = \"apple\", fruit2 = \"banana\", fruit3 = \"cherry\")\\n```\\nก็จะได้ The last fruit is cherry. เช่นเดิม\\n\\n3) ใช้ Arbitary keyword argument หรือ `**kwargs` โดยการใส่ `**` ก่อนชื่อ Parameter ตอนสร้างฟังก์ชัน วิธีนี้จะเป็นการส่ง Dictionary ของ Argument เข้าไปในฟังก์ชัน โดยตอนเรียกก็เรียก Key จะได้ Value ออกมา วิธีนี้ไม่ต้องระบุจำนวน Argument ที่แน่นอน เช่น:\\n```python\\ndef fruitlist(**fruits):\\n    print(\"The last fruit is \" + fruits[\"fruit3\"] + \".\")\\n\\nfruitlist(fruit1 = \"apple\", fruit2 = \"banana\", fruit3 = \"cherry\")\\n```\\nก็จะได้ The last fruit is cherry. เช่นเดียวกัน\\nใช้ List และ Dictionary เป็น Argument\\n\\nเราสามารถใส่ List หรือ Dictionary ที่กำหนดไว้แล้ว ลงไปเป็น Argument ได้ เช่น:\\n```python\\nfruits = [\"apple\", \"banana\", \"cherry\"]\\n\\ndef fruitlist(fruits):\\n    print(\"The last fruit is \" + fruits[-1] + \".\")\\n\\nfruitlist(fruits)\\n```\\nหรือ:\\n```python\\nfruitdict = {\\n    \"fruit1\": \"apple\",\\n    \"fruit2\": \"banana\",\\n    \"fruit3\": \"cherry\"\\n}\\n\\ndef fruitlist(fruitdict):\\n    print(\"The last fruit is \" + fruitdict[\"fruit3\"] + \".\")\\n\\nfruitlist(fruitdict)\\n```\\nก็จะได้ `The last fruit is cherry.` เช่นเดียวกันทั้งหมด\\nReturn value\\n\\nเราสามารถให้ฟังก์ชัน Output ค่าออกมาได้ เช่น:\\n```python\\ndef bmi(weight, height):\\n    return weight/(height**2)\\n\\nbmi(70, 1.72)\\n```\\n### Library import: การเรียกใช้ Library\\n\\nPython มี Library ที่ขยายความสามารถเฉพาะทางให้เลือกใช้จำนวนมาก เช่น numpy เป็น Library สำหรับการประมวลผลแบบ Array หลายมิติที่มีประสิทธิภาพสูง หรือ pandas เอาไว้จัดการข้อมูลแบบตาราง เป็นต้น\\n\\nก่อนจะใช้ Library เหล่านี้ เราต้องติดตั้งเสียก่อน โดยถ้าใช้ Python ปกติ ก็ใช้ pip เป็นตัวติดตั้งและจัดการ Package เช่น `pip install numpy` ส่วนถ้าใช้ Anaconda ที่แนะนำในตอนแรก ก็ใช้คำสั่ง conda จัดการ เช่น `conda install numpy` เป็นต้น\\n\\nการเรียกใช้ Library มีสองวิธีใหญ่ๆ ได้แก่:\\n\\n1) เรียกโดยตรง โดยใช้ import เช่น:\\n```python\\nimport numpy\\n```\\nเวลาจะใช้งาน ก็ต้องเรียก numpy ก่อน เช่น:\\n```python\\na = numpy.array([[1, 2], [3, 4]])\\n```\\n2) เรียกโดยการเปลี่ยนชื่อ หรือที่เรียกว่า Alias เพื่อย่นชื่อให้สั้นลงโดยใช้ as จะได้เรียกใช้สะดวก เช่น:\\n```python\\nimport numpy as np\\n```\\nเวลาจะใช้งาน ก็สามารถเรียก np ได้เลย เช่น:\\n```python\\na = np.array([[1, 2], [3, 4]])\\n```\\n3) เรียกเฉพาะบางฟังก์ชันหรือ Class ที่จะใช้ โดยใช้ `from...import` เช่น:\\n```python\\nfrom numpy import array\\n```\\nเวลาเรียก ก็เรียกชื่อฟังก์ชัน array เลย เช่น:\\n```python\\na = array([[1, 2], [3, 4]])\\n```\\nทั้งหมดนี้น่าจะพอให้สามารถเริ่มต้นเขียนโค้ดด้วย Python เรียกใช้ Library ต่างๆ และประยุกต์นำไปแก้ปัญหาต่างๆ ได้',\n",
       " 'source': 'pythainlp/thai-it-books',\n",
       " 'metadata': {'src': 'https://guopai.github.io', 'domain': 'book'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"thai-it-books.jsonl\",\"w\") as f:\n",
    "    for i in data:\n",
    "        f.write(json.dumps(i,ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
